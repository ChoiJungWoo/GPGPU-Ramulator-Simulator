diff -r gpgpu-sim/addrdec.cc ../../../gpgpu-sim_distribution/src/gpgpu-sim/addrdec.cc
39c39
< static void addrdec_getmasklimit(new_addr_type mask, unsigned char *high, unsigned char *low);
---
> static void addrdec_getmasklimit(new_addr_type mask, unsigned char *high, unsigned char *low); 
45,46c45,46
<    memset(addrdec_mklow, 0, N_ADDRDEC);
<    memset(addrdec_mkhigh, 64, N_ADDRDEC);
---
>    memset(addrdec_mklow,0,N_ADDRDEC);
>    memset(addrdec_mkhigh,64,N_ADDRDEC);
57,58c57,58
<                           "mapping memory address to dram model {dramid@<start bit>;<memory address map>}",
<                           NULL);
---
>       "mapping memory address to dram model {dramid@<start bit>;<memory address map>}",
>       NULL);
60,64c60,64
<                           "run sweep test to check address mapping for aliased address",
<                           "0");
<    option_parser_register(opp, "-gpgpu_mem_address_mask", OPT_INT32, &gpgpu_mem_address_mask,
<                           "0 = old addressing mask, 1 = new addressing mask, 2 = new add. mask + flipped bank sel and chip sel bits",
<                           "0");
---
>       "run sweep test to check address mapping for aliased address",
>       "0");
>    option_parser_register(opp, "-gpgpu_mem_address_mask", OPT_INT32, &gpgpu_mem_address_mask, 
>                "0 = old addressing mask, 1 = new addressing mask, 2 = new add. mask + flipped bank sel and chip sel bits",
>                "0");
67,68c67,68
< new_addr_type linear_to_raw_address_translation::partition_address( new_addr_type addr ) const
< {
---
> new_addr_type linear_to_raw_address_translation::partition_address( new_addr_type addr ) const 
> { 
70c70
<       return addrdec_packbits( ~(addrdec_mask[CHIP] | sub_partition_id_mask), addr, 64, 0 );
---
>       return addrdec_packbits( ~(addrdec_mask[CHIP] | sub_partition_id_mask), addr, 64, 0 ); 
72,75c72,75
<       // see addrdec_tlx for explanation
<       unsigned long long int partition_addr;
<       partition_addr = ( (addr >> ADDR_CHIP_S) / m_n_channel) << ADDR_CHIP_S;
<       partition_addr |= addr & ((1 << ADDR_CHIP_S) - 1);
---
>       // see addrdec_tlx for explanation 
>       unsigned long long int partition_addr; 
>       partition_addr = ( (addr>>ADDR_CHIP_S) / m_n_channel) << ADDR_CHIP_S; 
>       partition_addr |= addr & ((1 << ADDR_CHIP_S) - 1); 
77,78c77,78
<       partition_addr = addrdec_packbits( ~sub_partition_id_mask, partition_addr, 64, 0);
<       return partition_addr;
---
>       partition_addr = addrdec_packbits( ~sub_partition_id_mask, partition_addr, 64, 0); 
>       return partition_addr; 
83,84c83,84
< {
<    unsigned long long int addr_for_chip, rest_of_addr;
---
> {  
>    unsigned long long int addr_for_chip,rest_of_addr;
90,91c90
<       tlx->burst = addrdec_packbits(addrdec_mask[BURST], addr, addrdec_mkhigh[BURST], addrdec_mklow[BURST]);
<       //fprintf(stderr, "chip number is %u, the address is %lu\n", tlx->chip, addr );
---
>       tlx->burst= addrdec_packbits(addrdec_mask[BURST], addr, addrdec_mkhigh[BURST], addrdec_mklow[BURST]);
95,98c94,97
<       // - recreate the rest of the address by stitching the quotient of MSBs and the LSBs
<       addr_for_chip = (addr >> ADDR_CHIP_S) % m_n_channel;
<       rest_of_addr = ( (addr >> ADDR_CHIP_S) / m_n_channel) << ADDR_CHIP_S;
<       rest_of_addr |= addr & ((1 << ADDR_CHIP_S) - 1);
---
>       // - recreate the rest of the address by stitching the quotient of MSBs and the LSBs 
>       addr_for_chip = (addr>>ADDR_CHIP_S) % m_n_channel; 
>       rest_of_addr = ( (addr>>ADDR_CHIP_S) / m_n_channel) << ADDR_CHIP_S; 
>       rest_of_addr |= addr & ((1 << ADDR_CHIP_S) - 1); 
100c99
<       tlx->chip = addr_for_chip;
---
>       tlx->chip = addr_for_chip; 
104c103
<       tlx->burst = addrdec_packbits(addrdec_mask[BURST], rest_of_addr, addrdec_mkhigh[BURST], addrdec_mklow[BURST]);
---
>       tlx->burst= addrdec_packbits(addrdec_mask[BURST], rest_of_addr, addrdec_mkhigh[BURST], addrdec_mklow[BURST]);
108c107
<    unsigned sub_partition_addr_mask = m_n_sub_partition_in_channel - 1;
---
>    unsigned sub_partition_addr_mask = m_n_sub_partition_in_channel - 1; 
110c109
<                         + (tlx->bk & sub_partition_addr_mask);
---
>                         + (tlx->bk & sub_partition_addr_mask); 
122c121
< 
---
>    
134,135c133,134
<    addrdec_mask[BURST] = 0x0;
< 
---
>    addrdec_mask[BURST]= 0x0;
>    
139,152c138,151
<       case 'D': case 'd':
<          assert(dramid_parsed != 1); addrdec_mask[CHIP]  |= (1ULL << ofs); ofs--; break;
<       case 'B': case 'b':   addrdec_mask[BK]    |= (1ULL << ofs); ofs--; break;
<       case 'R': case 'r':   addrdec_mask[ROW]   |= (1ULL << ofs); ofs--; break;
<       case 'C': case 'c':   addrdec_mask[COL]   |= (1ULL << ofs); ofs--; break;
<       case 'S': case 's':   addrdec_mask[BURST] |= (1ULL << ofs); addrdec_mask[COL]   |= (1ULL << ofs); ofs--; break;
<       // ignore bit
<       case '0': ofs--; break;
<       // ignore character
<       case '|':
<       case ' ':
<       case '.': break;
<       default:
<          fprintf(stderr, "ERROR: Invalid address mapping character '%c' in option '%s'\n", *cmapping, option);
---
>          case 'D': case 'd':  
>             assert(dramid_parsed != 1); addrdec_mask[CHIP]  |= (1ULL << ofs); ofs--; break;
>          case 'B': case 'b':   addrdec_mask[BK]    |= (1ULL << ofs); ofs--; break;
>          case 'R': case 'r':   addrdec_mask[ROW]   |= (1ULL << ofs); ofs--; break;
>          case 'C': case 'c':   addrdec_mask[COL]   |= (1ULL << ofs); ofs--; break;
>          case 'S': case 's':   addrdec_mask[BURST] |= (1ULL << ofs); addrdec_mask[COL]   |= (1ULL << ofs); ofs--; break;
>          // ignore bit
>          case '0': ofs--; break;
>          // ignore character
>          case '|':
>          case ' ':
>          case '.': break;
>          default:
>             fprintf(stderr, "ERROR: Invalid address mapping character '%c' in option '%s'\n", *cmapping, option);
163c162
< void linear_to_raw_address_translation::init(unsigned int n_channel, unsigned int n_sub_partition_in_channel)
---
> void linear_to_raw_address_translation::init(unsigned int n_channel, unsigned int n_sub_partition_in_channel) 
169c168
<    m_n_sub_partition_in_channel = n_sub_partition_in_channel;
---
>    m_n_sub_partition_in_channel = n_sub_partition_in_channel; 
171c170
<    gap = (n_channel - ::powli(2, nchipbits));
---
>    gap = (n_channel - ::powli(2,nchipbits));
176c175
<    case 0:
---
>    case 0: 
240c239
<       break;
---
>       break;                         
262c261
<    case 160:
---
>    case 160: 
274c273
<    if (addrdec_option != NULL)
---
>    if (addrdec_option != NULL) 
277c276
<    if (ADDR_CHIP_S != -1) {
---
>    if (ADDR_CHIP_S != -1) { 
279c278
<          // number of chip is power of two:
---
>          // number of chip is power of two: 
286c285
<          for (i = ADDR_CHIP_S; i < (ADDR_CHIP_S + nchipbits); i++) {
---
>          for (i=ADDR_CHIP_S;i<(ADDR_CHIP_S+nchipbits);i++) {
293c292
<       assert((n_channel & (n_channel - 1)) == 0);
---
>       assert((n_channel & (n_channel - 1)) == 0); 
295,296c294,295
<    // make sure m_n_sub_partition_in_channel is power of two
<    assert((m_n_sub_partition_in_channel & (m_n_sub_partition_in_channel - 1)) == 0);
---
>    // make sure m_n_sub_partition_in_channel is power of two 
>    assert((m_n_sub_partition_in_channel & (m_n_sub_partition_in_channel - 1)) == 0); 
311c310
<    sub_partition_id_mask = 0;
---
>    sub_partition_id_mask = 0; 
313,317c312,316
<       unsigned n_sub_partition_log2 = LOGB2_32(m_n_sub_partition_in_channel);
<       unsigned pos = 0;
<       for (unsigned i = addrdec_mklow[BK]; i < addrdec_mkhigh[BK]; i++) {
<          if ((addrdec_mask[BK] & ((unsigned long long int)1 << i)) != 0) {
<             sub_partition_id_mask |= ((unsigned long long int)1 << i);
---
>       unsigned n_sub_partition_log2 = LOGB2_32(m_n_sub_partition_in_channel); 
>       unsigned pos=0;
>       for (unsigned i=addrdec_mklow[BK];i<addrdec_mkhigh[BK];i++) {
>          if ((addrdec_mask[BK] & ((unsigned long long int)1<<i)) != 0) {
>             sub_partition_id_mask |= ((unsigned long long int)1<<i);
319,320c318,319
<             if (pos >= n_sub_partition_log2)
<                break;
---
>             if (pos >= n_sub_partition_log2) 
>                break; 
327c326
<       sweep_test();
---
>       sweep_test(); 
331c330
< #include "../tr1_hash_map.h"
---
> #include "../tr1_hash_map.h" 
333c332
< bool operator==(const addrdec_t &x, const addrdec_t &y)
---
> bool operator==(const addrdec_t &x, const addrdec_t &y) 
335c334
<    return ( memcmp(&x, &y, sizeof(addrdec_t)) == 0 );
---
>    return ( memcmp(&x, &y, sizeof(addrdec_t)) == 0 ); 
338c337
< bool operator<(const addrdec_t &x, const addrdec_t &y)
---
> bool operator<(const addrdec_t &x, const addrdec_t &y) 
340c339
<    if (x.chip >= y.chip) return false;
---
>    if (x.chip >= y.chip) return false; 
345c344
<    else return true;
---
>    else return true; 
350c349
< public:
---
> public: 
352c351
<       return (x.chip ^ x.bk ^ x.row ^ x.col ^ x.burst);
---
>       return (x.chip ^ x.bk ^ x.row ^ x.col ^ x.burst); 
356c355
< // a simple sweep test to ensure that two linear addresses are not mapped to the same raw address
---
> // a simple sweep test to ensure that two linear addresses are not mapped to the same raw address 
359c358
<    new_addr_type sweep_range = 16 * 1024 * 1024;
---
>    new_addr_type sweep_range = 16 * 1024 * 1024; 
362c361
<    typedef tr1_hash_map<addrdec_t, new_addr_type> history_map_t;
---
>    typedef tr1_hash_map<addrdec_t, new_addr_type> history_map_t; 
364c363
<    typedef tr1_hash_map<addrdec_t, new_addr_type, hash_addrdec_t> history_map_t;
---
>    typedef tr1_hash_map<addrdec_t, new_addr_type, hash_addrdec_t> history_map_t; 
366c365
<    history_map_t history_map;
---
>    history_map_t history_map; 
369,370c368,369
<       addrdec_t tlx;
<       addrdec_tlx(raw_addr, &tlx);
---
>       addrdec_t tlx; 
>       addrdec_tlx(raw_addr, &tlx); 
372c371
<       history_map_t::iterator h = history_map.find(tlx);
---
>       history_map_t::iterator h = history_map.find(tlx); 
375,376c374,375
<          printf("[AddrDec] ** Error: address decoding mapping aliases two addresses to same partition with same intra-partition address: %llx %llx\n", h->second, raw_addr);
<          abort();
---
>          printf("[AddrDec] ** Error: address decoding mapping aliases two addresses to same partition with same intra-partition address: %llx %llx\n", h->second, raw_addr); 
>          abort(); 
378,382c377,381
<          assert((int)tlx.chip < m_n_channel);
<          // ensure that partition_address() returns the concatenated address
<          if ((ADDR_CHIP_S != -1 and raw_addr >= (1ULL << ADDR_CHIP_S)) or
<                (ADDR_CHIP_S == -1 and raw_addr >= (1ULL << addrdec_mklow[CHIP]))) {
<             assert(raw_addr != partition_address(raw_addr));
---
>          assert((int)tlx.chip < m_n_channel); 
>          // ensure that partition_address() returns the concatenated address 
>          if ((ADDR_CHIP_S != -1 and raw_addr >= (1ULL << ADDR_CHIP_S)) or 
>              (ADDR_CHIP_S == -1 and raw_addr >= (1ULL << addrdec_mklow[CHIP]))) {
>             assert(raw_addr != partition_address(raw_addr)); 
384c383
<          history_map[tlx] = raw_addr;
---
>          history_map[tlx] = raw_addr; 
387c386
<       if ((raw_addr & 0xffff) == 0) printf("%llu scaned\n", raw_addr);
---
>       if ((raw_addr & 0xffff) == 0) printf("%llu scaned\n", raw_addr); 
393,399c392,398
<    fprintf(fp, "\tchip:%x ", chip);
<    fprintf(fp, "\trow:%x ", row);
<    fprintf(fp, "\tcol:%x ", col);
<    fprintf(fp, "\tbk:%x ", bk);
<    fprintf(fp, "\tburst:%x ", burst);
<    fprintf(fp, "\tsub_partition:%x ", sub_partition);
< }
---
>    fprintf(fp,"\tchip:%x ", chip);
>    fprintf(fp,"\trow:%x ", row);
>    fprintf(fp,"\tcol:%x ", col);
>    fprintf(fp,"\tbk:%x ", bk);
>    fprintf(fp,"\tburst:%x ", burst);
>    fprintf(fp,"\tsub_partition:%x ", sub_partition);
> } 
405c404
<    int i;
---
>    int i; 
412c411
< static unsigned int LOGB2_32( unsigned int v )
---
> static unsigned int LOGB2_32( unsigned int v ) 
428c427
< static new_addr_type addrdec_packbits( new_addr_type mask, new_addr_type val, unsigned char high, unsigned char low)
---
> static new_addr_type addrdec_packbits( new_addr_type mask, new_addr_type val, unsigned char high, unsigned char low) 
430c429
<    unsigned pos = 0;
---
>    unsigned pos=0;
432,434c431,433
<    for (unsigned i = low; i < high; i++) {
<       if ((mask & ((unsigned long long int)1 << i)) != 0) {
<          result |= ((val & ((unsigned long long int)1 << i)) >> i) << pos;
---
>    for (unsigned i=low;i<high;i++) {
>       if ((mask & ((unsigned long long int)1<<i)) != 0) {
>          result |= ((val & ((unsigned long long int)1<<i)) >> i) << pos;
441c440
< static void addrdec_getmasklimit(new_addr_type mask, unsigned char *high, unsigned char *low)
---
> static void addrdec_getmasklimit(new_addr_type mask, unsigned char *high, unsigned char *low) 
448,449c447,448
<    for (i = 0; i < 64; i++) {
<       if ((mask & ((unsigned long long int)1 << i)) != 0) {
---
>    for (i=0;i<64;i++) {
>       if ((mask & ((unsigned long long int)1<<i)) != 0) {
Only in ../../../gpgpu-sim_distribution/src/gpgpu-sim/: dram.cc
Only in gpgpu-sim/: dram.cc.bak
Only in ../../../gpgpu-sim_distribution/src/gpgpu-sim/: dram.h
Only in gpgpu-sim/: dram.j.bak
Only in ../../../gpgpu-sim_distribution/src/gpgpu-sim/: dram_sched.cc
Only in gpgpu-sim/: dram_sched.cc.bak
Only in ../../../gpgpu-sim_distribution/src/gpgpu-sim/: dram_sched.h
Only in gpgpu-sim/: dram_sched.h.bak
diff -r gpgpu-sim/gpu-sim.cc ../../../gpgpu-sim_distribution/src/gpgpu-sim/gpu-sim.cc
39c39
< //#include "dram.h"
---
> #include "dram.h"
48c48
< //#include "dram.h"
---
> #include "dram.h"
80c80
< bool g_interactive_debugger_enabled = false;
---
> bool g_interactive_debugger_enabled=false;
84d83
< int core_numbers = 0;
88c87
< unsigned int gpu_stall_dramfull = 0;
---
> unsigned int gpu_stall_dramfull = 0; 
96c95
< #define  ICNT  0x08
---
> #define  ICNT  0x08  
110,137c109,136
<   option_parser_register(opp, "-gpuwattch_xml_file", OPT_CSTR,
<                          &g_power_config_name, "GPUWattch XML file",
<                          "gpuwattch.xml");
< 
<   option_parser_register(opp, "-power_simulation_enabled", OPT_BOOL,
<                          &g_power_simulation_enabled, "Turn on power simulator (1=On, 0=Off)",
<                          "0");
< 
<   option_parser_register(opp, "-power_per_cycle_dump", OPT_BOOL,
<                          &g_power_per_cycle_dump, "Dump detailed power output each cycle",
<                          "0");
< 
<   // Output Data Formats
<   option_parser_register(opp, "-power_trace_enabled", OPT_BOOL,
<                          &g_power_trace_enabled, "produce a file for the power trace (1=On, 0=Off)",
<                          "0");
< 
<   option_parser_register(opp, "-power_trace_zlevel", OPT_INT32,
<                          &g_power_trace_zlevel, "Compression level of the power trace output log (0=no comp, 9=highest)",
<                          "6");
< 
<   option_parser_register(opp, "-steady_power_levels_enabled", OPT_BOOL,
<                          &g_steady_power_levels_enabled, "produce a file for the steady power levels (1=On, 0=Off)",
<                          "0");
< 
<   option_parser_register(opp, "-steady_state_definition", OPT_CSTR,
<                          &gpu_steady_state_definition, "allowed deviation:number of samples",
<                          "8:4");
---
> 	  option_parser_register(opp, "-gpuwattch_xml_file", OPT_CSTR,
> 			  	  	  	  	 &g_power_config_name,"GPUWattch XML file",
> 	                   "gpuwattch.xml");
> 
> 	   option_parser_register(opp, "-power_simulation_enabled", OPT_BOOL,
> 	                          &g_power_simulation_enabled, "Turn on power simulator (1=On, 0=Off)",
> 	                          "0");
> 
> 	   option_parser_register(opp, "-power_per_cycle_dump", OPT_BOOL,
> 	                          &g_power_per_cycle_dump, "Dump detailed power output each cycle",
> 	                          "0");
> 
> 	   // Output Data Formats
> 	   option_parser_register(opp, "-power_trace_enabled", OPT_BOOL,
> 	                          &g_power_trace_enabled, "produce a file for the power trace (1=On, 0=Off)",
> 	                          "0");
> 
> 	   option_parser_register(opp, "-power_trace_zlevel", OPT_INT32,
> 	                          &g_power_trace_zlevel, "Compression level of the power trace output log (0=no comp, 9=highest)",
> 	                          "6");
> 
> 	   option_parser_register(opp, "-steady_power_levels_enabled", OPT_BOOL,
> 	                          &g_steady_power_levels_enabled, "produce a file for the steady power levels (1=On, 0=Off)",
> 	                          "0");
> 
> 	   option_parser_register(opp, "-steady_state_definition", OPT_CSTR,
> 			   	  &gpu_steady_state_definition, "allowed deviation:number of samples",
> 	                 	  "8:4");
143,194c142,193
<   option_parser_register(opp, "-gpgpu_dram_scheduler", OPT_INT32, &scheduler_type,
<                          "0 = fifo, 1 = FR-FCFS (defaul)", "1");
<   option_parser_register(opp, "-gpgpu_dram_partition_queues", OPT_CSTR, &gpgpu_L2_queue_config,
<                          "i2$:$2d:d2$:$2i",
<                          "8:8:8:8");
< 
<   option_parser_register(opp, "-l2_ideal", OPT_BOOL, &l2_ideal,
<                          "Use a ideal L2 cache that always hit",
<                          "0");
<   option_parser_register(opp, "-gpgpu_cache:dl2", OPT_CSTR, &m_L2_config.m_config_string,
<                          "unified banked L2 data cache config "
<                          " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>}",
<                          "64:128:8,L:B:m:N,A:16:4,4");
<   option_parser_register(opp, "-gpgpu_cache:dl2_texture_only", OPT_BOOL, &m_L2_texure_only,
<                          "L2 cache used for texture only",
<                          "1");
<   option_parser_register(opp, "-gpgpu_n_mem", OPT_UINT32, &m_n_mem,
<                          "number of memory modules (e.g. memory controllers) in gpu",
<                          "8");
<   option_parser_register(opp, "-gpgpu_n_sub_partition_per_mchannel", OPT_UINT32, &m_n_sub_partition_per_memory_channel,
<                          "number of memory subpartition in each memory module",
<                          "1");
<   option_parser_register(opp, "-gpgpu_n_mem_per_ctrlr", OPT_UINT32, &gpu_n_mem_per_ctrlr,
<                          "number of memory chips per memory controller",
<                          "1");
<   option_parser_register(opp, "-gpgpu_memlatency_stat", OPT_INT32, &gpgpu_memlatency_stat,
<                          "track and display latency statistics 0x2 enables MC, 0x4 enables queue logs",
<                          "0");
<   option_parser_register(opp, "-gpgpu_frfcfs_dram_sched_queue_size", OPT_INT32, &gpgpu_frfcfs_dram_sched_queue_size,
<                          "0 = unlimited (default); # entries per chip",
<                          "0");
<   option_parser_register(opp, "-gpgpu_dram_return_queue_size", OPT_INT32, &gpgpu_dram_return_queue_size,
<                          "0 = unlimited (default); # entries per chip",
<                          "0");
<   option_parser_register(opp, "-gpgpu_dram_buswidth", OPT_UINT32, &busW,
<                          "default = 4 bytes (8 bytes per cycle at DDR)",
<                          "4");
<   option_parser_register(opp, "-gpgpu_dram_burst_length", OPT_UINT32, &BL,
<                          "Burst length of each DRAM request (default = 4 data bus cycle)",
<                          "4");
<   option_parser_register(opp, "-dram_data_command_freq_ratio", OPT_UINT32, &data_command_freq_ratio,
<                          "Frequency ratio between DRAM data bus and command bus (default = 2 times, i.e. DDR)",
<                          "2");
<   option_parser_register(opp, "-gpgpu_dram_timing_opt", OPT_CSTR, &gpgpu_dram_timing_opt,
<                          "DRAM timing parameters = {nbk:tCCD:tRRD:tRCD:tRAS:tRP:tRC:CL:WL:tCDLR:tWR:nbkgrp:tCCDL:tRTPL}",
<                          "4:2:8:12:21:13:34:9:4:5:13:1:0:0");
<   option_parser_register(opp, "-rop_latency", OPT_UINT32, &rop_latency,
<                          "ROP queue latency (default 85)",
<                          "85");
<   option_parser_register(opp, "-dram_latency", OPT_UINT32, &dram_latency,
<                          "DRAM latency (default 30)",
<                          "30");
---
>     option_parser_register(opp, "-gpgpu_dram_scheduler", OPT_INT32, &scheduler_type, 
>                                 "0 = fifo, 1 = FR-FCFS (defaul)", "1");
>     option_parser_register(opp, "-gpgpu_dram_partition_queues", OPT_CSTR, &gpgpu_L2_queue_config, 
>                            "i2$:$2d:d2$:$2i",
>                            "8:8:8:8");
> 
>     option_parser_register(opp, "-l2_ideal", OPT_BOOL, &l2_ideal, 
>                            "Use a ideal L2 cache that always hit",
>                            "0");
>     option_parser_register(opp, "-gpgpu_cache:dl2", OPT_CSTR, &m_L2_config.m_config_string, 
>                    "unified banked L2 data cache config "
>                    " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>}",
>                    "64:128:8,L:B:m:N,A:16:4,4");
>     option_parser_register(opp, "-gpgpu_cache:dl2_texture_only", OPT_BOOL, &m_L2_texure_only, 
>                            "L2 cache used for texture only",
>                            "1");
>     option_parser_register(opp, "-gpgpu_n_mem", OPT_UINT32, &m_n_mem, 
>                  "number of memory modules (e.g. memory controllers) in gpu",
>                  "8");
>     option_parser_register(opp, "-gpgpu_n_sub_partition_per_mchannel", OPT_UINT32, &m_n_sub_partition_per_memory_channel, 
>                  "number of memory subpartition in each memory module",
>                  "1");
>     option_parser_register(opp, "-gpgpu_n_mem_per_ctrlr", OPT_UINT32, &gpu_n_mem_per_ctrlr, 
>                  "number of memory chips per memory controller",
>                  "1");
>     option_parser_register(opp, "-gpgpu_memlatency_stat", OPT_INT32, &gpgpu_memlatency_stat, 
>                 "track and display latency statistics 0x2 enables MC, 0x4 enables queue logs",
>                 "0");
>     option_parser_register(opp, "-gpgpu_frfcfs_dram_sched_queue_size", OPT_INT32, &gpgpu_frfcfs_dram_sched_queue_size, 
>                 "0 = unlimited (default); # entries per chip",
>                 "0");
>     option_parser_register(opp, "-gpgpu_dram_return_queue_size", OPT_INT32, &gpgpu_dram_return_queue_size, 
>                 "0 = unlimited (default); # entries per chip",
>                 "0");
>     option_parser_register(opp, "-gpgpu_dram_buswidth", OPT_UINT32, &busW, 
>                  "default = 4 bytes (8 bytes per cycle at DDR)",
>                  "4");
>     option_parser_register(opp, "-gpgpu_dram_burst_length", OPT_UINT32, &BL, 
>                  "Burst length of each DRAM request (default = 4 data bus cycle)",
>                  "4");
>     option_parser_register(opp, "-dram_data_command_freq_ratio", OPT_UINT32, &data_command_freq_ratio, 
>                  "Frequency ratio between DRAM data bus and command bus (default = 2 times, i.e. DDR)",
>                  "2");
>     option_parser_register(opp, "-gpgpu_dram_timing_opt", OPT_CSTR, &gpgpu_dram_timing_opt, 
>                 "DRAM timing parameters = {nbk:tCCD:tRRD:tRCD:tRAS:tRP:tRC:CL:WL:tCDLR:tWR:nbkgrp:tCCDL:tRTPL}",
>                 "4:2:8:12:21:13:34:9:4:5:13:1:0:0");
>     option_parser_register(opp, "-rop_latency", OPT_UINT32, &rop_latency,
>                      "ROP queue latency (default 85)",
>                      "85");
>     option_parser_register(opp, "-dram_latency", OPT_UINT32, &dram_latency,
>                      "DRAM latency (default 30)",
>                      "30");
196c195
<   m_address_mapping.addrdec_setoption(opp);
---
>     m_address_mapping.addrdec_setoption(opp);
201,369c200,368
<   option_parser_register(opp, "-gpgpu_simd_model", OPT_INT32, &model,
<                          "1 = post-dominator", "1");
<   option_parser_register(opp, "-gpgpu_shader_core_pipeline", OPT_CSTR, &gpgpu_shader_core_pipeline_opt,
<                          "shader core pipeline config, i.e., {<nthread>:<warpsize>}",
<                          "1024:32");
<   option_parser_register(opp, "-gpgpu_tex_cache:l1", OPT_CSTR, &m_L1T_config.m_config_string,
<                          "per-shader L1 texture cache  (READ-ONLY) config "
<                          " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>:<rf>}",
<                          "8:128:5,L:R:m:N,F:128:4,128:2");
<   option_parser_register(opp, "-gpgpu_const_cache:l1", OPT_CSTR, &m_L1C_config.m_config_string,
<                          "per-shader L1 constant memory cache  (READ-ONLY) config "
<                          " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>} ",
<                          "64:64:2,L:R:f:N,A:2:32,4" );
<   option_parser_register(opp, "-gpgpu_cache:il1", OPT_CSTR, &m_L1I_config.m_config_string,
<                          "shader L1 instruction cache config "
<                          " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>} ",
<                          "4:256:4,L:R:f:N,A:2:32,4" );
<   option_parser_register(opp, "-gpgpu_cache:dl1", OPT_CSTR, &m_L1D_config.m_config_string,
<                          "per-shader L1 data cache config "
<                          " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq> | none}",
<                          "none" );
<   option_parser_register(opp, "-gpgpu_cache:dl1PrefL1", OPT_CSTR, &m_L1D_config.m_config_stringPrefL1,
<                          "per-shader L1 data cache config "
<                          " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq> | none}",
<                          "none" );
<   option_parser_register(opp, "-gpgpu_cache:dl1PreShared", OPT_CSTR, &m_L1D_config.m_config_stringPrefShared,
<                          "per-shader L1 data cache config "
<                          " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq> | none}",
<                          "none" );
<   option_parser_register(opp, "-gmem_skip_L1D", OPT_BOOL, &gmem_skip_L1D,
<                          "global memory access skip L1D cache (implements -Xptxas -dlcm=cg, default=no skip)",
<                          "0");
< 
<   option_parser_register(opp, "-gpgpu_perfect_mem", OPT_BOOL, &gpgpu_perfect_mem,
<                          "enable perfect memory mode (no cache miss)",
<                          "0");
<   option_parser_register(opp, "-n_regfile_gating_group", OPT_UINT32, &n_regfile_gating_group,
<                          "group of lanes that should be read/written together)",
<                          "4");
<   option_parser_register(opp, "-gpgpu_clock_gated_reg_file", OPT_BOOL, &gpgpu_clock_gated_reg_file,
<                          "enable clock gated reg file for power calculations",
<                          "0");
<   option_parser_register(opp, "-gpgpu_clock_gated_lanes", OPT_BOOL, &gpgpu_clock_gated_lanes,
<                          "enable clock gated lanes for power calculations",
<                          "0");
<   option_parser_register(opp, "-gpgpu_shader_registers", OPT_UINT32, &gpgpu_shader_registers,
<                          "Number of registers per shader core. Limits number of concurrent CTAs. (default 8192)",
<                          "8192");
<   option_parser_register(opp, "-gpgpu_shader_cta", OPT_UINT32, &max_cta_per_core,
<                          "Maximum number of concurrent CTAs in shader (default 8)",
<                          "8");
<   option_parser_register(opp, "-gpgpu_num_cta_barriers", OPT_UINT32, &max_barriers_per_cta,
<                          "Maximum number of named barriers per CTA (default 16)",
<                          "16");
<   option_parser_register(opp, "-gpgpu_n_clusters", OPT_UINT32, &n_simt_clusters,
<                          "number of processing clusters",
<                          "10");
<   option_parser_register(opp, "-gpgpu_n_cores_per_cluster", OPT_UINT32, &n_simt_cores_per_cluster,
<                          "number of simd cores per cluster",
<                          "3");
<   option_parser_register(opp, "-gpgpu_n_cluster_ejection_buffer_size", OPT_UINT32, &n_simt_ejection_buffer_size,
<                          "number of packets in ejection buffer",
<                          "8");
<   option_parser_register(opp, "-gpgpu_n_ldst_response_buffer_size", OPT_UINT32, &ldst_unit_response_queue_size,
<                          "number of response packets in ld/st unit ejection buffer",
<                          "2");
<   option_parser_register(opp, "-gpgpu_shmem_size", OPT_UINT32, &gpgpu_shmem_size,
<                          "Size of shared memory per shader core (default 16kB)",
<                          "16384");
<   option_parser_register(opp, "-gpgpu_shmem_size", OPT_UINT32, &gpgpu_shmem_sizeDefault,
<                          "Size of shared memory per shader core (default 16kB)",
<                          "16384");
<   option_parser_register(opp, "-gpgpu_shmem_size_PrefL1", OPT_UINT32, &gpgpu_shmem_sizePrefL1,
<                          "Size of shared memory per shader core (default 16kB)",
<                          "16384");
<   option_parser_register(opp, "-gpgpu_shmem_size_PrefShared", OPT_UINT32, &gpgpu_shmem_sizePrefShared,
<                          "Size of shared memory per shader core (default 16kB)",
<                          "16384");
<   option_parser_register(opp, "-gpgpu_shmem_num_banks", OPT_UINT32, &num_shmem_bank,
<                          "Number of banks in the shared memory in each shader core (default 16)",
<                          "16");
<   option_parser_register(opp, "-gpgpu_shmem_limited_broadcast", OPT_BOOL, &shmem_limited_broadcast,
<                          "Limit shared memory to do one broadcast per cycle (default on)",
<                          "1");
<   option_parser_register(opp, "-gpgpu_shmem_warp_parts", OPT_INT32, &mem_warp_parts,
<                          "Number of portions a warp is divided into for shared memory bank conflict check ",
<                          "2");
<   option_parser_register(opp, "-gpgpu_warpdistro_shader", OPT_INT32, &gpgpu_warpdistro_shader,
<                          "Specify which shader core to collect the warp size distribution from",
<                          "-1");
<   option_parser_register(opp, "-gpgpu_warp_issue_shader", OPT_INT32, &gpgpu_warp_issue_shader,
<                          "Specify which shader core to collect the warp issue distribution from",
<                          "0");
<   option_parser_register(opp, "-gpgpu_local_mem_map", OPT_BOOL, &gpgpu_local_mem_map,
<                          "Mapping from local memory space address to simulated GPU physical address space (default = enabled)",
<                          "1");
<   option_parser_register(opp, "-gpgpu_num_reg_banks", OPT_INT32, &gpgpu_num_reg_banks,
<                          "Number of register banks (default = 8)",
<                          "8");
<   option_parser_register(opp, "-gpgpu_reg_bank_use_warp_id", OPT_BOOL, &gpgpu_reg_bank_use_warp_id,
<                          "Use warp ID in mapping registers to banks (default = off)",
<                          "0");
<   option_parser_register(opp, "-gpgpu_operand_collector_num_units_sp", OPT_INT32, &gpgpu_operand_collector_num_units_sp,
<                          "number of collector units (default = 4)",
<                          "4");
<   option_parser_register(opp, "-gpgpu_operand_collector_num_units_sfu", OPT_INT32, &gpgpu_operand_collector_num_units_sfu,
<                          "number of collector units (default = 4)",
<                          "4");
<   option_parser_register(opp, "-gpgpu_operand_collector_num_units_mem", OPT_INT32, &gpgpu_operand_collector_num_units_mem,
<                          "number of collector units (default = 2)",
<                          "2");
<   option_parser_register(opp, "-gpgpu_operand_collector_num_units_gen", OPT_INT32, &gpgpu_operand_collector_num_units_gen,
<                          "number of collector units (default = 0)",
<                          "0");
<   option_parser_register(opp, "-gpgpu_operand_collector_num_in_ports_sp", OPT_INT32, &gpgpu_operand_collector_num_in_ports_sp,
<                          "number of collector unit in ports (default = 1)",
<                          "1");
<   option_parser_register(opp, "-gpgpu_operand_collector_num_in_ports_sfu", OPT_INT32, &gpgpu_operand_collector_num_in_ports_sfu,
<                          "number of collector unit in ports (default = 1)",
<                          "1");
<   option_parser_register(opp, "-gpgpu_operand_collector_num_in_ports_mem", OPT_INT32, &gpgpu_operand_collector_num_in_ports_mem,
<                          "number of collector unit in ports (default = 1)",
<                          "1");
<   option_parser_register(opp, "-gpgpu_operand_collector_num_in_ports_gen", OPT_INT32, &gpgpu_operand_collector_num_in_ports_gen,
<                          "number of collector unit in ports (default = 0)",
<                          "0");
<   option_parser_register(opp, "-gpgpu_operand_collector_num_out_ports_sp", OPT_INT32, &gpgpu_operand_collector_num_out_ports_sp,
<                          "number of collector unit in ports (default = 1)",
<                          "1");
<   option_parser_register(opp, "-gpgpu_operand_collector_num_out_ports_sfu", OPT_INT32, &gpgpu_operand_collector_num_out_ports_sfu,
<                          "number of collector unit in ports (default = 1)",
<                          "1");
<   option_parser_register(opp, "-gpgpu_operand_collector_num_out_ports_mem", OPT_INT32, &gpgpu_operand_collector_num_out_ports_mem,
<                          "number of collector unit in ports (default = 1)",
<                          "1");
<   option_parser_register(opp, "-gpgpu_operand_collector_num_out_ports_gen", OPT_INT32, &gpgpu_operand_collector_num_out_ports_gen,
<                          "number of collector unit in ports (default = 0)",
<                          "0");
<   option_parser_register(opp, "-gpgpu_coalesce_arch", OPT_INT32, &gpgpu_coalesce_arch,
<                          "Coalescing arch (default = 13, anything else is off for now)",
<                          "13");
<   option_parser_register(opp, "-gpgpu_num_sched_per_core", OPT_INT32, &gpgpu_num_sched_per_core,
<                          "Number of warp schedulers per core",
<                          "1");
<   option_parser_register(opp, "-gpgpu_max_insn_issue_per_warp", OPT_INT32, &gpgpu_max_insn_issue_per_warp,
<                          "Max number of instructions that can be issued per warp in one cycle by scheduler",
<                          "2");
<   option_parser_register(opp, "-gpgpu_simt_core_sim_order", OPT_INT32, &simt_core_sim_order,
<                          "Select the simulation order of cores in a cluster (0=Fix, 1=Round-Robin)",
<                          "1");
<   option_parser_register(opp, "-gpgpu_pipeline_widths", OPT_CSTR, &pipeline_widths_string,
<                          "Pipeline widths "
<                          "ID_OC_SP,ID_OC_SFU,ID_OC_MEM,OC_EX_SP,OC_EX_SFU,OC_EX_MEM,EX_WB",
<                          "1,1,1,1,1,1,1" );
<   option_parser_register(opp, "-gpgpu_num_sp_units", OPT_INT32, &gpgpu_num_sp_units,
<                          "Number of SP units (default=1)",
<                          "1");
<   option_parser_register(opp, "-gpgpu_num_sfu_units", OPT_INT32, &gpgpu_num_sfu_units,
<                          "Number of SF units (default=1)",
<                          "1");
<   option_parser_register(opp, "-gpgpu_num_mem_units", OPT_INT32, &gpgpu_num_mem_units,
<                          "Number if ldst units (default=1) WARNING: not hooked up to anything",
<                          "1");
<   option_parser_register(opp, "-gpgpu_scheduler", OPT_CSTR, &gpgpu_scheduler_string,
<                          "Scheduler configuration: < lrr | gto | two_level_active > "
<                          "If two_level_active:<num_active_warps>:<inner_prioritization>:<outer_prioritization>"
<                          "For complete list of prioritization values see shader.h enum scheduler_prioritization_type"
<                          "Default: gto",
<                          "gto");
---
>     option_parser_register(opp, "-gpgpu_simd_model", OPT_INT32, &model, 
>                    "1 = post-dominator", "1");
>     option_parser_register(opp, "-gpgpu_shader_core_pipeline", OPT_CSTR, &gpgpu_shader_core_pipeline_opt, 
>                    "shader core pipeline config, i.e., {<nthread>:<warpsize>}",
>                    "1024:32");
>     option_parser_register(opp, "-gpgpu_tex_cache:l1", OPT_CSTR, &m_L1T_config.m_config_string, 
>                    "per-shader L1 texture cache  (READ-ONLY) config "
>                    " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>:<rf>}",
>                    "8:128:5,L:R:m:N,F:128:4,128:2");
>     option_parser_register(opp, "-gpgpu_const_cache:l1", OPT_CSTR, &m_L1C_config.m_config_string, 
>                    "per-shader L1 constant memory cache  (READ-ONLY) config "
>                    " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>} ",
>                    "64:64:2,L:R:f:N,A:2:32,4" );
>     option_parser_register(opp, "-gpgpu_cache:il1", OPT_CSTR, &m_L1I_config.m_config_string, 
>                    "shader L1 instruction cache config "
>                    " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq>} ",
>                    "4:256:4,L:R:f:N,A:2:32,4" );
>     option_parser_register(opp, "-gpgpu_cache:dl1", OPT_CSTR, &m_L1D_config.m_config_string,
>                    "per-shader L1 data cache config "
>                    " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq> | none}",
>                    "none" );
>     option_parser_register(opp, "-gpgpu_cache:dl1PrefL1", OPT_CSTR, &m_L1D_config.m_config_stringPrefL1,
>                    "per-shader L1 data cache config "
>                    " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq> | none}",
>                    "none" );
>     option_parser_register(opp, "-gpgpu_cache:dl1PreShared", OPT_CSTR, &m_L1D_config.m_config_stringPrefShared,
>                    "per-shader L1 data cache config "
>                    " {<nsets>:<bsize>:<assoc>,<rep>:<wr>:<alloc>:<wr_alloc>,<mshr>:<N>:<merge>,<mq> | none}",
>                    "none" );
>     option_parser_register(opp, "-gmem_skip_L1D", OPT_BOOL, &gmem_skip_L1D, 
>                    "global memory access skip L1D cache (implements -Xptxas -dlcm=cg, default=no skip)",
>                    "0");
> 
>     option_parser_register(opp, "-gpgpu_perfect_mem", OPT_BOOL, &gpgpu_perfect_mem, 
>                  "enable perfect memory mode (no cache miss)",
>                  "0");
>     option_parser_register(opp, "-n_regfile_gating_group", OPT_UINT32, &n_regfile_gating_group,
>                  "group of lanes that should be read/written together)",
>                  "4");
>     option_parser_register(opp, "-gpgpu_clock_gated_reg_file", OPT_BOOL, &gpgpu_clock_gated_reg_file,
>                  "enable clock gated reg file for power calculations",
>                  "0");
>     option_parser_register(opp, "-gpgpu_clock_gated_lanes", OPT_BOOL, &gpgpu_clock_gated_lanes,
>                  "enable clock gated lanes for power calculations",
>                  "0");
>     option_parser_register(opp, "-gpgpu_shader_registers", OPT_UINT32, &gpgpu_shader_registers, 
>                  "Number of registers per shader core. Limits number of concurrent CTAs. (default 8192)",
>                  "8192");
>     option_parser_register(opp, "-gpgpu_shader_cta", OPT_UINT32, &max_cta_per_core, 
>                  "Maximum number of concurrent CTAs in shader (default 8)",
>                  "8");
>     option_parser_register(opp, "-gpgpu_num_cta_barriers", OPT_UINT32, &max_barriers_per_cta,
>                  "Maximum number of named barriers per CTA (default 16)",
>                  "16");
>     option_parser_register(opp, "-gpgpu_n_clusters", OPT_UINT32, &n_simt_clusters, 
>                  "number of processing clusters",
>                  "10");
>     option_parser_register(opp, "-gpgpu_n_cores_per_cluster", OPT_UINT32, &n_simt_cores_per_cluster, 
>                  "number of simd cores per cluster",
>                  "3");
>     option_parser_register(opp, "-gpgpu_n_cluster_ejection_buffer_size", OPT_UINT32, &n_simt_ejection_buffer_size, 
>                  "number of packets in ejection buffer",
>                  "8");
>     option_parser_register(opp, "-gpgpu_n_ldst_response_buffer_size", OPT_UINT32, &ldst_unit_response_queue_size, 
>                  "number of response packets in ld/st unit ejection buffer",
>                  "2");
>     option_parser_register(opp, "-gpgpu_shmem_size", OPT_UINT32, &gpgpu_shmem_size,
>                  "Size of shared memory per shader core (default 16kB)",
>                  "16384");
>     option_parser_register(opp, "-gpgpu_shmem_size", OPT_UINT32, &gpgpu_shmem_sizeDefault,
>                  "Size of shared memory per shader core (default 16kB)",
>                  "16384");
>     option_parser_register(opp, "-gpgpu_shmem_size_PrefL1", OPT_UINT32, &gpgpu_shmem_sizePrefL1,
>                  "Size of shared memory per shader core (default 16kB)",
>                  "16384");
>     option_parser_register(opp, "-gpgpu_shmem_size_PrefShared", OPT_UINT32, &gpgpu_shmem_sizePrefShared,
>                  "Size of shared memory per shader core (default 16kB)",
>                  "16384");
>     option_parser_register(opp, "-gpgpu_shmem_num_banks", OPT_UINT32, &num_shmem_bank, 
>                  "Number of banks in the shared memory in each shader core (default 16)",
>                  "16");
>     option_parser_register(opp, "-gpgpu_shmem_limited_broadcast", OPT_BOOL, &shmem_limited_broadcast, 
>                  "Limit shared memory to do one broadcast per cycle (default on)",
>                  "1");
>     option_parser_register(opp, "-gpgpu_shmem_warp_parts", OPT_INT32, &mem_warp_parts,  
>                  "Number of portions a warp is divided into for shared memory bank conflict check ",
>                  "2");
>     option_parser_register(opp, "-gpgpu_warpdistro_shader", OPT_INT32, &gpgpu_warpdistro_shader, 
>                 "Specify which shader core to collect the warp size distribution from", 
>                 "-1");
>     option_parser_register(opp, "-gpgpu_warp_issue_shader", OPT_INT32, &gpgpu_warp_issue_shader, 
>                 "Specify which shader core to collect the warp issue distribution from", 
>                 "0");
>     option_parser_register(opp, "-gpgpu_local_mem_map", OPT_BOOL, &gpgpu_local_mem_map, 
>                 "Mapping from local memory space address to simulated GPU physical address space (default = enabled)", 
>                 "1");
>     option_parser_register(opp, "-gpgpu_num_reg_banks", OPT_INT32, &gpgpu_num_reg_banks, 
>                 "Number of register banks (default = 8)", 
>                 "8");
>     option_parser_register(opp, "-gpgpu_reg_bank_use_warp_id", OPT_BOOL, &gpgpu_reg_bank_use_warp_id,
>              "Use warp ID in mapping registers to banks (default = off)",
>              "0");
>     option_parser_register(opp, "-gpgpu_operand_collector_num_units_sp", OPT_INT32, &gpgpu_operand_collector_num_units_sp,
>                 "number of collector units (default = 4)", 
>                 "4");
>     option_parser_register(opp, "-gpgpu_operand_collector_num_units_sfu", OPT_INT32, &gpgpu_operand_collector_num_units_sfu,
>                 "number of collector units (default = 4)", 
>                 "4");
>     option_parser_register(opp, "-gpgpu_operand_collector_num_units_mem", OPT_INT32, &gpgpu_operand_collector_num_units_mem,
>                 "number of collector units (default = 2)", 
>                 "2");
>     option_parser_register(opp, "-gpgpu_operand_collector_num_units_gen", OPT_INT32, &gpgpu_operand_collector_num_units_gen,
>                 "number of collector units (default = 0)", 
>                 "0");
>     option_parser_register(opp, "-gpgpu_operand_collector_num_in_ports_sp", OPT_INT32, &gpgpu_operand_collector_num_in_ports_sp,
>                            "number of collector unit in ports (default = 1)", 
>                            "1");
>     option_parser_register(opp, "-gpgpu_operand_collector_num_in_ports_sfu", OPT_INT32, &gpgpu_operand_collector_num_in_ports_sfu,
>                            "number of collector unit in ports (default = 1)", 
>                            "1");
>     option_parser_register(opp, "-gpgpu_operand_collector_num_in_ports_mem", OPT_INT32, &gpgpu_operand_collector_num_in_ports_mem,
>                            "number of collector unit in ports (default = 1)", 
>                            "1");
>     option_parser_register(opp, "-gpgpu_operand_collector_num_in_ports_gen", OPT_INT32, &gpgpu_operand_collector_num_in_ports_gen,
>                            "number of collector unit in ports (default = 0)", 
>                            "0");
>     option_parser_register(opp, "-gpgpu_operand_collector_num_out_ports_sp", OPT_INT32, &gpgpu_operand_collector_num_out_ports_sp,
>                            "number of collector unit in ports (default = 1)", 
>                            "1");
>     option_parser_register(opp, "-gpgpu_operand_collector_num_out_ports_sfu", OPT_INT32, &gpgpu_operand_collector_num_out_ports_sfu,
>                            "number of collector unit in ports (default = 1)", 
>                            "1");
>     option_parser_register(opp, "-gpgpu_operand_collector_num_out_ports_mem", OPT_INT32, &gpgpu_operand_collector_num_out_ports_mem,
>                            "number of collector unit in ports (default = 1)", 
>                            "1");
>     option_parser_register(opp, "-gpgpu_operand_collector_num_out_ports_gen", OPT_INT32, &gpgpu_operand_collector_num_out_ports_gen,
>                            "number of collector unit in ports (default = 0)", 
>                            "0");
>     option_parser_register(opp, "-gpgpu_coalesce_arch", OPT_INT32, &gpgpu_coalesce_arch, 
>                             "Coalescing arch (default = 13, anything else is off for now)", 
>                             "13");
>     option_parser_register(opp, "-gpgpu_num_sched_per_core", OPT_INT32, &gpgpu_num_sched_per_core, 
>                             "Number of warp schedulers per core", 
>                             "1");
>     option_parser_register(opp, "-gpgpu_max_insn_issue_per_warp", OPT_INT32, &gpgpu_max_insn_issue_per_warp,
>                             "Max number of instructions that can be issued per warp in one cycle by scheduler",
>                             "2");
>     option_parser_register(opp, "-gpgpu_simt_core_sim_order", OPT_INT32, &simt_core_sim_order,
>                             "Select the simulation order of cores in a cluster (0=Fix, 1=Round-Robin)",
>                             "1");
>     option_parser_register(opp, "-gpgpu_pipeline_widths", OPT_CSTR, &pipeline_widths_string,
>                             "Pipeline widths "
>                             "ID_OC_SP,ID_OC_SFU,ID_OC_MEM,OC_EX_SP,OC_EX_SFU,OC_EX_MEM,EX_WB",
>                             "1,1,1,1,1,1,1" );
>     option_parser_register(opp, "-gpgpu_num_sp_units", OPT_INT32, &gpgpu_num_sp_units,
>                             "Number of SP units (default=1)",
>                             "1");
>     option_parser_register(opp, "-gpgpu_num_sfu_units", OPT_INT32, &gpgpu_num_sfu_units,
>                             "Number of SF units (default=1)",
>                             "1");
>     option_parser_register(opp, "-gpgpu_num_mem_units", OPT_INT32, &gpgpu_num_mem_units,
>                             "Number if ldst units (default=1) WARNING: not hooked up to anything",
>                              "1");
>     option_parser_register(opp, "-gpgpu_scheduler", OPT_CSTR, &gpgpu_scheduler_string,
>                                 "Scheduler configuration: < lrr | gto | two_level_active > "
>                                 "If two_level_active:<num_active_warps>:<inner_prioritization>:<outer_prioritization>"
>                                 "For complete list of prioritization values see shader.h enum scheduler_prioritization_type"
>                                 "Default: gto",
>                                  "gto");
374,441c373,440
<   gpgpu_functional_sim_config::reg_options(opp);
<   m_shader_config.reg_options(opp);
<   m_memory_config.reg_options(opp);
<   power_config::reg_options(opp);
<   option_parser_register(opp, "-gpgpu_max_cycle", OPT_INT32, &gpu_max_cycle_opt,
<                          "terminates gpu simulation early (0 = no limit)",
<                          "0");
<   option_parser_register(opp, "-gpgpu_max_insn", OPT_INT32, &gpu_max_insn_opt,
<                          "terminates gpu simulation early (0 = no limit)",
<                          "0");
<   option_parser_register(opp, "-gpgpu_max_cta", OPT_INT32, &gpu_max_cta_opt,
<                          "terminates gpu simulation early (0 = no limit)",
<                          "0");
<   option_parser_register(opp, "-gpgpu_runtime_stat", OPT_CSTR, &gpgpu_runtime_stat,
<                          "display runtime statistics such as dram utilization {<freq>:<flag>}",
<                          "10000:0");
<   option_parser_register(opp, "-liveness_message_freq", OPT_INT64, &liveness_message_freq,
<                          "Minimum number of seconds between simulation liveness messages (0 = always print)",
<                          "1");
<   option_parser_register(opp, "-gpgpu_flush_l1_cache", OPT_BOOL, &gpgpu_flush_l1_cache,
<                          "Flush L1 cache at the end of each kernel call",
<                          "0");
<   option_parser_register(opp, "-gpgpu_flush_l2_cache", OPT_BOOL, &gpgpu_flush_l2_cache,
<                          "Flush L2 cache at the end of each kernel call",
<                          "0");
< 
<   option_parser_register(opp, "-gpgpu_deadlock_detect", OPT_BOOL, &gpu_deadlock_detect,
<                          "Stop the simulation at deadlock (1=on (default), 0=off)",
<                          "1");
<   option_parser_register(opp, "-gpgpu_ptx_instruction_classification", OPT_INT32,
<                          &gpgpu_ptx_instruction_classification,
<                          "if enabled will classify ptx instruction types per kernel (Max 255 kernels now)",
<                          "0");
<   option_parser_register(opp, "-gpgpu_ptx_sim_mode", OPT_INT32, &g_ptx_sim_mode,
<                          "Select between Performance (default) or Functional simulation (1)",
<                          "0");
<   option_parser_register(opp, "-gpgpu_clock_domains", OPT_CSTR, &gpgpu_clock_domains,
<                          "Clock Domain Frequencies in MhZ {<Core Clock>:<ICNT Clock>:<L2 Clock>:<DRAM Clock>}",
<                          "500.0:2000.0:2000.0:2000.0");
<   option_parser_register(opp, "-gpgpu_max_concurrent_kernel", OPT_INT32, &max_concurrent_kernel,
<                          "maximum kernels that can run concurrently on GPU", "8" );
<   option_parser_register(opp, "-gpgpu_cflog_interval", OPT_INT32, &gpgpu_cflog_interval,
<                          "Interval between each snapshot in control flow logger",
<                          "0");
<   option_parser_register(opp, "-visualizer_enabled", OPT_BOOL,
<                          &g_visualizer_enabled, "Turn on visualizer output (1=On, 0=Off)",
<                          "1");
<   option_parser_register(opp, "-visualizer_outputfile", OPT_CSTR,
<                          &g_visualizer_filename, "Specifies the output log file for visualizer",
<                          NULL);
<   option_parser_register(opp, "-visualizer_zlevel", OPT_INT32,
<                          &g_visualizer_zlevel, "Compression level of the visualizer output log (0=no comp, 9=highest)",
<                          "6");
<   option_parser_register(opp, "-trace_enabled", OPT_BOOL,
<                          &Trace::enabled, "Turn on traces",
<                          "0");
<   option_parser_register(opp, "-trace_components", OPT_CSTR,
<                          &Trace::config_str, "comma seperated list of traces to enable. "
<                          "Complete list found in trace_streams.tup. "
<                          "Default none",
<                          "none");
<   option_parser_register(opp, "-trace_sampling_core", OPT_INT32,
<                          &Trace::sampling_core, "The core which is printed using CORE_DPRINTF. Default 0",
<                          "0");
<   option_parser_register(opp, "-trace_sampling_memory_partition", OPT_INT32,
<                          &Trace::sampling_memory_partition, "The memory partition which is printed using MEMPART_DPRINTF. Default -1 (i.e. all)",
<                          "-1");
<   ptx_file_line_stats_options(opp);
---
>     gpgpu_functional_sim_config::reg_options(opp);
>     m_shader_config.reg_options(opp);
>     m_memory_config.reg_options(opp);
>     power_config::reg_options(opp);
>    option_parser_register(opp, "-gpgpu_max_cycle", OPT_INT32, &gpu_max_cycle_opt, 
>                "terminates gpu simulation early (0 = no limit)",
>                "0");
>    option_parser_register(opp, "-gpgpu_max_insn", OPT_INT32, &gpu_max_insn_opt, 
>                "terminates gpu simulation early (0 = no limit)",
>                "0");
>    option_parser_register(opp, "-gpgpu_max_cta", OPT_INT32, &gpu_max_cta_opt, 
>                "terminates gpu simulation early (0 = no limit)",
>                "0");
>    option_parser_register(opp, "-gpgpu_runtime_stat", OPT_CSTR, &gpgpu_runtime_stat, 
>                   "display runtime statistics such as dram utilization {<freq>:<flag>}",
>                   "10000:0");
>    option_parser_register(opp, "-liveness_message_freq", OPT_INT64, &liveness_message_freq, 
>                "Minimum number of seconds between simulation liveness messages (0 = always print)",
>                "1");
>    option_parser_register(opp, "-gpgpu_flush_l1_cache", OPT_BOOL, &gpgpu_flush_l1_cache,
>                 "Flush L1 cache at the end of each kernel call",
>                 "0");
>    option_parser_register(opp, "-gpgpu_flush_l2_cache", OPT_BOOL, &gpgpu_flush_l2_cache,
>                    "Flush L2 cache at the end of each kernel call",
>                    "0");
> 
>    option_parser_register(opp, "-gpgpu_deadlock_detect", OPT_BOOL, &gpu_deadlock_detect, 
>                 "Stop the simulation at deadlock (1=on (default), 0=off)", 
>                 "1");
>    option_parser_register(opp, "-gpgpu_ptx_instruction_classification", OPT_INT32, 
>                &gpgpu_ptx_instruction_classification, 
>                "if enabled will classify ptx instruction types per kernel (Max 255 kernels now)", 
>                "0");
>    option_parser_register(opp, "-gpgpu_ptx_sim_mode", OPT_INT32, &g_ptx_sim_mode, 
>                "Select between Performance (default) or Functional simulation (1)", 
>                "0");
>    option_parser_register(opp, "-gpgpu_clock_domains", OPT_CSTR, &gpgpu_clock_domains, 
>                   "Clock Domain Frequencies in MhZ {<Core Clock>:<ICNT Clock>:<L2 Clock>:<DRAM Clock>}",
>                   "500.0:2000.0:2000.0:2000.0");
>    option_parser_register(opp, "-gpgpu_max_concurrent_kernel", OPT_INT32, &max_concurrent_kernel,
>                           "maximum kernels that can run concurrently on GPU", "8" );
>    option_parser_register(opp, "-gpgpu_cflog_interval", OPT_INT32, &gpgpu_cflog_interval, 
>                "Interval between each snapshot in control flow logger", 
>                "0");
>    option_parser_register(opp, "-visualizer_enabled", OPT_BOOL,
>                           &g_visualizer_enabled, "Turn on visualizer output (1=On, 0=Off)",
>                           "1");
>    option_parser_register(opp, "-visualizer_outputfile", OPT_CSTR, 
>                           &g_visualizer_filename, "Specifies the output log file for visualizer",
>                           NULL);
>    option_parser_register(opp, "-visualizer_zlevel", OPT_INT32,
>                           &g_visualizer_zlevel, "Compression level of the visualizer output log (0=no comp, 9=highest)",
>                           "6");
>     option_parser_register(opp, "-trace_enabled", OPT_BOOL, 
>                           &Trace::enabled, "Turn on traces",
>                           "0");
>     option_parser_register(opp, "-trace_components", OPT_CSTR, 
>                           &Trace::config_str, "comma seperated list of traces to enable. "
>                           "Complete list found in trace_streams.tup. "
>                           "Default none",
>                           "none");
>     option_parser_register(opp, "-trace_sampling_core", OPT_INT32, 
>                           &Trace::sampling_core, "The core which is printed using CORE_DPRINTF. Default 0",
>                           "0");
>     option_parser_register(opp, "-trace_sampling_memory_partition", OPT_INT32, 
>                           &Trace::sampling_memory_partition, "The memory partition which is printed using MEMPART_DPRINTF. Default -1 (i.e. all)",
>                           "-1");
>    ptx_file_line_stats_options(opp);
448,457c447,456
<   i.x++;
<   if ( i.x >= bound.x ) {
<     i.x = 0;
<     i.y++;
<     if ( i.y >= bound.y ) {
<       i.y = 0;
<       if ( i.z < bound.z )
<         i.z++;
<     }
<   }
---
>    i.x++;
>    if ( i.x >= bound.x ) {
>       i.x = 0;
>       i.y++;
>       if ( i.y >= bound.y ) {
>          i.y = 0;
>          if( i.z < bound.z ) 
>             i.z++;
>       }
>    }
462,478c461,477
<   unsigned cta_size = kinfo->threads_per_cta();
<   if ( cta_size > m_shader_config->n_thread_per_shader ) {
<     printf("Execution error: Shader kernel CTA (block) size is too large for microarch config.\n");
<     printf("                 CTA size (x*y*z) = %u, max supported = %u\n", cta_size,
<            m_shader_config->n_thread_per_shader );
<     printf("                 => either change -gpgpu_shader argument in gpgpusim.config file or\n");
<     printf("                 modify the CUDA source to decrease the kernel block size.\n");
<     abort();
<   }
<   unsigned n = 0;
<   for (n = 0; n < m_running_kernels.size(); n++ ) {
<     if ( (NULL == m_running_kernels[n]) || m_running_kernels[n]->done() ) {
<       m_running_kernels[n] = kinfo;
<       break;
<     }
<   }
<   assert(n < m_running_kernels.size());
---
>    unsigned cta_size = kinfo->threads_per_cta();
>    if ( cta_size > m_shader_config->n_thread_per_shader ) {
>       printf("Execution error: Shader kernel CTA (block) size is too large for microarch config.\n");
>       printf("                 CTA size (x*y*z) = %u, max supported = %u\n", cta_size, 
>              m_shader_config->n_thread_per_shader );
>       printf("                 => either change -gpgpu_shader argument in gpgpusim.config file or\n");
>       printf("                 modify the CUDA source to decrease the kernel block size.\n");
>       abort();
>    }
>    unsigned n=0;
>    for(n=0; n < m_running_kernels.size(); n++ ) {
>        if( (NULL==m_running_kernels[n]) || m_running_kernels[n]->done() ) {
>            m_running_kernels[n] = kinfo;
>            break;
>        }
>    }
>    assert(n < m_running_kernels.size());
483,487c482,486
<   for (unsigned n = 0; n < m_running_kernels.size(); n++ ) {
<     if ( (NULL == m_running_kernels[n]) || m_running_kernels[n]->done() )
<       return true;
<   }
<   return false;
---
>    for(unsigned n=0; n < m_running_kernels.size(); n++ ) {
>        if( (NULL==m_running_kernels[n]) || m_running_kernels[n]->done() ) 
>            return true;
>    }
>    return false;
491,500c490,499
< {
<   if (m_config.gpu_max_cta_opt != 0) {
<     if ( m_total_cta_launched >= m_config.gpu_max_cta_opt )
<       return false;
<   }
<   for (unsigned n = 0; n < m_running_kernels.size(); n++ ) {
<     if ( m_running_kernels[n] && !m_running_kernels[n]->no_more_ctas_to_run() )
<       return true;
<   }
<   return false;
---
> { 
>    if (m_config.gpu_max_cta_opt != 0) {
>       if( m_total_cta_launched >= m_config.gpu_max_cta_opt )
>           return false;
>    }
>    for(unsigned n=0; n < m_running_kernels.size(); n++ ) {
>        if( m_running_kernels[n] && !m_running_kernels[n]->no_more_ctas_to_run() ) 
>            return true;
>    }
>    return false;
505,514c504,513
<   for (unsigned n = 0; n < m_running_kernels.size(); n++ ) {
<     unsigned idx = (n + m_last_issued_kernel + 1) % m_config.max_concurrent_kernel;
<     if ( m_running_kernels[idx] && !m_running_kernels[idx]->no_more_ctas_to_run() ) {
<       m_last_issued_kernel = idx;
<       // record this kernel for stat print if it is the first time this kernel is selected for execution
<       unsigned launch_uid = m_running_kernels[idx]->get_uid();
<       if (std::find(m_executed_kernel_uids.begin(), m_executed_kernel_uids.end(), launch_uid) == m_executed_kernel_uids.end()) {
<         m_executed_kernel_uids.push_back(launch_uid);
<         m_executed_kernel_names.push_back(m_running_kernels[idx]->name());
<       }
---
>     for(unsigned n=0; n < m_running_kernels.size(); n++ ) {
>         unsigned idx = (n+m_last_issued_kernel+1)%m_config.max_concurrent_kernel;
>         if( m_running_kernels[idx] && !m_running_kernels[idx]->no_more_ctas_to_run() ) {
>             m_last_issued_kernel=idx;
>             // record this kernel for stat print if it is the first time this kernel is selected for execution  
>             unsigned launch_uid = m_running_kernels[idx]->get_uid(); 
>             if (std::find(m_executed_kernel_uids.begin(), m_executed_kernel_uids.end(), launch_uid) == m_executed_kernel_uids.end()) {
>                m_executed_kernel_uids.push_back(launch_uid); 
>                m_executed_kernel_names.push_back(m_running_kernels[idx]->name()); 
>             }
516c515,516
<       return m_running_kernels[idx];
---
>             return m_running_kernels[idx];
>         }
518,519c518
<   }
<   return NULL;
---
>     return NULL;
524,539c523,539
<   if ( m_finished_kernel.empty() )
<     return 0;
<   unsigned result = m_finished_kernel.front();
<   m_finished_kernel.pop_front();
<   return result;
< }
< 
< void gpgpu_sim::set_kernel_done( kernel_info_t *kernel )
< {
<   unsigned uid = kernel->get_uid();
<   m_finished_kernel.push_back(uid);
<   std::vector<kernel_info_t*>::iterator k;
<   for ( k = m_running_kernels.begin(); k != m_running_kernels.end(); k++ ) {
<     if ( *k == kernel ) {
<       *k = NULL;
<       break;
---
>     if( m_finished_kernel.empty() ) 
>         return 0;
>     unsigned result = m_finished_kernel.front();
>     m_finished_kernel.pop_front();
>     return result;
> }
> 
> void gpgpu_sim::set_kernel_done( kernel_info_t *kernel ) 
> { 
>     unsigned uid = kernel->get_uid();
>     m_finished_kernel.push_back(uid);
>     std::vector<kernel_info_t*>::iterator k;
>     for( k=m_running_kernels.begin(); k!=m_running_kernels.end(); k++ ) {
>         if( *k == kernel ) {
>             *k = NULL;
>             break;
>         }
541,542c541
<   }
<   assert( k != m_running_kernels.end() );
---
>     assert( k != m_running_kernels.end() ); 
547,553c546,552
< gpgpu_sim::gpgpu_sim( const gpgpu_sim_config &config )
<   : gpgpu_t(config), m_config(config)
< {
<   m_shader_config = &m_config.m_shader_config;
<   m_memory_config = &m_config.m_memory_config;
<   set_ptx_warp_size(m_shader_config);
<   ptx_file_line_stats_create_exposed_latency_tracker(m_config.num_shader());
---
> gpgpu_sim::gpgpu_sim( const gpgpu_sim_config &config ) 
>     : gpgpu_t(config), m_config(config)
> { 
>     m_shader_config = &m_config.m_shader_config;
>     m_memory_config = &m_config.m_memory_config;
>     set_ptx_warp_size(m_shader_config);
>     ptx_file_line_stats_create_exposed_latency_tracker(m_config.num_shader());
556c555
<   m_gpgpusim_wrapper = new gpgpu_sim_wrapper(config.g_power_simulation_enabled, config.g_power_config_name);
---
>         m_gpgpusim_wrapper = new gpgpu_sim_wrapper(config.g_power_simulation_enabled,config.g_power_config_name);
559,582c558,581
<   m_shader_stats = new shader_core_stats(m_shader_config);
<   m_memory_stats = new memory_stats_t(m_config.num_shader(), m_shader_config, m_memory_config);
<   average_pipeline_duty_cycle = (float *)malloc(sizeof(float));
<   active_sms = (float *)malloc(sizeof(float));
<   m_power_stats = new power_stat_t(m_shader_config, average_pipeline_duty_cycle, active_sms, m_shader_stats, m_memory_config, m_memory_stats);
< 
<   gpu_sim_insn = 0;
<   gpu_tot_sim_insn = 0;
<   gpu_tot_issued_cta = 0;
<   gpu_deadlock = false;
< 
< 
<   m_cluster = new simt_core_cluster*[m_shader_config->n_simt_clusters];
<   core_numbers = m_shader_config->n_simt_clusters;
<   for (unsigned i = 0; i < m_shader_config->n_simt_clusters; i++)
<     m_cluster[i] = new simt_core_cluster(this, i, m_shader_config, m_memory_config, m_shader_stats, m_memory_stats);
< 
<   m_memory_partition_unit = new memory_partition_unit*[m_memory_config->m_n_mem];
<   m_memory_sub_partition = new memory_sub_partition*[m_memory_config->m_n_mem_sub_partition];
<   for (unsigned i = 0; i < m_memory_config->m_n_mem; i++) {
<     m_memory_partition_unit[i] = new memory_partition_unit(i, m_memory_config, m_memory_stats);
<     for (unsigned p = 0; p < m_memory_config->m_n_sub_partition_per_memory_channel; p++) {
<       unsigned submpid = i * m_memory_config->m_n_sub_partition_per_memory_channel + p;
<       m_memory_sub_partition[submpid] = m_memory_partition_unit[i]->get_sub_partition(p);
---
>     m_shader_stats = new shader_core_stats(m_shader_config);
>     m_memory_stats = new memory_stats_t(m_config.num_shader(),m_shader_config,m_memory_config);
>     average_pipeline_duty_cycle = (float *)malloc(sizeof(float));
>     active_sms=(float *)malloc(sizeof(float));
>     m_power_stats = new power_stat_t(m_shader_config,average_pipeline_duty_cycle,active_sms,m_shader_stats,m_memory_config,m_memory_stats);
> 
>     gpu_sim_insn = 0;
>     gpu_tot_sim_insn = 0;
>     gpu_tot_issued_cta = 0;
>     gpu_deadlock = false;
> 
> 
>     m_cluster = new simt_core_cluster*[m_shader_config->n_simt_clusters];
>     for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) 
>         m_cluster[i] = new simt_core_cluster(this,i,m_shader_config,m_memory_config,m_shader_stats,m_memory_stats);
> 
>     m_memory_partition_unit = new memory_partition_unit*[m_memory_config->m_n_mem];
>     m_memory_sub_partition = new memory_sub_partition*[m_memory_config->m_n_mem_sub_partition];
>     for (unsigned i=0;i<m_memory_config->m_n_mem;i++) {
>         m_memory_partition_unit[i] = new memory_partition_unit(i, m_memory_config, m_memory_stats);
>         for (unsigned p = 0; p < m_memory_config->m_n_sub_partition_per_memory_channel; p++) {
>             unsigned submpid = i * m_memory_config->m_n_sub_partition_per_memory_channel + p; 
>             m_memory_sub_partition[submpid] = m_memory_partition_unit[i]->get_sub_partition(p); 
>         }
584d582
<   }
586,587c584,585
<   icnt_wrapper_init();
<   icnt_create(m_shader_config->n_simt_clusters, m_memory_config->m_n_mem_sub_partition);
---
>     icnt_wrapper_init();
>     icnt_create(m_shader_config->n_simt_clusters,m_memory_config->m_n_mem_sub_partition);
589,590c587,588
<   time_vector_create(NUM_MEM_REQ_STAT);
<   fprintf(stdout, "GPGPU-Sim uArch: performance model initialization complete.\n");
---
>     time_vector_create(NUM_MEM_REQ_STAT);
>     fprintf(stdout, "GPGPU-Sim uArch: performance model initialization complete.\n");
592,596c590,594
<   m_running_kernels.resize( config.max_concurrent_kernel, NULL );
<   m_last_issued_kernel = 0;
<   m_last_cluster_issue = 0;
<   *average_pipeline_duty_cycle = 0;
<   *active_sms = 0;
---
>     m_running_kernels.resize( config.max_concurrent_kernel, NULL );
>     m_last_issued_kernel = 0;
>     m_last_cluster_issue = 0;
>     *average_pipeline_duty_cycle=0;
>     *active_sms=0;
598c596
<   last_liveness_message_time = 0;
---
>     last_liveness_message_time = 0;
603c601
<   return m_shader_config->gpgpu_shmem_size;
---
>    return m_shader_config->gpgpu_shmem_size;
608c606
<   return m_shader_config->gpgpu_shader_registers;
---
>    return m_shader_config->gpgpu_shader_registers;
613c611
<   return m_shader_config->warp_size;
---
>    return m_shader_config->warp_size;
618c616
<   return m_config.core_freq / 1000;
---
>    return m_config.core_freq/1000;
623c621
<   m_cuda_properties = prop;
---
>    m_cuda_properties = prop;
628c626
<   return m_cuda_properties;
---
>    return m_cuda_properties;
633c631
<   return m_shader_config->model;
---
>    return m_shader_config->model;
636c634
< void gpgpu_sim_config::init_clock_domains(void )
---
> void gpgpu_sim_config::init_clock_domains(void ) 
638,649c636,647
<   sscanf(gpgpu_clock_domains, "%lf:%lf:%lf:%lf",
<          &core_freq, &icnt_freq, &l2_freq, &dram_freq);
<   core_freq = core_freq MhZ;
<   icnt_freq = icnt_freq MhZ;
<   l2_freq = l2_freq MhZ;
<   dram_freq = dram_freq MhZ;
<   core_period = 1 / core_freq;
<   icnt_period = 1 / icnt_freq;
<   dram_period = 1 / dram_freq;
<   l2_period = 1 / l2_freq;
<   printf("GPGPU-Sim uArch: clock freqs: %lf:%lf:%lf:%lf\n", core_freq, icnt_freq, l2_freq, dram_freq);
<   printf("GPGPU-Sim uArch: clock periods: %.20lf:%.20lf:%.20lf:%.20lf\n", core_period, icnt_period, l2_period, dram_period);
---
>    sscanf(gpgpu_clock_domains,"%lf:%lf:%lf:%lf", 
>           &core_freq, &icnt_freq, &l2_freq, &dram_freq);
>    core_freq = core_freq MhZ;
>    icnt_freq = icnt_freq MhZ;
>    l2_freq = l2_freq MhZ;
>    dram_freq = dram_freq MhZ;        
>    core_period = 1/core_freq;
>    icnt_period = 1/icnt_freq;
>    dram_period = 1/dram_freq;
>    l2_period = 1/l2_freq;
>    printf("GPGPU-Sim uArch: clock freqs: %lf:%lf:%lf:%lf\n",core_freq,icnt_freq,l2_freq,dram_freq);
>    printf("GPGPU-Sim uArch: clock periods: %.20lf:%.20lf:%.20lf:%.20lf\n",core_period,icnt_period,l2_period,dram_period);
654,657c652,655
<   core_time = 0;
<   dram_time = 0;
<   icnt_time = 0;
<   l2_time = 0;
---
>    core_time = 0;
>    dram_time = 0;
>    icnt_time = 0;
>    l2_time = 0;
662,666c660,677
<   if (m_config.gpu_max_cycle_opt && (gpu_tot_sim_cycle + gpu_sim_cycle) >= m_config.gpu_max_cycle_opt)
<     return false;
<   if (m_config.gpu_max_insn_opt && (gpu_tot_sim_insn + gpu_sim_insn) >= m_config.gpu_max_insn_opt)
<     return false;
<   if (m_config.gpu_max_cta_opt && (gpu_tot_issued_cta >= m_config.gpu_max_cta_opt) )
---
>     if (m_config.gpu_max_cycle_opt && (gpu_tot_sim_cycle + gpu_sim_cycle) >= m_config.gpu_max_cycle_opt) 
>        return false;
>     if (m_config.gpu_max_insn_opt && (gpu_tot_sim_insn + gpu_sim_insn) >= m_config.gpu_max_insn_opt) 
>        return false;
>     if (m_config.gpu_max_cta_opt && (gpu_tot_issued_cta >= m_config.gpu_max_cta_opt) )
>        return false;
>     if (m_config.gpu_deadlock_detect && gpu_deadlock) 
>        return false;
>     for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) 
>        if( m_cluster[i]->get_not_completed()>0 ) 
>            return true;;
>     for (unsigned i=0;i<m_memory_config->m_n_mem;i++) 
>        if( m_memory_partition_unit[i]->busy()>0 )
>            return true;;
>     if( icnt_busy() )
>         return true;
>     if( get_more_cta_left() )
>         return true;
668,680d678
<   if (m_config.gpu_deadlock_detect && gpu_deadlock)
<     return false;
<   for (unsigned i = 0; i < m_shader_config->n_simt_clusters; i++)
<     if ( m_cluster[i]->get_not_completed() > 0 )
<       return true;;
<   for (unsigned i = 0; i < m_memory_config->m_n_mem; i++)
<     if ( m_memory_partition_unit[i]->busy() > 0 )
<       return true;;
<   if ( icnt_busy() )
<     return true;
<   if ( get_more_cta_left() )
<     return true;
<   return false;
685,708c683,706
<   // run a CUDA grid on the GPU microarchitecture simulator
<   gpu_sim_cycle = 0;
<   gpu_sim_insn = 0;
<   last_gpu_sim_insn = 0;
<   m_total_cta_launched = 0;
< 
<   reinit_clock_domains();
<   set_param_gpgpu_num_shaders(m_config.num_shader());
<   for (unsigned i = 0; i < m_shader_config->n_simt_clusters; i++)
<     m_cluster[i]->reinit();
<   m_shader_stats->new_grid();
<   // initialize the control-flow, memory access, memory latency logger
<   if (m_config.g_visualizer_enabled) {
<     create_thread_CFlogger( m_config.num_shader(), m_shader_config->n_thread_per_shader, 0, m_config.gpgpu_cflog_interval );
<   }
<   shader_CTA_count_create( m_config.num_shader(), m_config.gpgpu_cflog_interval);
<   if (m_config.gpgpu_cflog_interval != 0) {
<     insn_warp_occ_create( m_config.num_shader(), m_shader_config->warp_size );
<     shader_warp_occ_create( m_config.num_shader(), m_shader_config->warp_size, m_config.gpgpu_cflog_interval);
<     shader_mem_acc_create( m_config.num_shader(), m_memory_config->m_n_mem, 4, m_config.gpgpu_cflog_interval);
<     shader_mem_lat_create( m_config.num_shader(), m_config.gpgpu_cflog_interval);
<     shader_cache_access_create( m_config.num_shader(), 3, m_config.gpgpu_cflog_interval);
<     set_spill_interval (m_config.gpgpu_cflog_interval * 40);
<   }
---
>     // run a CUDA grid on the GPU microarchitecture simulator
>     gpu_sim_cycle = 0;
>     gpu_sim_insn = 0;
>     last_gpu_sim_insn = 0;
>     m_total_cta_launched=0;
> 
>     reinit_clock_domains();
>     set_param_gpgpu_num_shaders(m_config.num_shader());
>     for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) 
>        m_cluster[i]->reinit();
>     m_shader_stats->new_grid();
>     // initialize the control-flow, memory access, memory latency logger
>     if (m_config.g_visualizer_enabled) {
>         create_thread_CFlogger( m_config.num_shader(), m_shader_config->n_thread_per_shader, 0, m_config.gpgpu_cflog_interval );
>     }
>     shader_CTA_count_create( m_config.num_shader(), m_config.gpgpu_cflog_interval);
>     if (m_config.gpgpu_cflog_interval != 0) {
>        insn_warp_occ_create( m_config.num_shader(), m_shader_config->warp_size );
>        shader_warp_occ_create( m_config.num_shader(), m_shader_config->warp_size, m_config.gpgpu_cflog_interval);
>        shader_mem_acc_create( m_config.num_shader(), m_memory_config->m_n_mem, 4, m_config.gpgpu_cflog_interval);
>        shader_mem_lat_create( m_config.num_shader(), m_config.gpgpu_cflog_interval);
>        shader_cache_access_create( m_config.num_shader(), 3, m_config.gpgpu_cflog_interval);
>        set_spill_interval (m_config.gpgpu_cflog_interval * 40);
>     }
710,711c708,709
<   if (g_network_mode)
<     icnt_init();
---
>     if (g_network_mode)
>        icnt_init();
713c711
<   // McPAT initialization function. Called on first launch of GPU
---
>     // McPAT initialization function. Called on first launch of GPU
715,717c713,715
<   if (m_config.g_power_simulation_enabled) {
<     init_mcpat(m_config, m_gpgpusim_wrapper, m_config.gpu_stat_sample_freq,  gpu_tot_sim_insn, gpu_sim_insn);
<   }
---
>     if(m_config.g_power_simulation_enabled){
>         init_mcpat(m_config, m_gpgpusim_wrapper, m_config.gpu_stat_sample_freq,  gpu_tot_sim_insn, gpu_sim_insn);
>     }
722,724c720,722
<   m_memory_stats->memlatstat_lat_pw();
<   gpu_tot_sim_cycle += gpu_sim_cycle;
<   gpu_tot_sim_insn += gpu_sim_insn;
---
>     m_memory_stats->memlatstat_lat_pw();
>     gpu_tot_sim_cycle += gpu_sim_cycle;
>     gpu_tot_sim_insn += gpu_sim_insn;
729,730c727,728
<   ptx_file_line_stats_write_file();
<   gpu_print_stat();
---
>     ptx_file_line_stats_write_file();
>     gpu_print_stat();
732,737c730,735
<   if (g_network_mode) {
<     printf("----------------------------Interconnect-DETAILS--------------------------------\n" );
<     icnt_display_stats();
<     icnt_display_overall_stats();
<     printf("----------------------------END-of-Interconnect-DETAILS-------------------------\n" );
<   }
---
>     if (g_network_mode) {
>         printf("----------------------------Interconnect-DETAILS--------------------------------\n" );
>         icnt_display_stats();
>         icnt_display_overall_stats();
>         printf("----------------------------END-of-Interconnect-DETAILS-------------------------\n" );
>     }
742,761c740,760
<   if (m_config.gpu_deadlock_detect && gpu_deadlock) {
<     fflush(stdout);
<     printf("\n\nGPGPU-Sim uArch: ERROR ** deadlock detected: last writeback core %u @ gpu_sim_cycle %u (+ gpu_tot_sim_cycle %u) (%u cycles ago)\n",
<            gpu_sim_insn_last_update_sid,
<            (unsigned) gpu_sim_insn_last_update, (unsigned) (gpu_tot_sim_cycle - gpu_sim_cycle),
<            (unsigned) (gpu_sim_cycle - gpu_sim_insn_last_update ));
<     unsigned num_cores = 0;
<     for (unsigned i = 0; i < m_shader_config->n_simt_clusters; i++) {
<       unsigned not_completed = m_cluster[i]->get_not_completed();
<       if ( not_completed ) {
<         if ( !num_cores )  {
<           printf("GPGPU-Sim uArch: DEADLOCK  shader cores no longer committing instructions [core(# threads)]:\n" );
<           printf("GPGPU-Sim uArch: DEADLOCK  ");
<           m_cluster[i]->print_not_completed(stdout);
<         } else if (num_cores < 8 ) {
<           m_cluster[i]->print_not_completed(stdout);
<         } else if (num_cores >= 8 ) {
<           printf(" + others ... ");
<         }
<         num_cores += m_shader_config->n_simt_cores_per_cluster;
---
>    if (m_config.gpu_deadlock_detect && gpu_deadlock) {
>       fflush(stdout);
>       printf("\n\nGPGPU-Sim uArch: ERROR ** deadlock detected: last writeback core %u @ gpu_sim_cycle %u (+ gpu_tot_sim_cycle %u) (%u cycles ago)\n", 
>              gpu_sim_insn_last_update_sid,
>              (unsigned) gpu_sim_insn_last_update, (unsigned) (gpu_tot_sim_cycle-gpu_sim_cycle),
>              (unsigned) (gpu_sim_cycle - gpu_sim_insn_last_update )); 
>       unsigned num_cores=0;
>       for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) {
>          unsigned not_completed = m_cluster[i]->get_not_completed();
>          if( not_completed ) {
>              if ( !num_cores )  {
>                  printf("GPGPU-Sim uArch: DEADLOCK  shader cores no longer committing instructions [core(# threads)]:\n" );
>                  printf("GPGPU-Sim uArch: DEADLOCK  ");
>                  m_cluster[i]->print_not_completed(stdout);
>              } else if (num_cores < 8 ) {
>                  m_cluster[i]->print_not_completed(stdout);
>              } else if (num_cores >= 8 ) {
>                  printf(" + others ... ");
>              }
>              num_cores+=m_shader_config->n_simt_cores_per_cluster;
>          }
763,777c762,775
<     }
<     printf("\n");
<     for (unsigned i = 0; i < m_memory_config->m_n_mem; i++) {
<       bool busy = m_memory_partition_unit[i]->busy();
<       if ( busy )
<         printf("GPGPU-Sim uArch DEADLOCK:  memory partition %u busy\n", i );
<     }
<     if ( icnt_busy() ) {
<       printf("GPGPU-Sim uArch DEADLOCK:  iterconnect contains traffic\n");
<       icnt_display_state( stdout );
<     }
<     printf("\nRe-run the simulator in gdb and use debug routines in .gdbinit to debug this\n");
<     fflush(stdout);
<     abort();
<   }
---
>       printf("\n");
>       for (unsigned i=0;i<m_memory_config->m_n_mem;i++) {
>          bool busy = m_memory_partition_unit[i]->busy();
>          if( busy ) 
>              printf("GPGPU-Sim uArch DEADLOCK:  memory partition %u busy\n", i );
>       }
>       if( icnt_busy() ) {
>          printf("GPGPU-Sim uArch DEADLOCK:  iterconnect contains traffic\n");
>          icnt_display_state( stdout );
>       }
>       printf("\nRe-run the simulator in gdb and use debug routines in .gdbinit to debug this\n");
>       fflush(stdout);
>       abort();
>    }
781c779
< std::string gpgpu_sim::executed_kernel_info_string()
---
> std::string gpgpu_sim::executed_kernel_info_string() 
783c781
<   std::stringstream statout;
---
>    std::stringstream statout; 
785,794c783,792
<   statout << "kernel_name = ";
<   for (unsigned int k = 0; k < m_executed_kernel_names.size(); k++) {
<     statout << m_executed_kernel_names[k] << " ";
<   }
<   statout << std::endl;
<   statout << "kernel_launch_uid = ";
<   for (unsigned int k = 0; k < m_executed_kernel_uids.size(); k++) {
<     statout << m_executed_kernel_uids[k] << " ";
<   }
<   statout << std::endl;
---
>    statout << "kernel_name = "; 
>    for (unsigned int k = 0; k < m_executed_kernel_names.size(); k++) {
>       statout << m_executed_kernel_names[k] << " "; 
>    }
>    statout << std::endl; 
>    statout << "kernel_launch_uid = ";
>    for (unsigned int k = 0; k < m_executed_kernel_uids.size(); k++) {
>       statout << m_executed_kernel_uids[k] << " "; 
>    }
>    statout << std::endl; 
796c794
<   return statout.str();
---
>    return statout.str(); 
800c798
<   m_special_cache_config[kernel_name] = cacheConfig ;
---
> 	m_special_cache_config[kernel_name]=cacheConfig ;
805,811c803,809
<   for ( std::map<std::string, FuncCache>::iterator iter = m_special_cache_config.begin(); iter != m_special_cache_config.end(); iter++) {
<     std::string kernel = iter->first;
<     if (kernel_name.compare(kernel) == 0) {
<       return iter->second;
<     }
<   }
<   return (FuncCache)0;
---
> 	for (	std::map<std::string, FuncCache>::iterator iter = m_special_cache_config.begin(); iter != m_special_cache_config.end(); iter++){
> 		    std::string kernel= iter->first;
> 			if (kernel_name.compare(kernel) == 0){
> 				return iter->second;
> 			}
> 	}
> 	return (FuncCache)0;
816,822c814,820
<   for ( std::map<std::string, FuncCache>::iterator iter = m_special_cache_config.begin(); iter != m_special_cache_config.end(); iter++) {
<     std::string kernel = iter->first;
<     if (kernel_name.compare(kernel) == 0) {
<       return true;
<     }
<   }
<   return false;
---
> 	for (	std::map<std::string, FuncCache>::iterator iter = m_special_cache_config.begin(); iter != m_special_cache_config.end(); iter++){
> 	    	std::string kernel= iter->first;
> 			if (kernel_name.compare(kernel) == 0){
> 				return true;
> 			}
> 	}
> 	return false;
828,832c826,830
<   if (has_special_cache_config(kernel_name)) {
<     change_cache_config(get_cache_config(kernel_name));
<   } else {
<     change_cache_config(FuncCachePreferNone);
<   }
---
> 	if(has_special_cache_config(kernel_name)){
> 		change_cache_config(get_cache_config(kernel_name));
> 	}else{
> 		change_cache_config(FuncCachePreferNone);
> 	}
838,875c836,873
<   if (cache_config != m_shader_config->m_L1D_config.get_cache_status()) {
<     printf("FLUSH L1 Cache at configuration change between kernels\n");
<     for (unsigned i = 0; i < m_shader_config->n_simt_clusters; i++) {
<       m_cluster[i]->cache_flush();
<     }
<   }
< 
<   switch (cache_config) {
<   case FuncCachePreferNone:
<     m_shader_config->m_L1D_config.init(m_shader_config->m_L1D_config.m_config_string, FuncCachePreferNone);
<     m_shader_config->gpgpu_shmem_size = m_shader_config->gpgpu_shmem_sizeDefault;
<     break;
<   case FuncCachePreferL1:
<     if ((m_shader_config->m_L1D_config.m_config_stringPrefL1 == NULL) || (m_shader_config->gpgpu_shmem_sizePrefL1 == (unsigned) - 1))
<     {
<       printf("WARNING: missing Preferred L1 configuration\n");
<       m_shader_config->m_L1D_config.init(m_shader_config->m_L1D_config.m_config_string, FuncCachePreferNone);
<       m_shader_config->gpgpu_shmem_size = m_shader_config->gpgpu_shmem_sizeDefault;
< 
<     } else {
<       m_shader_config->m_L1D_config.init(m_shader_config->m_L1D_config.m_config_stringPrefL1, FuncCachePreferL1);
<       m_shader_config->gpgpu_shmem_size = m_shader_config->gpgpu_shmem_sizePrefL1;
<     }
<     break;
<   case FuncCachePreferShared:
<     if ((m_shader_config->m_L1D_config.m_config_stringPrefShared == NULL) || (m_shader_config->gpgpu_shmem_sizePrefShared == (unsigned) - 1))
<     {
<       printf("WARNING: missing Preferred L1 configuration\n");
<       m_shader_config->m_L1D_config.init(m_shader_config->m_L1D_config.m_config_string, FuncCachePreferNone);
<       m_shader_config->gpgpu_shmem_size = m_shader_config->gpgpu_shmem_sizeDefault;
<     } else {
<       m_shader_config->m_L1D_config.init(m_shader_config->m_L1D_config.m_config_stringPrefShared, FuncCachePreferShared);
<       m_shader_config->gpgpu_shmem_size = m_shader_config->gpgpu_shmem_sizePrefShared;
<     }
<     break;
<   default:
<     break;
<   }
---
> 	if(cache_config != m_shader_config->m_L1D_config.get_cache_status()){
> 		printf("FLUSH L1 Cache at configuration change between kernels\n");
> 		for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) {
> 			m_cluster[i]->cache_flush();
> 	    }
> 	}
> 
> 	switch(cache_config){
> 	case FuncCachePreferNone:
> 		m_shader_config->m_L1D_config.init(m_shader_config->m_L1D_config.m_config_string, FuncCachePreferNone);
> 		m_shader_config->gpgpu_shmem_size=m_shader_config->gpgpu_shmem_sizeDefault;
> 		break;
> 	case FuncCachePreferL1:
> 		if((m_shader_config->m_L1D_config.m_config_stringPrefL1 == NULL) || (m_shader_config->gpgpu_shmem_sizePrefL1 == (unsigned)-1))
> 		{
> 			printf("WARNING: missing Preferred L1 configuration\n");
> 			m_shader_config->m_L1D_config.init(m_shader_config->m_L1D_config.m_config_string, FuncCachePreferNone);
> 			m_shader_config->gpgpu_shmem_size=m_shader_config->gpgpu_shmem_sizeDefault;
> 
> 		}else{
> 			m_shader_config->m_L1D_config.init(m_shader_config->m_L1D_config.m_config_stringPrefL1, FuncCachePreferL1);
> 			m_shader_config->gpgpu_shmem_size=m_shader_config->gpgpu_shmem_sizePrefL1;
> 		}
> 		break;
> 	case FuncCachePreferShared:
> 		if((m_shader_config->m_L1D_config.m_config_stringPrefShared == NULL) || (m_shader_config->gpgpu_shmem_sizePrefShared == (unsigned)-1))
> 		{
> 			printf("WARNING: missing Preferred L1 configuration\n");
> 			m_shader_config->m_L1D_config.init(m_shader_config->m_L1D_config.m_config_string, FuncCachePreferNone);
> 			m_shader_config->gpgpu_shmem_size=m_shader_config->gpgpu_shmem_sizeDefault;
> 		}else{
> 			m_shader_config->m_L1D_config.init(m_shader_config->m_L1D_config.m_config_stringPrefShared, FuncCachePreferShared);
> 			m_shader_config->gpgpu_shmem_size=m_shader_config->gpgpu_shmem_sizePrefShared;
> 		}
> 		break;
> 	default:
> 		break;
> 	}
881,882c879,880
<   m_executed_kernel_names.clear();
<   m_executed_kernel_uids.clear();
---
>    m_executed_kernel_names.clear();
>    m_executed_kernel_uids.clear();
884,889c882,895
< void gpgpu_sim::gpu_print_stat()
< {
<   FILE *statfout = stdout;
< 
<   std::string kernel_info_str = executed_kernel_info_string();
<   fprintf(statfout, "%s", kernel_info_str.c_str());
---
> void gpgpu_sim::gpu_print_stat() 
> {  
>    FILE *statfout = stdout; 
> 
>    std::string kernel_info_str = executed_kernel_info_string(); 
>    fprintf(statfout, "%s", kernel_info_str.c_str()); 
> 
>    printf("gpu_sim_cycle = %lld\n", gpu_sim_cycle);
>    printf("gpu_sim_insn = %lld\n", gpu_sim_insn);
>    printf("gpu_ipc = %12.4f\n", (float)gpu_sim_insn / gpu_sim_cycle);
>    printf("gpu_tot_sim_cycle = %lld\n", gpu_tot_sim_cycle+gpu_sim_cycle);
>    printf("gpu_tot_sim_insn = %lld\n", gpu_tot_sim_insn+gpu_sim_insn);
>    printf("gpu_tot_ipc = %12.4f\n", (float)(gpu_tot_sim_insn+gpu_sim_insn) / (gpu_tot_sim_cycle+gpu_sim_cycle));
>    printf("gpu_tot_issued_cta = %lld\n", gpu_tot_issued_cta);
891,897d896
<   printf("gpu_sim_cycle = %lld\n", gpu_sim_cycle);
<   printf("gpu_sim_insn = %lld\n", gpu_sim_insn);
<   printf("gpu_ipc = %12.4f\n", (float)gpu_sim_insn / gpu_sim_cycle);
<   printf("gpu_tot_sim_cycle = %lld\n", gpu_tot_sim_cycle + gpu_sim_cycle);
<   printf("gpu_tot_sim_insn = %lld\n", gpu_tot_sim_insn + gpu_sim_insn);
<   printf("gpu_tot_ipc = %12.4f\n", (float)(gpu_tot_sim_insn + gpu_sim_insn) / (gpu_tot_sim_cycle + gpu_sim_cycle));
<   printf("gpu_tot_issued_cta = %lld\n", gpu_tot_issued_cta);
899a899,918
>    // performance counter for stalls due to congestion.
>    printf("gpu_stall_dramfull = %d\n", gpu_stall_dramfull);
>    printf("gpu_stall_icnt2sh    = %d\n", gpu_stall_icnt2sh );
> 
>    time_t curr_time;
>    time(&curr_time);
>    unsigned long long elapsed_time = MAX( curr_time - g_simulation_starttime, 1 );
>    printf( "gpu_total_sim_rate=%u\n", (unsigned)( ( gpu_tot_sim_insn + gpu_sim_insn ) / elapsed_time ) );
> 
>    //shader_print_l1_miss_stat( stdout );
>    shader_print_cache_stats(stdout);
> 
>    cache_stats core_cache_stats;
>    core_cache_stats.clear();
>    for(unsigned i=0; i<m_config.num_cluster(); i++){
>        m_cluster[i]->get_cache_stats(core_cache_stats);
>    }
>    printf("\nTotal_core_cache_stats:\n");
>    core_cache_stats.print_stats(stdout, "Total_core_cache_stats_breakdown");
>    shader_print_scheduler_stat( stdout, false );
901,922c920
<   // performance counter for stalls due to congestion.
<   printf("gpu_stall_dramfull = %d\n", gpu_stall_dramfull);
<   printf("gpu_stall_icnt2sh    = %d\n", gpu_stall_icnt2sh );
< 
<   time_t curr_time;
<   time(&curr_time);
<   unsigned long long elapsed_time = MAX( curr_time - g_simulation_starttime, 1 );
<   printf( "gpu_total_sim_rate=%u\n", (unsigned)( ( gpu_tot_sim_insn + gpu_sim_insn ) / elapsed_time ) );
< 
<   //shader_print_l1_miss_stat( stdout );
<   shader_print_cache_stats(stdout);
< 
<   cache_stats core_cache_stats;
<   core_cache_stats.clear();
<   for (unsigned i = 0; i < m_config.num_cluster(); i++) {
<     m_cluster[i]->get_cache_stats(core_cache_stats);
<   }
<   printf("\nTotal_core_cache_stats:\n");
<   core_cache_stats.print_stats(stdout, "Total_core_cache_stats_breakdown");
<   shader_print_scheduler_stat( stdout, false );
< 
<   m_shader_stats->print(stdout);
---
>    m_shader_stats->print(stdout);
924,927c922,925
<   if (m_config.g_power_simulation_enabled) {
<     m_gpgpusim_wrapper->print_power_kernel_stats(gpu_sim_cycle, gpu_tot_sim_cycle, gpu_tot_sim_insn + gpu_sim_insn, kernel_info_str, true );
<     mcpat_reset_perf_count(m_gpgpusim_wrapper);
<   }
---
>    if(m_config.g_power_simulation_enabled){
> 	   m_gpgpusim_wrapper->print_power_kernel_stats(gpu_sim_cycle, gpu_tot_sim_cycle, gpu_tot_sim_insn + gpu_sim_insn, kernel_info_str, true );
> 	   mcpat_reset_perf_count(m_gpgpusim_wrapper);
>    }
930,975c928,973
<   // performance counter that are not local to one shader
<   m_memory_stats->memlatstat_print(m_memory_config->m_n_mem, m_memory_config->nbk);
<   for (unsigned i = 0; i < m_memory_config->m_n_mem; i++)
<     m_memory_partition_unit[i]->print(stdout);
< 
<   // L2 cache stats
<   if (!m_memory_config->m_L2_config.disabled()) {
<     cache_stats l2_stats;
<     struct cache_sub_stats l2_css;
<     struct cache_sub_stats total_l2_css;
<     l2_stats.clear();
<     l2_css.clear();
<     total_l2_css.clear();
< 
<     printf("\n========= L2 cache stats =========\n");
<     for (unsigned i = 0; i < m_memory_config->m_n_mem_sub_partition; i++) {
<       m_memory_sub_partition[i]->accumulate_L2cache_stats(l2_stats);
<       m_memory_sub_partition[i]->get_L2cache_sub_stats(l2_css);
< 
<       fprintf( stdout, "L2_cache_bank[%d]: Access = %u, Miss = %u, Miss_rate = %.3lf, Pending_hits = %u, Reservation_fails = %u\n",
<                i, l2_css.accesses, l2_css.misses, (double)l2_css.misses / (double)l2_css.accesses, l2_css.pending_hits, l2_css.res_fails);
< 
<       total_l2_css += l2_css;
<     }
<     if (!m_memory_config->m_L2_config.disabled() && m_memory_config->m_L2_config.get_num_lines()) {
<       //L2c_print_cache_stat();
<       printf("L2_total_cache_accesses = %u\n", total_l2_css.accesses);
<       printf("L2_total_cache_misses = %u\n", total_l2_css.misses);
<       if (total_l2_css.accesses > 0)
<         printf("L2_total_cache_miss_rate = %.4lf\n", (double)total_l2_css.misses / (double)total_l2_css.accesses);
<       printf("L2_total_cache_pending_hits = %u\n", total_l2_css.pending_hits);
<       printf("L2_total_cache_reservation_fails = %u\n", total_l2_css.res_fails);
<       printf("L2_total_cache_breakdown:\n");
<       l2_stats.print_stats(stdout, "L2_cache_stats_breakdown");
<       total_l2_css.print_port_stats(stdout, "L2_cache");
<     }
<   }
< 
<   if (m_config.gpgpu_cflog_interval != 0) {
<     spill_log_to_file (stdout, 1, gpu_sim_cycle);
<     insn_warp_occ_print(stdout);
<   }
<   if ( gpgpu_ptx_instruction_classification ) {
<     StatDisp( g_inst_classification_stat[g_ptx_kernel_count]);
<     StatDisp( g_inst_op_classification_stat[g_ptx_kernel_count]);
<   }
---
>    // performance counter that are not local to one shader
>    m_memory_stats->memlatstat_print(m_memory_config->m_n_mem,m_memory_config->nbk);
>    for (unsigned i=0;i<m_memory_config->m_n_mem;i++)
>       m_memory_partition_unit[i]->print(stdout);
> 
>    // L2 cache stats
>    if(!m_memory_config->m_L2_config.disabled()){
>        cache_stats l2_stats;
>        struct cache_sub_stats l2_css;
>        struct cache_sub_stats total_l2_css;
>        l2_stats.clear();
>        l2_css.clear();
>        total_l2_css.clear();
> 
>        printf("\n========= L2 cache stats =========\n");
>        for (unsigned i=0;i<m_memory_config->m_n_mem_sub_partition;i++){
>            m_memory_sub_partition[i]->accumulate_L2cache_stats(l2_stats);
>            m_memory_sub_partition[i]->get_L2cache_sub_stats(l2_css);
> 
>            fprintf( stdout, "L2_cache_bank[%d]: Access = %u, Miss = %u, Miss_rate = %.3lf, Pending_hits = %u, Reservation_fails = %u\n",
>                     i, l2_css.accesses, l2_css.misses, (double)l2_css.misses / (double)l2_css.accesses, l2_css.pending_hits, l2_css.res_fails);
> 
>            total_l2_css += l2_css;
>        }
>        if (!m_memory_config->m_L2_config.disabled() && m_memory_config->m_L2_config.get_num_lines()) {
>           //L2c_print_cache_stat();
>           printf("L2_total_cache_accesses = %u\n", total_l2_css.accesses);
>           printf("L2_total_cache_misses = %u\n", total_l2_css.misses);
>           if(total_l2_css.accesses > 0)
>               printf("L2_total_cache_miss_rate = %.4lf\n", (double)total_l2_css.misses/(double)total_l2_css.accesses);
>           printf("L2_total_cache_pending_hits = %u\n", total_l2_css.pending_hits);
>           printf("L2_total_cache_reservation_fails = %u\n", total_l2_css.res_fails);
>           printf("L2_total_cache_breakdown:\n");
>           l2_stats.print_stats(stdout, "L2_cache_stats_breakdown");
>           total_l2_css.print_port_stats(stdout, "L2_cache");
>        }
>    }
> 
>    if (m_config.gpgpu_cflog_interval != 0) {
>       spill_log_to_file (stdout, 1, gpu_sim_cycle);
>       insn_warp_occ_print(stdout);
>    }
>    if ( gpgpu_ptx_instruction_classification ) {
>       StatDisp( g_inst_classification_stat[g_ptx_kernel_count]);
>       StatDisp( g_inst_op_classification_stat[g_ptx_kernel_count]);
>    }
978,980c976,978
<   if (m_config.g_power_simulation_enabled) {
<     m_gpgpusim_wrapper->detect_print_steady_state(1, gpu_tot_sim_insn + gpu_sim_insn);
<   }
---
>    if(m_config.g_power_simulation_enabled){
>        m_gpgpusim_wrapper->detect_print_steady_state(1,gpu_tot_sim_insn+gpu_sim_insn);
>    }
984,995c982,993
<   // Interconnect power stat print
<   long total_simt_to_mem = 0;
<   long total_mem_to_simt = 0;
<   long temp_stm = 0;
<   long temp_mts = 0;
<   for (unsigned i = 0; i < m_config.num_cluster(); i++) {
<     m_cluster[i]->get_icnt_stats(temp_stm, temp_mts);
<     total_simt_to_mem += temp_stm;
<     total_mem_to_simt += temp_mts;
<   }
<   printf("\nicnt_total_pkts_mem_to_simt=%ld\n", total_mem_to_simt);
<   printf("icnt_total_pkts_simt_to_mem=%ld\n", total_simt_to_mem);
---
>    // Interconnect power stat print
>    long total_simt_to_mem=0;
>    long total_mem_to_simt=0;
>    long temp_stm=0;
>    long temp_mts = 0;
>    for(unsigned i=0; i<m_config.num_cluster(); i++){
> 	   m_cluster[i]->get_icnt_stats(temp_stm, temp_mts);
> 	   total_simt_to_mem += temp_stm;
> 	   total_mem_to_simt += temp_mts;
>    }
>    printf("\nicnt_total_pkts_mem_to_simt=%ld\n", total_mem_to_simt);
>    printf("icnt_total_pkts_simt_to_mem=%ld\n", total_simt_to_mem);
997,998c995,996
<   time_vector_print();
<   fflush(stdout);
---
>    time_vector_print();
>    fflush(stdout);
1000c998
<   clear_executed_kernel_info();
---
>    clear_executed_kernel_info(); 
1005,1007c1003,1005
< unsigned gpgpu_sim::threads_per_core() const
< {
<   return m_shader_config->n_thread_per_shader;
---
> unsigned gpgpu_sim::threads_per_core() const 
> { 
>    return m_shader_config->n_thread_per_shader; 
1012,1040c1010,1038
<   unsigned active_count = inst.active_count();
<   //this breaks some encapsulation: the is_[space] functions, if you change those, change this.
<   switch (inst.space.get_type()) {
<   case undefined_space:
<   case reg_space:
<     break;
<   case shared_space:
<     m_stats->gpgpu_n_shmem_insn += active_count;
<     break;
<   case const_space:
<     m_stats->gpgpu_n_const_insn += active_count;
<     break;
<   case param_space_kernel:
<   case param_space_local:
<     m_stats->gpgpu_n_param_insn += active_count;
<     break;
<   case tex_space:
<     m_stats->gpgpu_n_tex_insn += active_count;
<     break;
<   case global_space:
<   case local_space:
<     if ( inst.is_store() )
<       m_stats->gpgpu_n_store_insn += active_count;
<     else
<       m_stats->gpgpu_n_load_insn += active_count;
<     break;
<   default:
<     abort();
<   }
---
>     unsigned active_count = inst.active_count(); 
>     //this breaks some encapsulation: the is_[space] functions, if you change those, change this.
>     switch (inst.space.get_type()) {
>     case undefined_space:
>     case reg_space:
>         break;
>     case shared_space:
>         m_stats->gpgpu_n_shmem_insn += active_count; 
>         break;
>     case const_space:
>         m_stats->gpgpu_n_const_insn += active_count;
>         break;
>     case param_space_kernel:
>     case param_space_local:
>         m_stats->gpgpu_n_param_insn += active_count;
>         break;
>     case tex_space:
>         m_stats->gpgpu_n_tex_insn += active_count;
>         break;
>     case global_space:
>     case local_space:
>         if( inst.is_store() )
>             m_stats->gpgpu_n_store_insn += active_count;
>         else 
>             m_stats->gpgpu_n_load_insn += active_count;
>         break;
>     default:
>         abort();
>     }
1047,1050c1045,1048
<  * Launches a cooperative thread array (CTA).
<  *
<  * @param kernel
<  *    object that tells us which kernel to ask for a CTA from
---
>  * Launches a cooperative thread array (CTA). 
>  *  
>  * @param kernel 
>  *    object that tells us which kernel to ask for a CTA from 
1053c1051
< void shader_core_ctx::issue_block2core( kernel_info_t &kernel )
---
> void shader_core_ctx::issue_block2core( kernel_info_t &kernel ) 
1055c1053
<   set_max_cta(kernel);
---
>     set_max_cta(kernel);
1057,1062c1055,1061
<   // find a free CTA context
<   unsigned free_cta_hw_id = (unsigned) - 1;
<   for (unsigned i = 0; i < kernel_max_cta_per_shader; i++ ) {
<     if ( m_cta_status[i] == 0 ) {
<       free_cta_hw_id = i;
<       break;
---
>     // find a free CTA context 
>     unsigned free_cta_hw_id=(unsigned)-1;
>     for (unsigned i=0;i<kernel_max_cta_per_shader;i++ ) {
>       if( m_cta_status[i]==0 ) {
>          free_cta_hw_id=i;
>          break;
>       }
1064,1065c1063
<   }
<   assert( free_cta_hw_id != (unsigned) - 1 );
---
>     assert( free_cta_hw_id!=(unsigned)-1 );
1067,1068c1065,1066
<   // determine hardware threads and warps that will be used for this CTA
<   int cta_size = kernel.threads_per_cta();
---
>     // determine hardware threads and warps that will be used for this CTA
>     int cta_size = kernel.threads_per_cta();
1070,1102c1068,1100
<   // hw warp id = hw thread id mod warp size, so we need to find a range
<   // of hardware thread ids corresponding to an integral number of hardware
<   // thread ids
<   int padded_cta_size = cta_size;
<   if (cta_size % m_config->warp_size)
<     padded_cta_size = ((cta_size / m_config->warp_size) + 1) * (m_config->warp_size);
<   unsigned start_thread = free_cta_hw_id * padded_cta_size;
<   unsigned end_thread  = start_thread +  cta_size;
< 
<   // reset the microarchitecture state of the selected hardware thread and warp contexts
<   reinit(start_thread, end_thread, false);
< 
<   // initalize scalar threads and determine which hardware warps they are allocated to
<   // bind functional simulation state of threads to hardware resources (simulation)
<   warp_set_t warps;
<   unsigned nthreads_in_block = 0;
<   for (unsigned i = start_thread; i < end_thread; i++) {
<     m_threadState[i].m_cta_id = free_cta_hw_id;
<     unsigned warp_id = i / m_config->warp_size;
<     nthreads_in_block += ptx_sim_init_thread(kernel, &m_thread[i], m_sid, i, cta_size - (i - start_thread), m_config->n_thread_per_shader, this, free_cta_hw_id, warp_id, m_cluster->get_gpu());
<     m_threadState[i].m_active = true;
<     warps.set( warp_id );
<   }
<   assert( nthreads_in_block > 0 && nthreads_in_block <= m_config->n_thread_per_shader); // should be at least one, but less than max
<   m_cta_status[free_cta_hw_id] = nthreads_in_block;
< 
<   // now that we know which warps are used in this CTA, we can allocate
<   // resources for use in CTA-wide barrier operations
<   m_barriers.allocate_barrier(free_cta_hw_id, warps);
< 
<   // initialize the SIMT stacks and fetch hardware
<   init_warps( free_cta_hw_id, start_thread, end_thread);
<   m_n_active_cta++;
---
>     // hw warp id = hw thread id mod warp size, so we need to find a range 
>     // of hardware thread ids corresponding to an integral number of hardware
>     // thread ids
>     int padded_cta_size = cta_size; 
>     if (cta_size%m_config->warp_size)
>       padded_cta_size = ((cta_size/m_config->warp_size)+1)*(m_config->warp_size);
>     unsigned start_thread = free_cta_hw_id * padded_cta_size;
>     unsigned end_thread  = start_thread +  cta_size;
> 
>     // reset the microarchitecture state of the selected hardware thread and warp contexts
>     reinit(start_thread, end_thread,false);
>      
>     // initalize scalar threads and determine which hardware warps they are allocated to
>     // bind functional simulation state of threads to hardware resources (simulation) 
>     warp_set_t warps;
>     unsigned nthreads_in_block= 0;
>     for (unsigned i = start_thread; i<end_thread; i++) {
>         m_threadState[i].m_cta_id = free_cta_hw_id;
>         unsigned warp_id = i/m_config->warp_size;
>         nthreads_in_block += ptx_sim_init_thread(kernel,&m_thread[i],m_sid,i,cta_size-(i-start_thread),m_config->n_thread_per_shader,this,free_cta_hw_id,warp_id,m_cluster->get_gpu());
>         m_threadState[i].m_active = true; 
>         warps.set( warp_id );
>     }
>     assert( nthreads_in_block > 0 && nthreads_in_block <= m_config->n_thread_per_shader); // should be at least one, but less than max
>     m_cta_status[free_cta_hw_id]=nthreads_in_block;
> 
>     // now that we know which warps are used in this CTA, we can allocate
>     // resources for use in CTA-wide barrier operations
>     m_barriers.allocate_barrier(free_cta_hw_id,warps);
> 
>     // initialize the SIMT stacks and fetch hardware
>     init_warps( free_cta_hw_id, start_thread, end_thread);
>     m_n_active_cta++;
1104,1105c1102,1103
<   shader_CTA_count_log(m_sid, 1);
<   printf("GPGPU-Sim uArch: core:%3d, cta:%2u initialized @(%lld,%lld)\n", m_sid, free_cta_hw_id, gpu_sim_cycle, gpu_tot_sim_cycle );
---
>     shader_CTA_count_log(m_sid, 1);
>     //printf("GPGPU-Sim uArch: core:%3d, cta:%2u initialized @(%lld,%lld)\n", m_sid, free_cta_hw_id, gpu_sim_cycle, gpu_tot_sim_cycle );
1110,1117c1108,1115
< // void dram_t::dram_log( int task )
< // {
< //   if (task == SAMPLELOG) {
< //     StatAddSample(mrqq_Dist, que_length());
< //   } else if (task == DUMPLOG) {
< //     printf ("Queue Length DRAM[%d] ", id); StatDisp(mrqq_Dist);
< //   }
< // }
---
> void dram_t::dram_log( int task ) 
> {
>    if (task == SAMPLELOG) {
>       StatAddSample(mrqq_Dist, que_length());   
>    } else if (task == DUMPLOG) {
>       printf ("Queue Length DRAM[%d] ",id);StatDisp(mrqq_Dist);
>    }
> }
1120c1118
< int gpgpu_sim::next_clock_domain(void)
---
> int gpgpu_sim::next_clock_domain(void) 
1122,1141c1120,1139
<   double smallest = min3(core_time, icnt_time, dram_time);
<   int mask = 0x00;
<   if ( l2_time <= smallest ) {
<     smallest = l2_time;
<     mask |= L2 ;
<     l2_time += m_config.l2_period;
<   }
<   if ( icnt_time <= smallest ) {
<     mask |= ICNT;
<     icnt_time += m_config.icnt_period;
<   }
<   if ( dram_time <= smallest ) {
<     mask |= DRAM;
<     dram_time += m_config.dram_period;
<   }
<   if ( core_time <= smallest ) {
<     mask |= CORE;
<     core_time += m_config.core_period;
<   }
<   return mask;
---
>    double smallest = min3(core_time,icnt_time,dram_time);
>    int mask = 0x00;
>    if ( l2_time <= smallest ) {
>       smallest = l2_time;
>       mask |= L2 ;
>       l2_time += m_config.l2_period;
>    }
>    if ( icnt_time <= smallest ) {
>       mask |= ICNT;
>       icnt_time += m_config.icnt_period;
>    }
>    if ( dram_time <= smallest ) {
>       mask |= DRAM;
>       dram_time += m_config.dram_period;
>    }
>    if ( core_time <= smallest ) {
>       mask |= CORE;
>       core_time += m_config.core_period;
>    }
>    return mask;
1146,1152c1144,1151
<   unsigned last_issued = m_last_cluster_issue;
<   for (unsigned i = 0; i < m_shader_config->n_simt_clusters; i++) {
<     unsigned idx = (i + last_issued + 1) % m_shader_config->n_simt_clusters;
<     unsigned num = m_cluster[idx]->issue_block2core();
<     if ( num ) {
<       m_last_cluster_issue = idx;
<       m_total_cta_launched += num;
---
>     unsigned last_issued = m_last_cluster_issue; 
>     for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) {
>         unsigned idx = (i + last_issued + 1) % m_shader_config->n_simt_clusters;
>         unsigned num = m_cluster[idx]->issue_block2core();
>         if( num ) {
>             m_last_cluster_issue=idx;
>             m_total_cta_launched += num;
>         }
1154d1152
<   }
1157c1155
< unsigned long long g_single_step = 0; // set this in gdb to single step the pipeline
---
> unsigned long long g_single_step=0; // set this in gdb to single step the pipeline
1161c1159
<   int clock_mask = next_clock_domain();
---
>    int clock_mask = next_clock_domain();
1163,1181c1161,1183
<   if (clock_mask & CORE ) {
<     // shader core loading (pop from ICNT into core) follows CORE clock
<     for (unsigned i = 0; i < m_shader_config->n_simt_clusters; i++)
<       m_cluster[i]->icnt_cycle();
<   }
<   if (clock_mask & ICNT) {
<     // pop from memory controller to interconnect
<     for (unsigned i = 0; i < m_memory_config->m_n_mem_sub_partition; i++) {
<       mem_fetch* mf = m_memory_sub_partition[i]->top();
<       if (mf) {
<         unsigned response_size = mf->get_is_write() ? mf->get_ctrl_size() : mf->size();
<         if ( ::icnt_has_buffer( m_shader_config->mem2device(i), response_size ) ) {
<           if (!mf->get_is_write())
<             mf->set_return_timestamp(gpu_sim_cycle + gpu_tot_sim_cycle);
<           mf->set_status(IN_ICNT_TO_SHADER, gpu_sim_cycle + gpu_tot_sim_cycle);
<           ::icnt_push( m_shader_config->mem2device(i), mf->get_tpc(), mf, response_size );
<           m_memory_sub_partition[i]->pop();
<         } else {
<           gpu_stall_icnt2sh++;
---
>    if (clock_mask & CORE ) {
>        // shader core loading (pop from ICNT into core) follows CORE clock
>       for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) 
>          m_cluster[i]->icnt_cycle(); 
>    }
>     if (clock_mask & ICNT) {
>         // pop from memory controller to interconnect
>         for (unsigned i=0;i<m_memory_config->m_n_mem_sub_partition;i++) {
>             mem_fetch* mf = m_memory_sub_partition[i]->top();
>             if (mf) {
>                 unsigned response_size = mf->get_is_write()?mf->get_ctrl_size():mf->size();
>                 if ( ::icnt_has_buffer( m_shader_config->mem2device(i), response_size ) ) {
>                     if (!mf->get_is_write()) 
>                        mf->set_return_timestamp(gpu_sim_cycle+gpu_tot_sim_cycle);
>                     mf->set_status(IN_ICNT_TO_SHADER,gpu_sim_cycle+gpu_tot_sim_cycle);
>                     ::icnt_push( m_shader_config->mem2device(i), mf->get_tpc(), mf, response_size );
>                     m_memory_sub_partition[i]->pop();
>                 } else {
>                     gpu_stall_icnt2sh++;
>                 }
>             } else {
>                m_memory_sub_partition[i]->pop();
>             }
1183,1195d1184
<       } else {
<         m_memory_sub_partition[i]->pop();
<       }
<     }
<   }
< 
<   if (clock_mask & DRAM) {
<     for (unsigned i = 0; i < m_memory_config->m_n_mem; i++) {
<       m_memory_partition_unit[i]->dram_cycle(); // Issue the dram command (scheduler + delay model)
<       // Update performance counters for DRAM
<       // m_memory_partition_unit[i]->set_dram_power_stats(m_power_stats->pwr_mem_stat->n_cmd[CURRENT_STAT_IDX][i], m_power_stats->pwr_mem_stat->n_activity[CURRENT_STAT_IDX][i],
<       //   m_power_stats->pwr_mem_stat->n_nop[CURRENT_STAT_IDX][i], m_power_stats->pwr_mem_stat->n_act[CURRENT_STAT_IDX][i], m_power_stats->pwr_mem_stat->n_pre[CURRENT_STAT_IDX][i],
<       // m_power_stats->pwr_mem_stat->n_rd[CURRENT_STAT_IDX][i], m_power_stats->pwr_mem_stat->n_wr[CURRENT_STAT_IDX][i], m_power_stats->pwr_mem_stat->n_req[CURRENT_STAT_IDX][i]);
1197d1185
<   }
1199,1209c1187,1193
<   // L2 operations follow L2 clock domain
<   if (clock_mask & L2) {
<     m_power_stats->pwr_mem_stat->l2_cache_stats[CURRENT_STAT_IDX].clear();
<     for (unsigned i = 0; i < m_memory_config->m_n_mem_sub_partition; i++) {
<       //move memory request from interconnect into memory partition (if not backed up)
<       //Note:This needs to be called in DRAM clock domain if there is no L2 cache in the system
<       if ( m_memory_sub_partition[i]->full() ) {
<         gpu_stall_dramfull++;
<       } else {
<         mem_fetch* mf = (mem_fetch*) icnt_pop( m_shader_config->mem2device(i) );
<         m_memory_sub_partition[i]->push( mf, gpu_sim_cycle + gpu_tot_sim_cycle );
---
>    if (clock_mask & DRAM) {
>       for (unsigned i=0;i<m_memory_config->m_n_mem;i++){
>          m_memory_partition_unit[i]->dram_cycle(); // Issue the dram command (scheduler + delay model)
>          // Update performance counters for DRAM
>          m_memory_partition_unit[i]->set_dram_power_stats(m_power_stats->pwr_mem_stat->n_cmd[CURRENT_STAT_IDX][i], m_power_stats->pwr_mem_stat->n_activity[CURRENT_STAT_IDX][i],
>                         m_power_stats->pwr_mem_stat->n_nop[CURRENT_STAT_IDX][i], m_power_stats->pwr_mem_stat->n_act[CURRENT_STAT_IDX][i], m_power_stats->pwr_mem_stat->n_pre[CURRENT_STAT_IDX][i],
>                         m_power_stats->pwr_mem_stat->n_rd[CURRENT_STAT_IDX][i], m_power_stats->pwr_mem_stat->n_wr[CURRENT_STAT_IDX][i], m_power_stats->pwr_mem_stat->n_req[CURRENT_STAT_IDX][i]);
1211,1214c1195
<       m_memory_sub_partition[i]->cache_cycle(gpu_sim_cycle + gpu_tot_sim_cycle);
<       m_memory_sub_partition[i]->accumulate_L2cache_stats(m_power_stats->pwr_mem_stat->l2_cache_stats[CURRENT_STAT_IDX]);
<     }
<   }
---
>    }
1216,1226c1197,1228
<   if (clock_mask & ICNT) {
<     icnt_transfer();
<   }
< 
<   if (clock_mask & CORE) {
<     // L1 cache + shader core pipeline stages
<     m_power_stats->pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].clear();
<     for (unsigned i = 0; i < m_shader_config->n_simt_clusters; i++) {
<       if (m_cluster[i]->get_not_completed() || get_more_cta_left() ) {
<         m_cluster[i]->core_cycle();
<         *active_sms += m_cluster[i]->get_n_active_sms();
---
>    // L2 operations follow L2 clock domain
>    if (clock_mask & L2) {
>        m_power_stats->pwr_mem_stat->l2_cache_stats[CURRENT_STAT_IDX].clear();
>       for (unsigned i=0;i<m_memory_config->m_n_mem_sub_partition;i++) {
>           //move memory request from interconnect into memory partition (if not backed up)
>           //Note:This needs to be called in DRAM clock domain if there is no L2 cache in the system
>           if ( m_memory_sub_partition[i]->full() ) {
>              gpu_stall_dramfull++;
>           } else {
>               mem_fetch* mf = (mem_fetch*) icnt_pop( m_shader_config->mem2device(i) );
>               m_memory_sub_partition[i]->push( mf, gpu_sim_cycle + gpu_tot_sim_cycle );
>           }
>           m_memory_sub_partition[i]->cache_cycle(gpu_sim_cycle+gpu_tot_sim_cycle);
>           m_memory_sub_partition[i]->accumulate_L2cache_stats(m_power_stats->pwr_mem_stat->l2_cache_stats[CURRENT_STAT_IDX]);
>        }
>    }
> 
>    if (clock_mask & ICNT) {
>       icnt_transfer();
>    }
> 
>    if (clock_mask & CORE) {
>       // L1 cache + shader core pipeline stages
>       m_power_stats->pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX].clear();
>       for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) {
>          if (m_cluster[i]->get_not_completed() || get_more_cta_left() ) {
>                m_cluster[i]->core_cycle();
>                *active_sms+=m_cluster[i]->get_n_active_sms();
>          }
>          // Update core icnt/cache stats for GPUWattch
>          m_cluster[i]->get_icnt_stats(m_power_stats->pwr_mem_stat->n_simt_to_mem[CURRENT_STAT_IDX][i], m_power_stats->pwr_mem_stat->n_mem_to_simt[CURRENT_STAT_IDX][i]);
>          m_cluster[i]->get_cache_stats(m_power_stats->pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX]);
1228,1238c1230,1236
<       // Update core icnt/cache stats for GPUWattch
<       m_cluster[i]->get_icnt_stats(m_power_stats->pwr_mem_stat->n_simt_to_mem[CURRENT_STAT_IDX][i], m_power_stats->pwr_mem_stat->n_mem_to_simt[CURRENT_STAT_IDX][i]);
<       m_cluster[i]->get_cache_stats(m_power_stats->pwr_mem_stat->core_cache_stats[CURRENT_STAT_IDX]);
<     }
<     float temp = 0;
<     for (unsigned i = 0; i < m_shader_config->num_shader(); i++) {
<       temp += m_shader_stats->m_pipeline_duty_cycle[i];
<     }
<     temp = temp / m_shader_config->num_shader();
<     *average_pipeline_duty_cycle = ((*average_pipeline_duty_cycle) + temp);
<     //cout<<"Average pipeline duty cycle: "<<*average_pipeline_duty_cycle<<endl;
---
>       float temp=0;
>       for (unsigned i=0;i<m_shader_config->num_shader();i++){
>         temp+=m_shader_stats->m_pipeline_duty_cycle[i];
>       }
>       temp=temp/m_shader_config->num_shader();
>       *average_pipeline_duty_cycle=((*average_pipeline_duty_cycle)+temp);
>         //cout<<"Average pipeline duty cycle: "<<*average_pipeline_duty_cycle<<endl;
1241,1246c1239,1244
<     if ( g_single_step && ((gpu_sim_cycle + gpu_tot_sim_cycle) >= g_single_step) ) {
<       asm("int $03");
<     }
<     gpu_sim_cycle++;
<     if ( g_interactive_debugger_enabled )
<       gpgpu_debug();
---
>       if( g_single_step && ((gpu_sim_cycle+gpu_tot_sim_cycle) >= g_single_step) ) {
>           asm("int $03");
>       }
>       gpu_sim_cycle++;
>       if( g_interactive_debugger_enabled ) 
>          gpgpu_debug();
1248c1246
<     // McPAT main cycle (interface with McPAT)
---
>       // McPAT main cycle (interface with McPAT)
1250,1252c1248,1250
<     if (m_config.g_power_simulation_enabled) {
<       mcpat_cycle(m_config, getShaderCoreConfig(), m_gpgpusim_wrapper, m_power_stats, m_config.gpu_stat_sample_freq, gpu_tot_sim_cycle, gpu_sim_cycle, gpu_tot_sim_insn, gpu_sim_insn);
<     }
---
>       if(m_config.g_power_simulation_enabled){
>           mcpat_cycle(m_config, getShaderCoreConfig(), m_gpgpusim_wrapper, m_power_stats, m_config.gpu_stat_sample_freq, gpu_tot_sim_cycle, gpu_sim_cycle, gpu_tot_sim_insn, gpu_sim_insn);
>       }
1255,1264c1253,1263
<     issue_block2core();
< 
<     // Depending on configuration, flush the caches once all of threads are completed.
<     int all_threads_complete = 1;
<     if (m_config.gpgpu_flush_l1_cache) {
<       for (unsigned i = 0; i < m_shader_config->n_simt_clusters; i++) {
<         if (m_cluster[i]->get_not_completed() == 0)
<           m_cluster[i]->cache_flush();
<         else
<           all_threads_complete = 0 ;
---
>       issue_block2core();
>       
>       // Depending on configuration, flush the caches once all of threads are completed.
>       int all_threads_complete = 1;
>       if (m_config.gpgpu_flush_l1_cache) {
>          for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) {
>             if (m_cluster[i]->get_not_completed() == 0)
>                 m_cluster[i]->cache_flush();
>             else
>                all_threads_complete = 0 ;
>          }
1266d1264
<     }
1268,1273c1266,1273
<     if (m_config.gpgpu_flush_l2_cache) {
<       if (!m_config.gpgpu_flush_l1_cache) {
<         for (unsigned i = 0; i < m_shader_config->n_simt_clusters; i++) {
<           if (m_cluster[i]->get_not_completed() != 0) {
<             all_threads_complete = 0 ;
<             break;
---
>       if(m_config.gpgpu_flush_l2_cache){
>           if(!m_config.gpgpu_flush_l1_cache){
>               for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) {
>                   if (m_cluster[i]->get_not_completed() != 0){
>                       all_threads_complete = 0 ;
>                       break;
>                   }
>               }
1275,1276d1274
<         }
<       }
1278,1287c1276,1286
<       if (all_threads_complete && !m_memory_config->m_L2_config.disabled() ) {
<         printf("Flushed L2 caches...\n");
<         if (m_memory_config->m_L2_config.get_num_lines()) {
<           int dlc = 0;
<           for (unsigned i = 0; i < m_memory_config->m_n_mem; i++) {
<             dlc = m_memory_sub_partition[i]->flushL2();
<             assert (dlc == 0); // need to model actual writes to DRAM here
<             printf("Dirty lines flushed from L2 %d is %d\n", i, dlc  );
<           }
<         }
---
>          if (all_threads_complete && !m_memory_config->m_L2_config.disabled() ) {
>             printf("Flushed L2 caches...\n");
>             if (m_memory_config->m_L2_config.get_num_lines()) {
>                int dlc = 0;
>                for (unsigned i=0;i<m_memory_config->m_n_mem;i++) {
>                   dlc = m_memory_sub_partition[i]->flushL2();
>                   assert (dlc == 0); // need to model actual writes to DRAM here
>                   printf("Dirty lines flushed from L2 %d is %d\n", i, dlc  );
>                }
>             }
>          }
1289d1287
<     }
1291,1308c1289,1323
<     if (!(gpu_sim_cycle % m_config.gpu_stat_sample_freq)) {
<       time_t days, hrs, minutes, sec;
<       time_t curr_time;
<       time(&curr_time);
<       unsigned long long  elapsed_time = MAX(curr_time - g_simulation_starttime, 1);
<       if ( (elapsed_time - last_liveness_message_time) >= m_config.liveness_message_freq ) {
<         days    = elapsed_time / (3600 * 24);
<         hrs     = elapsed_time / 3600 - 24 * days;
<         minutes = elapsed_time / 60 - 60 * (hrs + 24 * days);
<         sec = elapsed_time - 60 * (minutes + 60 * (hrs + 24 * days));
<         printf("GPGPU-Sim uArch: cycles simulated: %lld  inst.: %lld (ipc=%4.1f) sim_rate=%u (inst/sec) elapsed = %u:%u:%02u:%02u / %s",
<                gpu_tot_sim_cycle + gpu_sim_cycle, gpu_tot_sim_insn + gpu_sim_insn,
<                (double)gpu_sim_insn / (double)gpu_sim_cycle,
<                (unsigned)((gpu_tot_sim_insn + gpu_sim_insn) / elapsed_time),
<                (unsigned)days, (unsigned)hrs, (unsigned)minutes, (unsigned)sec,
<                ctime(&curr_time));
<         fflush(stdout);
<         last_liveness_message_time = elapsed_time;
---
>       if (!(gpu_sim_cycle % m_config.gpu_stat_sample_freq)) {
>          time_t days, hrs, minutes, sec;
>          time_t curr_time;
>          time(&curr_time);
>          unsigned long long  elapsed_time = MAX(curr_time - g_simulation_starttime, 1);
>          if ( (elapsed_time - last_liveness_message_time) >= m_config.liveness_message_freq ) {
>             days    = elapsed_time/(3600*24);
>             hrs     = elapsed_time/3600 - 24*days;
>             minutes = elapsed_time/60 - 60*(hrs + 24*days);
>             sec = elapsed_time - 60*(minutes + 60*(hrs + 24*days));
> 	    //            printf("GPGPU-Sim uArch: cycles simulated: %lld  inst.: %lld (ipc=%4.1f) sim_rate=%u (inst/sec) elapsed = %u:%u:%02u:%02u / %s", 
> 	    //     gpu_tot_sim_cycle + gpu_sim_cycle, gpu_tot_sim_insn + gpu_sim_insn, 
> 	    //      (double)gpu_sim_insn/(double)gpu_sim_cycle,
>             //       (unsigned)((gpu_tot_sim_insn+gpu_sim_insn) / elapsed_time),
> 	    //     (unsigned)days,(unsigned)hrs,(unsigned)minutes,(unsigned)sec,
> 	    //     ctime(&curr_time));
>             fflush(stdout);
>             last_liveness_message_time = elapsed_time; 
>          }
>          visualizer_printstat();
>          m_memory_stats->memlatstat_lat_pw();
>          if (m_config.gpgpu_runtime_stat && (m_config.gpu_runtime_stat_flag != 0) ) {
>             if (m_config.gpu_runtime_stat_flag & GPU_RSTAT_BW_STAT) {
>                for (unsigned i=0;i<m_memory_config->m_n_mem;i++) 
>                   m_memory_partition_unit[i]->print_stat(stdout);
>                printf("maxmrqlatency = %d \n", m_memory_stats->max_mrq_latency);
>                printf("maxmflatency = %d \n", m_memory_stats->max_mf_latency);
>             }
>             if (m_config.gpu_runtime_stat_flag & GPU_RSTAT_SHD_INFO) 
>                shader_print_runtime_stat( stdout );
>             if (m_config.gpu_runtime_stat_flag & GPU_RSTAT_L1MISS) 
>                shader_print_l1_miss_stat( stdout );
>             if (m_config.gpu_runtime_stat_flag & GPU_RSTAT_SCHED) 
>                shader_print_scheduler_stat( stdout, false );
>          }
1310,1328d1324
<       visualizer_printstat();
<       m_memory_stats->memlatstat_lat_pw();
<       if (m_config.gpgpu_runtime_stat && (m_config.gpu_runtime_stat_flag != 0) ) {
<         if (m_config.gpu_runtime_stat_flag & GPU_RSTAT_BW_STAT) {
<           for (unsigned i = 0; i < m_memory_config->m_n_mem; i++)
<           {
<             m_memory_partition_unit[i]->print_stat(stdout);
<           }
<           printf("maxmrqlatency = %d \n", m_memory_stats->max_mrq_latency);
<           printf("maxmflatency = %d \n", m_memory_stats->max_mf_latency);
<         }
<         if (m_config.gpu_runtime_stat_flag & GPU_RSTAT_SHD_INFO)
<           shader_print_runtime_stat( stdout );
<         if (m_config.gpu_runtime_stat_flag & GPU_RSTAT_L1MISS)
<           shader_print_l1_miss_stat( stdout );
<         if (m_config.gpu_runtime_stat_flag & GPU_RSTAT_SCHED)
<           shader_print_scheduler_stat( stdout, false );
<       }
<     }
1330,1335c1326,1332
<     if (!(gpu_sim_cycle % 20000)) {
<       // deadlock detection
<       if (m_config.gpu_deadlock_detect && gpu_sim_insn == last_gpu_sim_insn) {
<         gpu_deadlock = true;
<       } else {
<         last_gpu_sim_insn = gpu_sim_insn;
---
>       if (!(gpu_sim_cycle % 20000)) {
>          // deadlock detection 
>          if (m_config.gpu_deadlock_detect && gpu_sim_insn == last_gpu_sim_insn) {
>             gpu_deadlock = true;
>          } else {
>             last_gpu_sim_insn = gpu_sim_insn;
>          }
1337,1340c1334,1336
<     }
<     try_snap_shot(gpu_sim_cycle);
<     spill_log_to_file (stdout, 0, gpu_sim_cycle);
<   }
---
>       try_snap_shot(gpu_sim_cycle);
>       spill_log_to_file (stdout, 0, gpu_sim_cycle);
>    }
1346,1349c1342,1345
<   fprintf(fout, "\n");
<   fprintf(fout, "per warp functional simulation status:\n");
<   for (unsigned w = 0; w < m_config->max_warps_per_shader; w++ )
<     m_warp[w].print(fout);
---
>    fprintf(fout, "\n");
>    fprintf(fout, "per warp functional simulation status:\n");
>    for (unsigned w=0; w < m_config->max_warps_per_shader; w++ ) 
>        m_warp[w].print(fout);
1354,1379c1350,1365
<   /*
<      You may want to use this function while running GPGPU-Sim in gdb.
<      One way to do that is add the following to your .gdbinit file:
< 
<         define dp
<            call g_the_gpu.dump_pipeline_impl((0x40|0x4|0x1),$arg0,0)
<         end
< 
<      Then, typing "dp 3" will show the contents of the pipeline for shader core 3.
<   */
< 
<   printf("Dumping pipeline state...\n");
<   if (!mask) mask = 0xFFFFFFFF;
<   for (unsigned i = 0; i < m_shader_config->n_simt_clusters; i++) {
<     if (s != -1) {
<       i = s;
<     }
<     if (mask & 1) m_cluster[m_shader_config->sid_to_cluster(i)]->display_pipeline(i, stdout, 1, mask & 0x2E);
<     if (s != -1) {
<       break;
<     }
<   }
<   if (mask & 0x10000) {
<     for (unsigned i = 0; i < m_memory_config->m_n_mem; i++) {
<       if (m != -1) {
<         i = m;
---
> /*
>    You may want to use this function while running GPGPU-Sim in gdb.
>    One way to do that is add the following to your .gdbinit file:
>  
>       define dp
>          call g_the_gpu.dump_pipeline_impl((0x40|0x4|0x1),$arg0,0)
>       end
>  
>    Then, typing "dp 3" will show the contents of the pipeline for shader core 3.
> */
> 
>    printf("Dumping pipeline state...\n");
>    if(!mask) mask = 0xFFFFFFFF;
>    for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++) {
>       if(s != -1) {
>          i = s;
1381,1386c1367,1369
<       printf("DRAM / memory controller %u:\n", i);
<       // if (mask & 0x100000) m_memory_partition_unit[i]->print_stat(stdout);
<       // if (mask & 0x1000000)   m_memory_partition_unit[i]->visualize();
<       // if (mask & 0x10000000)   m_memory_partition_unit[i]->print(stdout);
<       if (m != -1) {
<         break;
---
>       if(mask&1) m_cluster[m_shader_config->sid_to_cluster(i)]->display_pipeline(i,stdout,1,mask & 0x2E);
>       if(s != -1) {
>          break;
1388,1390c1371,1386
<     }
<   }
<   fflush(stdout);
---
>    }
>    if(mask&0x10000) {
>       for (unsigned i=0;i<m_memory_config->m_n_mem;i++) {
>          if(m != -1) {
>             i=m;
>          }
>          printf("DRAM / memory controller %u:\n", i);
>          if(mask&0x100000) m_memory_partition_unit[i]->print_stat(stdout);
>          if(mask&0x1000000)   m_memory_partition_unit[i]->visualize();
>          if(mask&0x10000000)   m_memory_partition_unit[i]->print(stdout);
>          if(m != -1) {
>             break;
>          }
>       }
>    }
>    fflush(stdout);
1395c1391
<   return m_shader_config;
---
>    return m_shader_config;
1400c1396
<   return m_memory_config;
---
>    return m_memory_config;
1405c1401
<   return *m_cluster;
---
>    return *m_cluster;
Only in gpgpu-sim/: .gpu-sim.cc.swp
diff -r gpgpu-sim/gpu-sim.h ../../../gpgpu-sim_distribution/src/gpgpu-sim/gpu-sim.h
278d277
< extern int core_numbers;
diff -r gpgpu-sim/l2cache.cc ../../../gpgpu-sim_distribution/src/gpgpu-sim/l2cache.cc
37,38c37
< // #include "dram.h"'
< 
---
> #include "dram.h"
48,50d46
< #include "../ramulator_sim/Config.h"
< //#include "../ramulator_sim/gpu_wrapper.h"
< 
53c49
< mem_fetch * partition_mf_allocator::alloc(new_addr_type addr, mem_access_type type, unsigned size, bool wr ) const
---
> mem_fetch * partition_mf_allocator::alloc(new_addr_type addr, mem_access_type type, unsigned size, bool wr ) const 
57c53
<     mem_fetch *mf = new mem_fetch( access,
---
>     mem_fetch *mf = new mem_fetch( access, 
59,61c55,57
<                                    WRITE_PACKET_SIZE,
<                                    -1,
<                                    -1,
---
>                                    WRITE_PACKET_SIZE, 
>                                    -1, 
>                                    -1, 
67,71c63,66
< memory_partition_unit::memory_partition_unit( unsigned partition_id,
<         const struct memory_config *config,
<         //class ramulator::Config *r_config,
<         class memory_stats_t *stats )
<     : m_id(partition_id), m_config(config), /*m_r_config(r_config),*/ m_stats(stats), m_arbitration_metadata(config)
---
> memory_partition_unit::memory_partition_unit( unsigned partition_id, 
>                                               const struct memory_config *config,
>                                               class memory_stats_t *stats )
> : m_id(partition_id), m_config(config), m_stats(stats), m_arbitration_metadata(config) 
72a68
>     m_dram = new dram_t(m_id,m_config,m_stats,this);
74,86c70
< 
<     /*Modified by Yongbin Gu*/
<     //Original
<     //m_dram = new dram_t(m_id,m_config,m_stats,this);
< 
<     Config m_r_config("HBM-config.cfg");
<     m_r_config.set_core_num(core_numbers);
< 
<     m_dram_r = new GpuWrapper(m_r_config, m_config->m_L2_config.get_line_sz() , this, m_id);
< 
<     /*Modified by Yongbin Gu */
< 
<     m_sub_partition = new memory_sub_partition*[m_config->m_n_sub_partition_per_memory_channel];
---
>     m_sub_partition = new memory_sub_partition*[m_config->m_n_sub_partition_per_memory_channel]; 
88,89c72,73
<         unsigned sub_partition_id = m_id * m_config->m_n_sub_partition_per_memory_channel + p;
<         m_sub_partition[p] = new memory_sub_partition(sub_partition_id, m_config, stats);
---
>         unsigned sub_partition_id = m_id * m_config->m_n_sub_partition_per_memory_channel + p; 
>         m_sub_partition[p] = new memory_sub_partition(sub_partition_id, m_config, stats); 
93c77
< memory_partition_unit::~memory_partition_unit()
---
> memory_partition_unit::~memory_partition_unit() 
95,102c79
<     /*Modified by Yongbin Gu*/
<     //Original
<     //   delete m_dram;
<     //Original
<     delete m_dram_r;
<     /*Modified by Yongbin Gu */
< 
< 
---
>     delete m_dram; 
104,121c81,98
<         delete m_sub_partition[p];
<     }
<     delete[] m_sub_partition;
< }
< 
< memory_partition_unit::arbitration_metadata::arbitration_metadata(const struct memory_config *config)
<     : m_last_borrower(config->m_n_sub_partition_per_memory_channel - 1),
<       m_private_credit(config->m_n_sub_partition_per_memory_channel, 0),
<       m_shared_credit(0)
< {
<     // each sub partition get at least 1 credit for forward progress
<     // the rest is shared among with other partitions
<     m_private_credit_limit = 1;
<     m_shared_credit_limit = config->gpgpu_frfcfs_dram_sched_queue_size
<                             + config->gpgpu_dram_return_queue_size
<                             - (config->m_n_sub_partition_per_memory_channel - 1);
<     if (config->gpgpu_frfcfs_dram_sched_queue_size == 0
<             or config->gpgpu_dram_return_queue_size == 0)
---
>         delete m_sub_partition[p]; 
>     } 
>     delete[] m_sub_partition; 
> }
> 
> memory_partition_unit::arbitration_metadata::arbitration_metadata(const struct memory_config *config) 
> : m_last_borrower(config->m_n_sub_partition_per_memory_channel - 1), 
>   m_private_credit(config->m_n_sub_partition_per_memory_channel, 0), 
>   m_shared_credit(0) 
> {
>     // each sub partition get at least 1 credit for forward progress 
>     // the rest is shared among with other partitions 
>     m_private_credit_limit = 1; 
>     m_shared_credit_limit = config->gpgpu_frfcfs_dram_sched_queue_size 
>                             + config->gpgpu_dram_return_queue_size 
>                             - (config->m_n_sub_partition_per_memory_channel - 1); 
>     if (config->gpgpu_frfcfs_dram_sched_queue_size == 0 
>         or config->gpgpu_dram_return_queue_size == 0) 
123c100
<         m_shared_credit_limit = 0; // no limit if either of the queue has no limit in size
---
>         m_shared_credit_limit = 0; // no limit if either of the queue has no limit in size 
125c102
<     assert(m_shared_credit_limit >= 0);
---
>     assert(m_shared_credit_limit >= 0); 
128c105
< bool memory_partition_unit::arbitration_metadata::has_credits(int inner_sub_partition_id) const
---
> bool memory_partition_unit::arbitration_metadata::has_credits(int inner_sub_partition_id) const 
130c107
<     int spid = inner_sub_partition_id;
---
>     int spid = inner_sub_partition_id; 
132c109
<         return true;
---
>         return true; 
134c111
<         return true;
---
>         return true; 
136c113
<         return false;
---
>         return false; 
140c117
< void memory_partition_unit::arbitration_metadata::borrow_credit(int inner_sub_partition_id)
---
> void memory_partition_unit::arbitration_metadata::borrow_credit(int inner_sub_partition_id) 
142c119
<     int spid = inner_sub_partition_id;
---
>     int spid = inner_sub_partition_id; 
144c121
<         m_private_credit[spid] += 1;
---
>         m_private_credit[spid] += 1; 
146c123
<         m_shared_credit += 1;
---
>         m_shared_credit += 1; 
148c125
<         assert(0 && "DRAM arbitration error: Borrowing from depleted credit!");
---
>         assert(0 && "DRAM arbitration error: Borrowing from depleted credit!"); 
150c127
<     m_last_borrower = spid;
---
>     m_last_borrower = spid; 
153c130
< void memory_partition_unit::arbitration_metadata::return_credit(int inner_sub_partition_id)
---
> void memory_partition_unit::arbitration_metadata::return_credit(int inner_sub_partition_id) 
155c132
<     int spid = inner_sub_partition_id;
---
>     int spid = inner_sub_partition_id; 
157c134
<         m_private_credit[spid] -= 1;
---
>         m_private_credit[spid] -= 1; 
159,161c136,138
<         m_shared_credit -= 1;
<     }
<     assert((m_shared_credit >= 0) && "DRAM arbitration error: Returning more than available credits!");
---
>         m_shared_credit -= 1; 
>     } 
>     assert((m_shared_credit >= 0) && "DRAM arbitration error: Returning more than available credits!"); 
164c141
< void memory_partition_unit::arbitration_metadata::print( FILE *fp ) const
---
> void memory_partition_unit::arbitration_metadata::print( FILE *fp ) const 
166c143
<     fprintf(fp, "private_credit = ");
---
>     fprintf(fp, "private_credit = "); 
168c145
<         fprintf(fp, "%d ", m_private_credit[p]);
---
>         fprintf(fp, "%d ", m_private_credit[p]); 
170,171c147,148
<     fprintf(fp, "(limit = %d)\n", m_private_credit_limit);
<     fprintf(fp, "shared_credit = %d (limit = %d)\n", m_shared_credit, m_shared_credit_limit);
---
>     fprintf(fp, "(limit = %d)\n", m_private_credit_limit); 
>     fprintf(fp, "shared_credit = %d (limit = %d)\n", m_shared_credit, m_shared_credit_limit); 
174c151
< bool memory_partition_unit::busy() const
---
> bool memory_partition_unit::busy() const 
176c153
<     bool busy = false;
---
>     bool busy = false; 
179c156
<             busy = true;
---
>             busy = true; 
182c159
<     return busy;
---
>     return busy; 
185c162
< void memory_partition_unit::cache_cycle(unsigned cycle)
---
> void memory_partition_unit::cache_cycle(unsigned cycle) 
188c165
<         m_sub_partition[p]->cache_cycle(cycle);
---
>         m_sub_partition[p]->cache_cycle(cycle); 
192c169
< void memory_partition_unit::visualizer_print( gzFile visualizer_file ) const
---
> void memory_partition_unit::visualizer_print( gzFile visualizer_file ) const 
194,197c171,174
< //     // m_dram->visualizer_print(visualizer_file);
< //     // for (unsigned p = 0; p < m_config->m_n_sub_partition_per_memory_channel; p++) {
< //     //     m_sub_partition[p]->visualizer_print(visualizer_file);
< //     // }
---
>     m_dram->visualizer_print(visualizer_file);
>     for (unsigned p = 0; p < m_config->m_n_sub_partition_per_memory_channel; p++) {
>         m_sub_partition[p]->visualizer_print(visualizer_file); 
>     }
200,201c177,178
< // determine whether a given subpartition can issue to DRAM
< bool memory_partition_unit::can_issue_to_dram(int inner_sub_partition_id)
---
> // determine whether a given subpartition can issue to DRAM 
> bool memory_partition_unit::can_issue_to_dram(int inner_sub_partition_id) 
203,205c180,182
<     int spid = inner_sub_partition_id;
<     bool sub_partition_contention = m_sub_partition[spid]->dram_L2_queue_full();
<     bool has_dram_resource = m_arbitration_metadata.has_credits(spid);
---
>     int spid = inner_sub_partition_id; 
>     bool sub_partition_contention = m_sub_partition[spid]->dram_L2_queue_full(); 
>     bool has_dram_resource = m_arbitration_metadata.has_credits(spid); 
207,208c184,185
<     MEMPART_DPRINTF("sub partition %d sub_partition_contention=%c has_dram_resource=%c\n",
<                     spid, (sub_partition_contention) ? 'T' : 'F', (has_dram_resource) ? 'T' : 'F');
---
>     MEMPART_DPRINTF("sub partition %d sub_partition_contention=%c has_dram_resource=%c\n", 
>                     spid, (sub_partition_contention)? 'T':'F', (has_dram_resource)? 'T':'F'); 
210c187
<     return (has_dram_resource && !sub_partition_contention);
---
>     return (has_dram_resource && !sub_partition_contention); 
215c192
<     return (global_sub_partition_id - m_id * m_config->m_n_sub_partition_per_memory_channel);
---
>     return (global_sub_partition_id - m_id * m_config->m_n_sub_partition_per_memory_channel); 
218,222c195,198
< void memory_partition_unit::dram_cycle()
< {
<     /*    Original
<     // pop completed memory request from dram and push it to dram-to-L2 queue
<     // of the original sub partition
---
> void memory_partition_unit::dram_cycle() 
> { 
>     // pop completed memory request from dram and push it to dram-to-L2 queue 
>     // of the original sub partition 
225,227c201,203
<         unsigned dest_global_spid = mf_return->get_sub_partition_id();
<         int dest_spid = global_sub_partition_id_to_local_id(dest_global_spid);
<         assert(m_sub_partition[dest_spid]->get_id() == dest_global_spid);
---
>         unsigned dest_global_spid = mf_return->get_sub_partition_id(); 
>         int dest_spid = global_sub_partition_id_to_local_id(dest_global_spid); 
>         assert(m_sub_partition[dest_spid]->get_id() == dest_global_spid); 
229,230c205,206
<             if ( mf_return->get_access_type() == L1_WRBK_ACC ) {
<                 m_sub_partition[dest_spid]->set_done(mf_return);
---
>             if( mf_return->get_access_type() == L1_WRBK_ACC ) {
>                 m_sub_partition[dest_spid]->set_done(mf_return); 
234,236c210,212
<                 mf_return->set_status(IN_PARTITION_DRAM_TO_L2_QUEUE, gpu_sim_cycle + gpu_tot_sim_cycle);
<                 m_arbitration_metadata.return_credit(dest_spid);
<                 MEMPART_DPRINTF("mem_fetch request %p return from dram to sub partition %d\n", mf_return, dest_spid);
---
>                 mf_return->set_status(IN_PARTITION_DRAM_TO_L2_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
>                 m_arbitration_metadata.return_credit(dest_spid); 
>                 MEMPART_DPRINTF("mem_fetch request %p return from dram to sub partition %d\n", mf_return, dest_spid); 
238c214
<             m_dram->return_queue_pop();
---
>             m_dram->return_queue_pop(); 
241c217
<         m_dram->return_queue_pop();
---
>         m_dram->return_queue_pop(); 
242a219,221
>     
>     m_dram->cycle(); 
>     m_dram->dram_log(SAMPLELOG);   
244,247c223
<     m_dram->cycle();
<     m_dram->dram_log(SAMPLELOG);
< 
<     if ( !m_dram->full() ) {
---
>     if( !m_dram->full() ) {
249,250c225,226
<         // Arbitrate among multiple L2 subpartitions
<         int last_issued_partition = m_arbitration_metadata.last_borrower();
---
>         // Arbitrate among multiple L2 subpartitions 
>         int last_issued_partition = m_arbitration_metadata.last_borrower(); 
252c228
<             int spid = (p + last_issued_partition + 1) % m_config->m_n_sub_partition_per_memory_channel;
---
>             int spid = (p + last_issued_partition + 1) % m_config->m_n_sub_partition_per_memory_channel; 
256c232
<                 MEMPART_DPRINTF("Issue mem_fetch request %p from sub partition %d to dram\n", mf, spid);
---
>                 MEMPART_DPRINTF("Issue mem_fetch request %p from sub partition %d to dram\n", mf, spid); 
259c235
<                 d.ready_cycle = gpu_sim_cycle + gpu_tot_sim_cycle + m_config->dram_latency;
---
>                 d.ready_cycle = gpu_sim_cycle+gpu_tot_sim_cycle + m_config->dram_latency;
261,263c237,239
<                 mf->set_status(IN_PARTITION_DRAM_LATENCY_QUEUE, gpu_sim_cycle + gpu_tot_sim_cycle);
<                 m_arbitration_metadata.borrow_credit(spid);
<                 break;  // the DRAM should only accept one request per cycle
---
>                 mf->set_status(IN_PARTITION_DRAM_LATENCY_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
>                 m_arbitration_metadata.borrow_credit(spid); 
>                 break;  // the DRAM should only accept one request per cycle 
269c245
<     if ( !m_dram_latency_queue.empty() && ( (gpu_sim_cycle + gpu_tot_sim_cycle) >= m_dram_latency_queue.front().ready_cycle ) && !m_dram->full() ) {
---
>     if( !m_dram_latency_queue.empty() && ( (gpu_sim_cycle+gpu_tot_sim_cycle) >= m_dram_latency_queue.front().ready_cycle ) && !m_dram->full() ) {
273,293d248
<     } */
< 
<     mem_fetch* mf_return = m_dram_r->r_return_queue_top();
<     if (mf_return) {
<         unsigned dest_global_spid = mf_return->get_sub_partition_id();
<         int dest_spid = global_sub_partition_id_to_local_id(dest_global_spid);
<         assert(m_sub_partition[dest_spid]->get_id() == dest_global_spid);
<         if (!m_sub_partition[dest_spid]->dram_L2_queue_full()) {
<             if ( mf_return->get_access_type() == L1_WRBK_ACC ) {
<                 m_sub_partition[dest_spid]->set_done(mf_return);
<                 delete mf_return;
<             } else {
<                 m_sub_partition[dest_spid]->dram_L2_queue_push(mf_return);
<                 mf_return->set_status(IN_PARTITION_DRAM_TO_L2_QUEUE, gpu_sim_cycle + gpu_tot_sim_cycle);
<                 m_arbitration_metadata.return_credit(dest_spid);
<                 MEMPART_DPRINTF("mem_fetch request %p return from dram to sub partition %d\n", mf_return, dest_spid);
<             }
<             m_dram_r->r_return_queue_pop();
<         }
<     } else {
<         m_dram_r->r_return_queue_pop();
295,357d249
<     m_dram_r->cycle(); // In this part, when read/write complete, the return q should be automatically written due to the call back function.
<     int last_issued_partition = m_arbitration_metadata.last_borrower();
<     for (unsigned p = 0; p < m_config->m_n_sub_partition_per_memory_channel; p++) {
<         int spid = (p + last_issued_partition + 1) % m_config->m_n_sub_partition_per_memory_channel;
<         if (!m_sub_partition[spid]->L2_dram_queue_empty() && can_issue_to_dram(spid)) {
<             mem_fetch *mf = m_sub_partition[spid]->L2_dram_queue_top();
< 
<             if (mf->is_write())
<             {   // 1 is for write, while 0 for read
<                 if ( !m_dram_r->full(1, (long)mf->get_addr()) )
<                 {
<                     m_sub_partition[spid]->L2_dram_queue_pop();
<                     MEMPART_DPRINTF("Issue mem_fetch request %p from sub partition %d to dram\n", mf, spid);
<                     dram_delay_t d;
<                     d.req = mf;
<                     d.ready_cycle = gpu_sim_cycle + gpu_tot_sim_cycle + m_config->dram_latency;
<                     m_dram_latency_queue.push_back(d);
<                     mf->set_status(IN_PARTITION_DRAM_LATENCY_QUEUE, gpu_sim_cycle + gpu_tot_sim_cycle);
<                     m_arbitration_metadata.borrow_credit(spid);
<                     break;  // the DRAM should only accept one request per cycle
< 
<                 }
<             } else
<             {
< 
<                 if ( !m_dram_r->full(0, (long)mf->get_addr()) )
<                 {
<                     m_sub_partition[spid]->L2_dram_queue_pop();
<                     MEMPART_DPRINTF("Issue mem_fetch request %p from sub partition %d to dram\n", mf, spid);
<                     dram_delay_t d;
<                     d.req = mf;
<                     d.ready_cycle = gpu_sim_cycle + gpu_tot_sim_cycle + m_config->dram_latency;
<                     m_dram_latency_queue.push_back(d);
<                     mf->set_status(IN_PARTITION_DRAM_LATENCY_QUEUE, gpu_sim_cycle + gpu_tot_sim_cycle);
<                     m_arbitration_metadata.borrow_credit(spid);
<                     break;  // the DRAM should only accept one request per cycle
<                 }
<             }
<         }
<     }
< 
<     if ( !m_dram_latency_queue.empty() && ( (gpu_sim_cycle + gpu_tot_sim_cycle) >= m_dram_latency_queue.front().ready_cycle ) ) {
< 
< 
<         mem_fetch* mf = m_dram_latency_queue.front().req;
<         if (mf->is_write())
<         {
<             if ( !m_dram_r->full(1, (long)mf->get_addr()) )
<             {
<                 m_dram_latency_queue.pop_front();
<                 m_dram_r->push(mf);
<             }
<         } else
<         {
<             if ( !m_dram_r->full(0, (long)mf->get_addr()) )
<             {
<                 m_dram_latency_queue.pop_front();
<                 m_dram_r->push(mf);
<             }
<         }
<     }
< 
< 
360c252
< void memory_partition_unit::set_done( mem_fetch * mf )
---
> void memory_partition_unit::set_done( mem_fetch *mf )
362,364c254,256
<     unsigned global_spid = mf->get_sub_partition_id();
<     int spid = global_sub_partition_id_to_local_id(global_spid);
<     assert(m_sub_partition[spid]->get_id() == global_spid);
---
>     unsigned global_spid = mf->get_sub_partition_id(); 
>     int spid = global_sub_partition_id_to_local_id(global_spid); 
>     assert(m_sub_partition[spid]->get_id() == global_spid); 
366,367c258,259
<         m_arbitration_metadata.return_credit(spid);
<         MEMPART_DPRINTF("mem_fetch request %p return from dram to sub partition %d\n", mf, spid);
---
>         m_arbitration_metadata.return_credit(spid); 
>         MEMPART_DPRINTF("mem_fetch request %p return from dram to sub partition %d\n", mf, spid); 
369c261
<     m_sub_partition[spid]->set_done(mf);
---
>     m_sub_partition[spid]->set_done(mf); 
372c264,271
< void memory_partition_unit::print_stat( FILE * fp ) const
---
> void memory_partition_unit::set_dram_power_stats(unsigned &n_cmd,
>                                                  unsigned &n_activity,
>                                                  unsigned &n_nop,
>                                                  unsigned &n_act,
>                                                  unsigned &n_pre,
>                                                  unsigned &n_rd,
>                                                  unsigned &n_wr,
>                                                  unsigned &n_req) const
374,375c273
<     m_dram_r->finish();
<     //FIX ME to print the statistics data
---
>     m_dram->set_dram_power_stats(n_cmd, n_activity, n_nop, n_act, n_pre, n_rd, n_wr, n_req);
378,390c276
< // void memory_partition_unit::set_dram_power_stats(unsigned & n_cmd,
< //         unsigned & n_activity,
< //         unsigned & n_nop,
< //         unsigned & n_act,
< //         unsigned & n_pre,
< //         unsigned & n_rd,
< //         unsigned & n_wr,
< //         unsigned & n_req) const
< // {
< //     m_dram->set_dram_power_stats(n_cmd, n_activity, n_nop, n_act, n_pre, n_rd, n_wr, n_req);
< // }
< 
< void memory_partition_unit::print( FILE * fp ) const
---
> void memory_partition_unit::print( FILE *fp ) const
392c278
<     fprintf(fp, "Memory Partition %u: \n", m_id);
---
>     fprintf(fp, "Memory Partition %u: \n", m_id); 
394,404c280
<         m_sub_partition[p]->print(fp);
<     }
<     fprintf(fp, "In Dram Latency Queue (total = %zd): \n", m_dram_latency_queue.size());
<     for (std::list<dram_delay_t>::const_iterator mf_dlq = m_dram_latency_queue.begin();
<             mf_dlq != m_dram_latency_queue.end(); ++mf_dlq) {
<         mem_fetch *mf = mf_dlq->req;
<         fprintf(fp, "Ready @ %llu - ", mf_dlq->ready_cycle);
<         if (mf)
<             mf->print(fp);
<         else
<             fprintf(fp, " <NULL mem_fetch?>\n");
---
>         m_sub_partition[p]->print(fp); 
406,415c282,297
<     ///FIX ME to get the statistics data
<     m_dram_r->finish();
< }
< 
< 
< 
< 
< memory_sub_partition::memory_sub_partition( unsigned sub_partition_id,
<         const struct memory_config * config,
<         class memory_stats_t *stats )
---
>     fprintf(fp, "In Dram Latency Queue (total = %zd): \n", m_dram_latency_queue.size()); 
>     for (std::list<dram_delay_t>::const_iterator mf_dlq = m_dram_latency_queue.begin(); 
>          mf_dlq != m_dram_latency_queue.end(); ++mf_dlq) {
>         mem_fetch *mf = mf_dlq->req; 
>         fprintf(fp, "Ready @ %llu - ", mf_dlq->ready_cycle); 
>         if (mf) 
>             mf->print(fp); 
>         else 
>             fprintf(fp, " <NULL mem_fetch?>\n"); 
>     }
>     m_dram->print(fp); 
> }
> 
> memory_sub_partition::memory_sub_partition( unsigned sub_partition_id, 
>                                             const struct memory_config *config,
>                                             class memory_stats_t *stats )
418,419c300,301
<     m_config = config;
<     m_stats = stats;
---
>     m_config=config;
>     m_stats=stats;
421c303
<     assert(m_id < m_config->m_n_mem_sub_partition);
---
>     assert(m_id < m_config->m_n_mem_sub_partition); 
428,429c310,311
<     if (!m_config->m_L2_config.disabled())
<         m_L2cache = new l2_cache(L2c_name, m_config->m_L2_config, -1, -1, m_L2interface, m_mf_allocator, IN_PARTITION_L2_MISS_QUEUE);
---
>     if(!m_config->m_L2_config.disabled())
>        m_L2cache = new l2_cache(L2c_name,m_config->m_L2_config,-1,-1,m_L2interface,m_mf_allocator,IN_PARTITION_L2_MISS_QUEUE);
435,440c317,322
<     sscanf(m_config->gpgpu_L2_queue_config, "%u:%u:%u:%u", &icnt_L2, &L2_dram, &dram_L2, &L2_icnt );
<     m_icnt_L2_queue = new fifo_pipeline<mem_fetch>("icnt-to-L2", 0, icnt_L2);
<     m_L2_dram_queue = new fifo_pipeline<mem_fetch>("L2-to-dram", 0, L2_dram);
<     m_dram_L2_queue = new fifo_pipeline<mem_fetch>("dram-to-L2", 0, dram_L2);
<     m_L2_icnt_queue = new fifo_pipeline<mem_fetch>("L2-to-icnt", 0, L2_icnt);
<     wb_addr = -1;
---
>     sscanf(m_config->gpgpu_L2_queue_config,"%u:%u:%u:%u", &icnt_L2,&L2_dram,&dram_L2,&L2_icnt );
>     m_icnt_L2_queue = new fifo_pipeline<mem_fetch>("icnt-to-L2",0,icnt_L2); 
>     m_L2_dram_queue = new fifo_pipeline<mem_fetch>("L2-to-dram",0,L2_dram);
>     m_dram_L2_queue = new fifo_pipeline<mem_fetch>("dram-to-L2",0,dram_L2);
>     m_L2_icnt_queue = new fifo_pipeline<mem_fetch>("L2-to-icnt",0,L2_icnt);
>     wb_addr=-1;
456,467c338,349
<     if ( !m_config->m_L2_config.disabled()) {
<         if ( m_L2cache->access_ready() && !m_L2_icnt_queue->full() ) {
<             mem_fetch *mf = m_L2cache->next_access();
<             if (mf->get_access_type() != L2_WR_ALLOC_R) { // Don't pass write allocate read request back to upper level cache
<                 mf->set_reply();
<                 mf->set_status(IN_PARTITION_L2_TO_ICNT_QUEUE, gpu_sim_cycle + gpu_tot_sim_cycle);
<                 m_L2_icnt_queue->push(mf);
<             } else {
<                 m_request_tracker.erase(mf);
<                 delete mf;
<             }
<         }
---
>     if( !m_config->m_L2_config.disabled()) {
>        if ( m_L2cache->access_ready() && !m_L2_icnt_queue->full() ) {
>            mem_fetch *mf = m_L2cache->next_access();
>            if(mf->get_access_type() != L2_WR_ALLOC_R){ // Don't pass write allocate read request back to upper level cache
> 				mf->set_reply();
> 				mf->set_status(IN_PARTITION_L2_TO_ICNT_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
> 				m_L2_icnt_queue->push(mf);
>            }else{
> 				m_request_tracker.erase(mf);
> 				delete mf;
>            }
>        }
475,476c357,358
<                 mf->set_status(IN_PARTITION_L2_FILL_QUEUE, gpu_sim_cycle + gpu_tot_sim_cycle);
<                 m_L2cache->fill(mf, gpu_sim_cycle + gpu_tot_sim_cycle);
---
>                 mf->set_status(IN_PARTITION_L2_FILL_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
>                 m_L2cache->fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle);
480c362
<             mf->set_status(IN_PARTITION_L2_TO_ICNT_QUEUE, gpu_sim_cycle + gpu_tot_sim_cycle);
---
>             mf->set_status(IN_PARTITION_L2_TO_ICNT_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
487,488c369,370
<     if ( !m_config->m_L2_config.disabled() )
<         m_L2cache->cycle();
---
>     if( !m_config->m_L2_config.disabled() )
>        m_L2cache->cycle();
494c376
<                 ( (m_config->m_L2_texure_only && mf->istexture()) || (!m_config->m_L2_texure_only) )
---
>               ( (m_config->m_L2_texure_only && mf->istexture()) || (!m_config->m_L2_texure_only) )
497,498c379,380
<             bool output_full = m_L2_icnt_queue->full();
<             bool port_free = m_L2cache->data_port_free();
---
>             bool output_full = m_L2_icnt_queue->full(); 
>             bool port_free = m_L2cache->data_port_free(); 
501c383
<                 enum cache_request_status status = m_L2cache->access(mf->get_addr(), mf, gpu_sim_cycle + gpu_tot_sim_cycle, events);
---
>                 enum cache_request_status status = m_L2cache->access(mf->get_addr(),mf,gpu_sim_cycle+gpu_tot_sim_cycle,events);
506c388
<                     if ( !write_sent ) {
---
>                     if( !write_sent ) {
509c391
<                         if ( mf->get_access_type() == L1_WRBK_ACC ) {
---
>                         if( mf->get_access_type() == L1_WRBK_ACC ) {
514c396
<                             mf->set_status(IN_PARTITION_L2_TO_ICNT_QUEUE, gpu_sim_cycle + gpu_tot_sim_cycle);
---
>                             mf->set_status(IN_PARTITION_L2_TO_ICNT_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
533c415
<             mf->set_status(IN_PARTITION_L2_TO_DRAM_QUEUE, gpu_sim_cycle + gpu_tot_sim_cycle);
---
>             mf->set_status(IN_PARTITION_L2_TO_DRAM_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
540c422
<     if ( !m_rop.empty() && (cycle >= m_rop.front().ready_cycle) && !m_icnt_L2_queue->full() ) {
---
>     if( !m_rop.empty() && (cycle >= m_rop.front().ready_cycle) && !m_icnt_L2_queue->full() ) {
544c426
<         mf->set_status(IN_PARTITION_ICNT_TO_L2_QUEUE, gpu_sim_cycle + gpu_tot_sim_cycle);
---
>         mf->set_status(IN_PARTITION_ICNT_TO_L2_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
555c437
<     return m_L2_dram_queue->empty();
---
>    return m_L2_dram_queue->empty(); 
560c442
<     return m_L2_dram_queue->top();
---
>    return m_L2_dram_queue->top(); 
563c445
< void memory_sub_partition::L2_dram_queue_pop()
---
> void memory_sub_partition::L2_dram_queue_pop() 
565c447
<     m_L2_dram_queue->pop();
---
>    m_L2_dram_queue->pop(); 
570c452
<     return m_dram_L2_queue->full();
---
>    return m_dram_L2_queue->full(); 
573c455
< void memory_sub_partition::dram_L2_queue_push( class mem_fetch * mf )
---
> void memory_sub_partition::dram_L2_queue_push( class mem_fetch* mf )
575c457
<     m_dram_L2_queue->push(mf);
---
>    m_dram_L2_queue->push(mf); 
578c460
< void memory_sub_partition::print_cache_stat(unsigned & accesses, unsigned & misses) const
---
> void memory_sub_partition::print_cache_stat(unsigned &accesses, unsigned &misses) const
581,582c463,464
<     if ( !m_config->m_L2_config.disabled() )
<         m_L2cache->print(fp, accesses, misses);
---
>     if( !m_config->m_L2_config.disabled() )
>        m_L2cache->print(fp,accesses,misses);
585c467
< void memory_sub_partition::print( FILE * fp ) const
---
> void memory_sub_partition::print( FILE *fp ) const
588,589c470,471
<         fprintf(fp, "Memory Sub Parition %u: pending memory requests:\n", m_id);
<         for ( std::set<mem_fetch*>::const_iterator r = m_request_tracker.begin(); r != m_request_tracker.end(); ++r ) {
---
>         fprintf(fp,"Memory Sub Parition %u: pending memory requests:\n", m_id);
>         for ( std::set<mem_fetch*>::const_iterator r=m_request_tracker.begin(); r != m_request_tracker.end(); ++r ) {
594c476
<                 fprintf(fp, " <NULL mem_fetch?>\n");
---
>                 fprintf(fp," <NULL mem_fetch?>\n");
597,598c479,480
<     if ( !m_config->m_L2_config.disabled() )
<         m_L2cache->display_state(fp);
---
>     if( !m_config->m_L2_config.disabled() )
>        m_L2cache->display_state(fp);
601,602d482
< 
< 
605,647c485,527
<     // gzprintf(visualizer_file, "Ltwowritemiss: %d\n", L2_write_miss);
<     // gzprintf(visualizer_file, "Ltwowritehit: %d\n",  L2_write_access-L2_write_miss);
<     // gzprintf(visualizer_file, "Ltworeadmiss: %d\n", L2_read_miss);
<     // gzprintf(visualizer_file, "Ltworeadhit: %d\n", L2_read_access-L2_read_miss);
<     if (num_mfs)
<         gzprintf(visualizer_file, "averagemflatency: %lld\n", mf_total_lat / num_mfs);
< }
< 
< void gpgpu_sim::print_dram_stats(FILE * fout) const
< {
<     unsigned cmd = 0;
<     unsigned activity = 0;
<     unsigned nop = 0;
<     unsigned act = 0;
<     unsigned pre = 0;
<     unsigned rd = 0;
<     unsigned wr = 0;
<     unsigned req = 0;
<     unsigned tot_cmd = 0;
<     unsigned tot_nop = 0;
<     unsigned tot_act = 0;
<     unsigned tot_pre = 0;
<     unsigned tot_rd = 0;
<     unsigned tot_wr = 0;
<     unsigned tot_req = 0;
< 
<     for (unsigned i = 0; i < m_memory_config->m_n_mem; i++) {
<         // m_memory_partition_unit[i]->set_dram_power_stats(cmd, activity, nop, act, pre, rd, wr, req);
<         tot_cmd += cmd;
<         tot_nop += nop;
<         tot_act += act;
<         tot_pre += pre;
<         tot_rd += rd;
<         tot_wr += wr;
<         tot_req += req;
<     }
<     fprintf(fout, "gpgpu_n_dram_reads = %d\n", tot_rd );
<     fprintf(fout, "gpgpu_n_dram_writes = %d\n", tot_wr );
<     fprintf(fout, "gpgpu_n_dram_activate = %d\n", tot_act );
<     fprintf(fout, "gpgpu_n_dram_commands = %d\n", tot_cmd);
<     fprintf(fout, "gpgpu_n_dram_noops = %d\n", tot_nop );
<     fprintf(fout, "gpgpu_n_dram_precharges = %d\n", tot_pre );
<     fprintf(fout, "gpgpu_n_dram_requests = %d\n", tot_req );
---
>    // gzprintf(visualizer_file, "Ltwowritemiss: %d\n", L2_write_miss);
>    // gzprintf(visualizer_file, "Ltwowritehit: %d\n",  L2_write_access-L2_write_miss);
>    // gzprintf(visualizer_file, "Ltworeadmiss: %d\n", L2_read_miss);
>    // gzprintf(visualizer_file, "Ltworeadhit: %d\n", L2_read_access-L2_read_miss);
>    if (num_mfs)
>       gzprintf(visualizer_file, "averagemflatency: %lld\n", mf_total_lat/num_mfs);
> }
> 
> void gpgpu_sim::print_dram_stats(FILE *fout) const
> {
> 	unsigned cmd=0;
> 	unsigned activity=0;
> 	unsigned nop=0;
> 	unsigned act=0;
> 	unsigned pre=0;
> 	unsigned rd=0;
> 	unsigned wr=0;
> 	unsigned req=0;
> 	unsigned tot_cmd=0;
> 	unsigned tot_nop=0;
> 	unsigned tot_act=0;
> 	unsigned tot_pre=0;
> 	unsigned tot_rd=0;
> 	unsigned tot_wr=0;
> 	unsigned tot_req=0;
> 
> 	for (unsigned i=0;i<m_memory_config->m_n_mem;i++){
> 		m_memory_partition_unit[i]->set_dram_power_stats(cmd,activity,nop,act,pre,rd,wr,req);
> 		tot_cmd+=cmd;
> 		tot_nop+=nop;
> 		tot_act+=act;
> 		tot_pre+=pre;
> 		tot_rd+=rd;
> 		tot_wr+=wr;
> 		tot_req+=req;
> 	}
>     fprintf(fout,"gpgpu_n_dram_reads = %d\n",tot_rd );
>     fprintf(fout,"gpgpu_n_dram_writes = %d\n",tot_wr );
>     fprintf(fout,"gpgpu_n_dram_activate = %d\n",tot_act );
>     fprintf(fout,"gpgpu_n_dram_commands = %d\n",tot_cmd);
>     fprintf(fout,"gpgpu_n_dram_noops = %d\n",tot_nop );
>     fprintf(fout,"gpgpu_n_dram_precharges = %d\n",tot_pre );
>     fprintf(fout,"gpgpu_n_dram_requests = %d\n",tot_req );
650,651c530,531
< unsigned memory_sub_partition::flushL2()
< {
---
> unsigned memory_sub_partition::flushL2() 
> { 
653c533
<         m_L2cache->flush();
---
>         m_L2cache->flush(); 
658c538
< bool memory_sub_partition::busy() const
---
> bool memory_sub_partition::busy() const 
663c543
< void memory_sub_partition::push( mem_fetch * req, unsigned long long cycle )
---
> void memory_sub_partition::push( mem_fetch* req, unsigned long long cycle ) 
668c548
<         if ( req->istexture() ) {
---
>         if( req->istexture() ) {
670c550
<             req->set_status(IN_PARTITION_ICNT_TO_L2_QUEUE, gpu_sim_cycle + gpu_tot_sim_cycle);
---
>             req->set_status(IN_PARTITION_ICNT_TO_L2_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
676c556
<             req->set_status(IN_PARTITION_ROP_DELAY, gpu_sim_cycle + gpu_tot_sim_cycle);
---
>             req->set_status(IN_PARTITION_ROP_DELAY,gpu_sim_cycle+gpu_tot_sim_cycle);
681c561
< mem_fetch* memory_sub_partition::pop()
---
> mem_fetch* memory_sub_partition::pop() 
687c567
<     if ( mf && (mf->get_access_type() == L2_WRBK_ACC || mf->get_access_type() == L1_WRBK_ACC) ) {
---
>     if( mf && (mf->get_access_type() == L2_WRBK_ACC || mf->get_access_type() == L1_WRBK_ACC) ) {
690c570
<     }
---
>     } 
694c574
< mem_fetch* memory_sub_partition::top()
---
> mem_fetch* memory_sub_partition::top() 
697c577
<     if ( mf && (mf->get_access_type() == L2_WRBK_ACC || mf->get_access_type() == L1_WRBK_ACC) ) {
---
>     if( mf && (mf->get_access_type() == L2_WRBK_ACC || mf->get_access_type() == L1_WRBK_ACC) ) {
702c582
<     }
---
>     } 
706c586
< void memory_sub_partition::set_done( mem_fetch * mf )
---
> void memory_sub_partition::set_done( mem_fetch *mf )
711c591
< void memory_sub_partition::accumulate_L2cache_stats(class cache_stats & l2_stats) const {
---
> void memory_sub_partition::accumulate_L2cache_stats(class cache_stats &l2_stats) const {
717c597
< void memory_sub_partition::get_L2cache_sub_stats(struct cache_sub_stats & css) const {
---
> void memory_sub_partition::get_L2cache_sub_stats(struct cache_sub_stats &css) const{
725c605
<     // TODO: Add visualizer stats for L2 cache
---
>     // TODO: Add visualizer stats for L2 cache 
diff -r gpgpu-sim/l2cache.h ../../../gpgpu-sim_distribution/src/gpgpu-sim/l2cache.h
31c31
< // #include "dram.h"
---
> #include "dram.h"
33,34d32
< #include "../ramulator_sim/gpu_wrapper.h"
< 
38,39c36
< #include <zlib.h>
< #include <set>
---
> 
44,53c41,50
<   partition_mf_allocator( const memory_config *config )
<   {
<     m_memory_config = config;
<   }
<   virtual mem_fetch * alloc(const class warp_inst_t &inst, const mem_access_t &access) const
<   {
<     abort();
<     return NULL;
<   }
<   virtual mem_fetch * alloc(new_addr_type addr, mem_access_type type, unsigned size, bool wr) const;
---
>     partition_mf_allocator( const memory_config *config )
>     {
>         m_memory_config = config;
>     }
>     virtual mem_fetch * alloc(const class warp_inst_t &inst, const mem_access_t &access) const 
>     {
>         abort();
>         return NULL;
>     }
>     virtual mem_fetch * alloc(new_addr_type addr, mem_access_type type, unsigned size, bool wr) const;
55c52
<   const memory_config *m_memory_config;
---
>     const memory_config *m_memory_config;
58,60c55,57
< // Memory partition unit contains all the units assolcated with a single DRAM channel.
< // - It arbitrates the DRAM channel among multiple sub partitions.
< // - It does not connect directly with the interconnection network.
---
> // Memory partition unit contains all the units assolcated with a single DRAM channel. 
> // - It arbitrates the DRAM channel among multiple sub partitions.  
> // - It does not connect directly with the interconnection network. 
63,151c60,141
< public:
<   memory_partition_unit( unsigned partition_id, const struct memory_config *config, class memory_stats_t *stats );
<   ~memory_partition_unit();
< 
<   bool busy() const;
< 
<   void cache_cycle( unsigned cycle );
<   void dram_cycle();
< 
<   void set_done( mem_fetch *mf );
< 
<   void visualizer_print( gzFile visualizer_file ) const;
<   void print_stat( FILE *fp ) const;
< 
<   // {
<   //   m_dram_r->finish();
<   //   //FIX ME: print the m_dram statistics
<   //   //m_dram_r->print_stat(fp);
<   // }
<   //void visualize() const { m_dram->visualize(); }
<   void print( FILE *fp ) const;
< 
<   class memory_sub_partition * get_sub_partition(int sub_partition_id)
<   {
<     return m_sub_partition[sub_partition_id];
<   }
< 
<   // Power model
<   void set_dram_power_stats(unsigned &n_cmd,
<                             unsigned &n_activity,
<                             unsigned &n_nop,
<                             unsigned &n_act,
<                             unsigned &n_pre,
<                             unsigned &n_rd,
<                             unsigned &n_wr,
<                             unsigned &n_req) const;
< 
<   int global_sub_partition_id_to_local_id(int global_sub_partition_id) const;
< 
<   unsigned get_mpid() const { return m_id; }
< 
< private:
< 
<   unsigned m_id;
<   const struct memory_config *m_config;
<   class memory_stats_t *m_stats;
<   class memory_sub_partition **m_sub_partition;
<   //class dram_t *m_dram;
<   class GpuWrapper *m_dram_r;
< 
<   class arbitration_metadata
<   {
<   public:
<     arbitration_metadata(const struct memory_config *config);
< 
<     // check if a subpartition still has credit
<     bool has_credits(int inner_sub_partition_id) const;
<     // borrow a credit for a subpartition
<     void borrow_credit(int inner_sub_partition_id);
<     // return a credit from a subpartition
<     void return_credit(int inner_sub_partition_id);
< 
<     // return the last subpartition that borrowed credit
<     int last_borrower() const { return m_last_borrower; }
< 
<     void print( FILE *fp ) const;
<   private:
<     // id of the last subpartition that borrowed credit
<     int m_last_borrower;
< 
<     int m_shared_credit_limit;
<     int m_private_credit_limit;
< 
<     // credits borrowed by the subpartitions
<     std::vector<int> m_private_credit;
<     int m_shared_credit;
<   };
<   arbitration_metadata m_arbitration_metadata;
< 
<   // determine wheither a given subpartition can issue to DRAM
<   bool can_issue_to_dram(int inner_sub_partition_id);
< 
<   // model DRAM access scheduler latency (fixed latency between L2 and DRAM)
<   struct dram_delay_t
<   {
<     unsigned long long ready_cycle;
<     class mem_fetch* req;
<   };
<   std::list<dram_delay_t> m_dram_latency_queue;
---
> public: 
>    memory_partition_unit( unsigned partition_id, const struct memory_config *config, class memory_stats_t *stats );
>    ~memory_partition_unit(); 
> 
>    bool busy() const;
> 
>    void cache_cycle( unsigned cycle );
>    void dram_cycle();
> 
>    void set_done( mem_fetch *mf );
> 
>    void visualizer_print( gzFile visualizer_file ) const;
>    void print_stat( FILE *fp ) { m_dram->print_stat(fp); }
>    void visualize() const { m_dram->visualize(); }
>    void print( FILE *fp ) const;
> 
>    class memory_sub_partition * get_sub_partition(int sub_partition_id) 
>    {
>       return m_sub_partition[sub_partition_id]; 
>    }
> 
>    // Power model
>    void set_dram_power_stats(unsigned &n_cmd,
>                              unsigned &n_activity,
>                              unsigned &n_nop,
>                              unsigned &n_act,
>                              unsigned &n_pre,
>                              unsigned &n_rd,
>                              unsigned &n_wr,
>                              unsigned &n_req) const;
> 
>    int global_sub_partition_id_to_local_id(int global_sub_partition_id) const; 
> 
>    unsigned get_mpid() const { return m_id; }
> 
> private: 
> 
>    unsigned m_id;
>    const struct memory_config *m_config;
>    class memory_stats_t *m_stats;
>    class memory_sub_partition **m_sub_partition; 
>    class dram_t *m_dram;
> 
>    class arbitration_metadata
>    {
>    public: 
>       arbitration_metadata(const struct memory_config *config); 
> 
>       // check if a subpartition still has credit 
>       bool has_credits(int inner_sub_partition_id) const; 
>       // borrow a credit for a subpartition 
>       void borrow_credit(int inner_sub_partition_id); 
>       // return a credit from a subpartition 
>       void return_credit(int inner_sub_partition_id); 
> 
>       // return the last subpartition that borrowed credit 
>       int last_borrower() const { return m_last_borrower; } 
> 
>       void print( FILE *fp ) const; 
>    private: 
>       // id of the last subpartition that borrowed credit 
>       int m_last_borrower; 
> 
>       int m_shared_credit_limit; 
>       int m_private_credit_limit; 
> 
>       // credits borrowed by the subpartitions
>       std::vector<int> m_private_credit; 
>       int m_shared_credit; 
>    }; 
>    arbitration_metadata m_arbitration_metadata; 
> 
>    // determine wheither a given subpartition can issue to DRAM 
>    bool can_issue_to_dram(int inner_sub_partition_id); 
> 
>    // model DRAM access scheduler latency (fixed latency between L2 and DRAM)
>    struct dram_delay_t
>    {
>       unsigned long long ready_cycle;
>       class mem_fetch* req;
>    };
>    std::list<dram_delay_t> m_dram_latency_queue;
157,158c147,148
<   memory_sub_partition( unsigned sub_partition_id, const struct memory_config *config, class memory_stats_t *stats );
<   ~memory_sub_partition();
---
>    memory_sub_partition( unsigned sub_partition_id, const struct memory_config *config, class memory_stats_t *stats );
>    ~memory_sub_partition(); 
160c150
<   unsigned get_id() const { return m_id; }
---
>    unsigned get_id() const { return m_id; } 
162c152
<   bool busy() const;
---
>    bool busy() const;
164c154
<   void cache_cycle( unsigned cycle );
---
>    void cache_cycle( unsigned cycle );
166,170c156,160
<   bool full() const;
<   void push( class mem_fetch* mf, unsigned long long clock_cycle );
<   class mem_fetch* pop();
<   class mem_fetch* top();
<   void set_done( mem_fetch *mf );
---
>    bool full() const;
>    void push( class mem_fetch* mf, unsigned long long clock_cycle );
>    class mem_fetch* pop(); 
>    class mem_fetch* top();
>    void set_done( mem_fetch *mf );
172c162
<   unsigned flushL2();
---
>    unsigned flushL2();
174,177c164,167
<   // interface to L2_dram_queue
<   bool L2_dram_queue_empty() const;
<   class mem_fetch* L2_dram_queue_top() const;
<   void L2_dram_queue_pop();
---
>    // interface to L2_dram_queue
>    bool L2_dram_queue_empty() const; 
>    class mem_fetch* L2_dram_queue_top() const; 
>    void L2_dram_queue_pop(); 
179,181c169,171
<   // interface to dram_L2_queue
<   bool dram_L2_queue_full() const;
<   void dram_L2_queue_push( class mem_fetch* mf );
---
>    // interface to dram_L2_queue
>    bool dram_L2_queue_full() const; 
>    void dram_L2_queue_push( class mem_fetch* mf ); 
183,185c173,175
<   void visualizer_print( gzFile visualizer_file );
<   void print_cache_stat(unsigned &accesses, unsigned &misses) const;
<   void print( FILE *fp ) const;
---
>    void visualizer_print( gzFile visualizer_file );
>    void print_cache_stat(unsigned &accesses, unsigned &misses) const;
>    void print( FILE *fp ) const;
187,188c177,178
<   void accumulate_L2cache_stats(class cache_stats &l2_stats) const;
<   void get_L2cache_sub_stats(struct cache_sub_stats &css) const;
---
>    void accumulate_L2cache_stats(class cache_stats &l2_stats) const;
>    void get_L2cache_sub_stats(struct cache_sub_stats &css) const;
192,210c182,200
<   unsigned m_id;  //< the global sub partition ID
<   const struct memory_config *m_config;
<   class l2_cache *m_L2cache;
<   class L2interface *m_L2interface;
<   partition_mf_allocator *m_mf_allocator;
< 
<   // model delay of ROP units with a fixed latency
<   struct rop_delay_t
<   {
<     unsigned long long ready_cycle;
<     class mem_fetch* req;
<   };
<   std::queue<rop_delay_t> m_rop;
< 
<   // these are various FIFOs between units within a memory partition
<   fifo_pipeline<mem_fetch> *m_icnt_L2_queue;
<   fifo_pipeline<mem_fetch> *m_L2_dram_queue;
<   fifo_pipeline<mem_fetch> *m_dram_L2_queue;
<   fifo_pipeline<mem_fetch> *m_L2_icnt_queue; // L2 cache hit response queue
---
>    unsigned m_id;  //< the global sub partition ID
>    const struct memory_config *m_config;
>    class l2_cache *m_L2cache;
>    class L2interface *m_L2interface;
>    partition_mf_allocator *m_mf_allocator;
> 
>    // model delay of ROP units with a fixed latency
>    struct rop_delay_t
>    {
>     	unsigned long long ready_cycle;
>     	class mem_fetch* req;
>    };
>    std::queue<rop_delay_t> m_rop;
> 
>    // these are various FIFOs between units within a memory partition
>    fifo_pipeline<mem_fetch> *m_icnt_L2_queue;
>    fifo_pipeline<mem_fetch> *m_L2_dram_queue;
>    fifo_pipeline<mem_fetch> *m_dram_L2_queue;
>    fifo_pipeline<mem_fetch> *m_L2_icnt_queue; // L2 cache hit response queue
212,213c202,203
<   class mem_fetch *L2dramout;
<   unsigned long long int wb_addr;
---
>    class mem_fetch *L2dramout; 
>    unsigned long long int wb_addr;
215c205
<   class memory_stats_t *m_stats;
---
>    class memory_stats_t *m_stats;
217c207
<   std::set<mem_fetch*> m_request_tracker;
---
>    std::set<mem_fetch*> m_request_tracker;
219c209
<   friend class L2interface;
---
>    friend class L2interface;
224,235c214,225
<   L2interface( memory_sub_partition *unit ) { m_unit = unit; }
<   virtual ~L2interface() {}
<   virtual bool full( unsigned size, bool write) const
<   {
<     // assume read and write packets all same size
<     return m_unit->m_L2_dram_queue->full();
<   }
<   virtual void push(mem_fetch *mf)
<   {
<     mf->set_status(IN_PARTITION_L2_TO_DRAM_QUEUE, 0/*FIXME*/);
<     m_unit->m_L2_dram_queue->push(mf);
<   }
---
>     L2interface( memory_sub_partition *unit ) { m_unit=unit; }
>     virtual ~L2interface() {}
>     virtual bool full( unsigned size, bool write) const 
>     {
>         // assume read and write packets all same size
>         return m_unit->m_L2_dram_queue->full();
>     }
>     virtual void push(mem_fetch *mf) 
>     {
>         mf->set_status(IN_PARTITION_L2_TO_DRAM_QUEUE,0/*FIXME*/);
>         m_unit->m_L2_dram_queue->push(mf);
>     }
237c227
<   memory_sub_partition *m_unit;
---
>     memory_sub_partition *m_unit;
diff -r gpgpu-sim/mem_latency_stat.cc ../../../gpgpu-sim_distribution/src/gpgpu-sim/mem_latency_stat.cc
39c39
< //#include "dram.h"
---
> #include "dram.h"
50c50
<    unsigned i, j;
---
>    unsigned i,j;
59c59
<    for (unsigned i = 0; i < mem_config->m_n_mem ; i++ ) {
---
>    for (unsigned i=0;i<mem_config->m_n_mem ;i++ ) {
68,72c68,72
<    m_n_shader = n_shader;
<    m_memory_config = mem_config;
<    total_n_access = 0;
<    total_n_reads = 0;
<    total_n_writes = 0;
---
>    m_n_shader=n_shader;
>    m_memory_config=mem_config;
>    total_n_access=0;
>    total_n_reads=0;
>    total_n_writes=0;
78,83c78,83
<    memset(mrq_lat_table, 0, sizeof(unsigned) * 32);
<    memset(dq_lat_table, 0, sizeof(unsigned) * 32);
<    memset(mf_lat_table, 0, sizeof(unsigned) * 32);
<    memset(icnt2mem_lat_table, 0, sizeof(unsigned) * 24);
<    memset(icnt2sh_lat_table, 0, sizeof(unsigned) * 24);
<    memset(mf_lat_pw_table, 0, sizeof(unsigned) * 32);
---
>    memset(mrq_lat_table, 0, sizeof(unsigned)*32);
>    memset(dq_lat_table, 0, sizeof(unsigned)*32);
>    memset(mf_lat_table, 0, sizeof(unsigned)*32);
>    memset(icnt2mem_lat_table, 0, sizeof(unsigned)*24);
>    memset(icnt2sh_lat_table, 0, sizeof(unsigned)*24);
>    memset(mf_lat_pw_table, 0, sizeof(unsigned)*32);
85c85
<    max_warps = n_shader * (shader_config->n_thread_per_shader / shader_config->warp_size + 1);
---
>    max_warps = n_shader * (shader_config->n_thread_per_shader / shader_config->warp_size+1);
97c97
<    num_MCBs_accessed = (unsigned int*) calloc(mem_config->m_n_mem * mem_config->nbk, sizeof(unsigned int));
---
>    num_MCBs_accessed = (unsigned int*) calloc(mem_config->m_n_mem*mem_config->nbk, sizeof(unsigned int));
102c102
<    for (i = 0; i < n_shader ; i++ ) {
---
>    for (i=0;i<n_shader ;i++ ) {
105c105
<       for (j = 0; j < mem_config->m_n_mem ; j++ ) {
---
>       for (j=0;j<mem_config->m_n_mem ;j++ ) {
111c111
<    for (i = 0; i < mem_config->m_n_mem ; i++ ) {
---
>    for (i=0;i<mem_config->m_n_mem ;i++ ) {
123,124c123,124
<       for (j = 0; (unsigned) j < mem_config->m_n_mem; j++) {
<          mem_access_type_stats[i][j] = (unsigned *) calloc((mem_config->nbk + 1), sizeof(unsigned*));
---
>       for (j=0; (unsigned) j< mem_config->m_n_mem; j++) {
>          mem_access_type_stats[i][j] = (unsigned *) calloc((mem_config->nbk+1), sizeof(unsigned*));
140c140
<    mf_latency = (gpu_sim_cycle + gpu_tot_sim_cycle) - mf->get_timestamp();
---
>    mf_latency = (gpu_sim_cycle+gpu_tot_sim_cycle) - mf->get_timestamp();
144c144
<    assert(idx < 32);
---
>    assert(idx<32);
157c157
<       if (mf_latency > mf_max_lat_table[mf->get_tlx_addr().chip][mf->get_tlx_addr().bk])
---
>       if (mf_latency > mf_max_lat_table[mf->get_tlx_addr().chip][mf->get_tlx_addr().bk]) 
160c160
<       icnt2sh_latency = (gpu_tot_sim_cycle + gpu_sim_cycle) - mf->get_return_timestamp();
---
>       icnt2sh_latency = (gpu_tot_sim_cycle+gpu_sim_cycle) - mf->get_return_timestamp();
171c171
<    if (m_memory_config->gpgpu_memlatency_stat) {
---
>    if (m_memory_config->gpgpu_memlatency_stat) { 
173c173
<          if ( mf->get_sid() < m_n_shader  ) {   //do not count L2_writebacks here
---
>          if ( mf->get_sid() < m_n_shader  ) {   //do not count L2_writebacks here 
185c185
<    if (mf->get_pc() != (unsigned) - 1)
---
>    if (mf->get_pc() != (unsigned)-1) 
193c193
<       icnt2mem_latency = (gpu_tot_sim_cycle + gpu_sim_cycle) - mf->get_timestamp();
---
>       icnt2mem_latency = (gpu_tot_sim_cycle+gpu_sim_cycle) - mf->get_timestamp();
206c206
<       mf_lat_pw_table[LOGB2(mf_tot_lat_pw / mf_num_lat_pw)]++;
---
>       mf_lat_pw_table[LOGB2(mf_tot_lat_pw/mf_num_lat_pw)]++;
215c215
<    unsigned i, j, k, l, m;
---
>    unsigned i,j,k,l,m;
223c223
<          printf("averagemflatency = %lld \n", mf_total_lat / num_mfs);
---
>          printf("averagemflatency = %lld \n", mf_total_lat/num_mfs);
228c228
<       for (i = 0; i < 32; i++) {
---
>       for (i=0; i< 32; i++) {
233c233
<       for (i = 0; i < 32; i++) {
---
>       for (i=0; i< 32; i++) {
238c238
<       for (i = 0; i < 32; i++) {
---
>       for (i=0; i< 32; i++) {
243c243
<       for (i = 0; i < 24; i++) {
---
>       for (i=0; i< 24; i++) {
248c248
<       for (i = 0; i < 24; i++) {
---
>       for (i=0; i< 24; i++) {
253c253
<       for (i = 0; i < 32; i++) {
---
>       for (i=0; i< 32; i++) {
260c260
<       for (i = 0; i < n_mem ; i++ ) {
---
>       for (i=0;i<n_mem ;i++ ) {
262,263c262,263
<          for (j = 0; j < gpu_mem_n_bk; j++ ) {
<             printf("%9d ", max_conc_access2samerow[i][j]);
---
>          for (j=0;j<gpu_mem_n_bk;j++ ) {
>             printf("%9d ",max_conc_access2samerow[i][j]);
270c270
<       for (i = 0; i < n_mem ; i++ ) {
---
>       for (i=0;i<n_mem ;i++ ) {
272,273c272,273
<          for (j = 0; j < gpu_mem_n_bk; j++ ) {
<             printf("%9d ", max_servicetime2samerow[i][j]);
---
>          for (j=0;j<gpu_mem_n_bk;j++ ) {
>             printf("%9d ",max_servicetime2samerow[i][j]);
282c282
<       for (i = 0; i < n_mem ; i++ ) {
---
>       for (i=0;i<n_mem ;i++ ) {
284c284
<          for (j = 0; j < gpu_mem_n_bk; j++ ) {
---
>          for (j=0;j<gpu_mem_n_bk;j++ ) {
287c287
<             printf("%9f ", (float) row_access[i][j] / num_activates[i][j]);
---
>             printf("%9f ",(float) row_access[i][j]/num_activates[i][j]);
291c291
<       printf("average row locality = %d/%d = %f\n", total_row_accesses, total_num_activates, (float)total_row_accesses / total_num_activates);
---
>       printf("average row locality = %d/%d = %f\n", total_row_accesses, total_num_activates, (float)total_row_accesses/total_num_activates);
301c301
<       for (i = 0; i < n_mem ; i++ ) {
---
>       for (i=0;i<n_mem ;i++ ) {
303c303
<          for (j = 0; j < gpu_mem_n_bk; j++ ) {
---
>          for (j=0;j<gpu_mem_n_bk;j++ ) {
311c311
<             printf("%9d ", l);
---
>             printf("%9d ",l);
322c322
<          printf("bank skew: %d/%d = %4.2f\n", max_bank_accesses, min_bank_accesses, (float)max_bank_accesses / min_bank_accesses);
---
>          printf("bank skew: %d/%d = %4.2f\n", max_bank_accesses, min_bank_accesses, (float)max_bank_accesses/min_bank_accesses);
326c326
<          printf("chip skew: %d/%d = %4.2f\n", max_chip_accesses, min_chip_accesses, (float)max_chip_accesses / min_chip_accesses);
---
>          printf("chip skew: %d/%d = %4.2f\n", max_chip_accesses, min_chip_accesses, (float)max_chip_accesses/min_chip_accesses);
339c339
<       for (i = 0; i < n_mem ; i++ ) {
---
>       for (i=0;i<n_mem ;i++ ) {
341c341
<          for (j = 0; j < gpu_mem_n_bk; j++ ) {
---
>          for (j=0;j<gpu_mem_n_bk;j++ ) {
349c349
<             printf("%9d ", l);
---
>             printf("%9d ",l);
360c360
<          printf("bank skew: %d/%d = %4.2f\n", max_bank_accesses, min_bank_accesses, (float)max_bank_accesses / min_bank_accesses);
---
>          printf("bank skew: %d/%d = %4.2f\n", max_bank_accesses, min_bank_accesses, (float)max_bank_accesses/min_bank_accesses);
364c364
<          printf("chip skew: %d/%d = %4.2f\n", max_chip_accesses, min_chip_accesses, (float)max_chip_accesses / min_chip_accesses);
---
>          printf("chip skew: %d/%d = %4.2f\n", max_chip_accesses, min_chip_accesses, (float)max_chip_accesses/min_chip_accesses);
377c377
<       for (i = 0; i < n_mem ; i++ ) {
---
>       for (i=0;i<n_mem ;i++ ) {
379c379
<          for (j = 0; j < gpu_mem_n_bk; j++ ) {
---
>          for (j=0;j<gpu_mem_n_bk;j++ ) {
387c387
<             printf("%9d ", l);
---
>             printf("%9d ",l);
398c398
<          printf("bank skew: %d/%d = %4.2f\n", max_bank_accesses, min_bank_accesses, (float)max_bank_accesses / min_bank_accesses);
---
>          printf("bank skew: %d/%d = %4.2f\n", max_bank_accesses, min_bank_accesses, (float)max_bank_accesses/min_bank_accesses);
402c402
<          printf("chip skew: %d/%d = %4.2f\n", max_chip_accesses, min_chip_accesses, (float)max_chip_accesses / min_chip_accesses);
---
>          printf("chip skew: %d/%d = %4.2f\n", max_chip_accesses, min_chip_accesses, (float)max_chip_accesses/min_chip_accesses);
409c409
<       for (i = 0; i < n_mem ; i++ ) {
---
>       for (i=0;i<n_mem ;i++ ) {
411c411
<          for (j = 0; j < gpu_mem_n_bk; j++ ) {
---
>          for (j=0;j<gpu_mem_n_bk;j++ ) {
423c423
<       for (i = 0; i < n_mem ; i++ ) {
---
>       for (i=0;i<n_mem ;i++ ) {
425c425
<          for (j = 0; j < gpu_mem_n_bk; j++ ) {
---
>          for (j=0;j<gpu_mem_n_bk;j++ ) {
436,437c436,437
<       for (i = 0; i < n_mem * gpu_mem_n_bk ; i++ ) {
<          accum_MCBs_accessed += i * num_MCBs_accessed[i];
---
>       for (i=0;i< n_mem*gpu_mem_n_bk ; i++ ) {
>          accum_MCBs_accessed += i*num_MCBs_accessed[i];
442c442
<       printf("\nAverage # of Memory Banks Accessed per Memory Operation per Warp=%f\n", (float)accum_MCBs_accessed / tot_mem_ops_per_warp);
---
>       printf("\nAverage # of Memory Banks Accessed per Memory Operation per Warp=%f\n", (float)accum_MCBs_accessed/tot_mem_ops_per_warp);
453,454c453,454
<       k = 0; l = 0;
<       for (i = 0; i < j; i++ ) {
---
>       k=0;l=0;
>       for (i=0;i< j; i++ ) {
457c457
<          l += i * position_of_mrq_chosen[i];
---
>          l += i*position_of_mrq_chosen[i];
460c460
<       printf("\naverage position of mrq chosen = %f\n", (float)l / k);
---
>       printf("\naverage position of mrq chosen = %f\n", (float)l/k);
diff -r gpgpu-sim/power_stat.cc ../../../gpgpu-sim_distribution/src/gpgpu-sim/power_stat.cc
37c37
< //#include "dram.h"
---
> #include "dram.h"
45,50c45,50
< power_mem_stat_t::power_mem_stat_t(const struct memory_config *mem_config, const struct shader_core_config *shdr_config, memory_stats_t *mem_stats, shader_core_stats *shdr_stats) {
<     assert( mem_config->m_valid );
<     m_mem_stats = mem_stats;
<     m_config = mem_config;
<     m_core_stats = shdr_stats;
<     m_core_config = shdr_config;
---
> power_mem_stat_t::power_mem_stat_t(const struct memory_config *mem_config, const struct shader_core_config *shdr_config, memory_stats_t *mem_stats, shader_core_stats *shdr_stats){
> 	   assert( mem_config->m_valid );
> 	   m_mem_stats = mem_stats;
> 	   m_config = mem_config;
> 	   m_core_stats = shdr_stats;
> 	   m_core_config = shdr_config;
52c52
<     init();
---
> 	   init();
55c55
< void power_mem_stat_t::init() {
---
> void power_mem_stat_t::init(){
57,58c57,58
<     shmem_read_access[CURRENT_STAT_IDX] = m_core_stats->gpgpu_n_shmem_bank_access;  // Shared memory access
<     shmem_read_access[PREV_STAT_IDX] = (unsigned *)calloc(m_core_config->num_shader(), sizeof(unsigned));
---
>     shmem_read_access[CURRENT_STAT_IDX] = m_core_stats->gpgpu_n_shmem_bank_access; 	// Shared memory access
>     shmem_read_access[PREV_STAT_IDX] = (unsigned *)calloc(m_core_config->num_shader(),sizeof(unsigned));
60c60
<     for (unsigned i = 0; i < NUM_STAT_IDX; ++i) {
---
>     for(unsigned i=0; i<NUM_STAT_IDX; ++i){
64,71c64,71
<         n_cmd[i] = (unsigned *)calloc(m_config->m_n_mem, sizeof(unsigned));
<         n_activity[i] = (unsigned *)calloc(m_config->m_n_mem, sizeof(unsigned));
<         n_nop[i] = (unsigned *)calloc(m_config->m_n_mem, sizeof(unsigned));
<         n_act[i] = (unsigned *)calloc(m_config->m_n_mem, sizeof(unsigned));
<         n_pre[i] = (unsigned *)calloc(m_config->m_n_mem, sizeof(unsigned));
<         n_rd[i] = (unsigned *)calloc(m_config->m_n_mem, sizeof(unsigned));
<         n_wr[i] = (unsigned *)calloc(m_config->m_n_mem, sizeof(unsigned));
<         n_req[i] = (unsigned *)calloc(m_config->m_n_mem, sizeof(unsigned));
---
>         n_cmd[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
>         n_activity[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
>         n_nop[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
>         n_act[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
>         n_pre[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
>         n_rd[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
>         n_wr[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
>         n_req[i] = (unsigned *)calloc(m_config->m_n_mem,sizeof(unsigned));
74,75c74,75
<         n_mem_to_simt[i] = (long *)calloc(m_core_config->n_simt_clusters, sizeof(long)); // Counted at SM
<         n_simt_to_mem[i] = (long *)calloc(m_core_config->n_simt_clusters, sizeof(long)); // Counted at SM
---
>         n_mem_to_simt[i] = (long *)calloc(m_core_config->n_simt_clusters,sizeof(long)); // Counted at SM
>         n_simt_to_mem[i] = (long *)calloc(m_core_config->n_simt_clusters,sizeof(long)); // Counted at SM
79c79
< void power_mem_stat_t::save_stats() {
---
> void power_mem_stat_t::save_stats(){
84,85c84,85
<     for (unsigned i = 0; i < m_core_config->num_shader(); ++i) {
<         shmem_read_access[PREV_STAT_IDX][i] = shmem_read_access[CURRENT_STAT_IDX][i] ;  // Shared memory access
---
>     for(unsigned i=0; i<m_core_config->num_shader(); ++i){
>         shmem_read_access[PREV_STAT_IDX][i] = shmem_read_access[CURRENT_STAT_IDX][i] ; 	// Shared memory access
88c88
<     for (unsigned i = 0; i < m_config->m_n_mem; ++i) {
---
>     for(unsigned i=0; i<m_config->m_n_mem; ++i){
99c99
<     for (unsigned i = 0; i < m_core_config->n_simt_clusters; i++) {
---
>     for(unsigned i=0; i<m_core_config->n_simt_clusters;i++){
105c105
< void power_mem_stat_t::visualizer_print( gzFile power_visualizer_file ) {
---
> void power_mem_stat_t::visualizer_print( gzFile power_visualizer_file ){
110,113c110,113
<     fprintf(fout, "\n\n==========Power Metrics -- Memory==========\n");
<     unsigned total_mem_reads = 0;
<     unsigned total_mem_writes = 0;
<     for (unsigned i = 0; i < m_config->m_n_mem; ++i) {
---
> 	fprintf(fout, "\n\n==========Power Metrics -- Memory==========\n");
>     unsigned total_mem_reads=0;
>     unsigned total_mem_writes=0;
>     for(unsigned i=0; i<m_config->m_n_mem; ++i){
117c117
<     fprintf(fout, "Total memory controller accesses: %u\n", total_mem_reads + total_mem_writes);
---
>     fprintf(fout, "Total memory controller accesses: %u\n", total_mem_reads+total_mem_writes);
130,134c130,134
<     assert( shader_config->m_valid );
<     m_config = shader_config;
<     shader_core_power_stats_pod *pod = this;
<     memset(pod, 0, sizeof(shader_core_power_stats_pod));
<     m_core_stats = core_stats;
---
>      	assert( shader_config->m_valid );
>         m_config = shader_config;
>         shader_core_power_stats_pod *pod = this;
>         memset(pod,0,sizeof(shader_core_power_stats_pod));
>         m_core_stats=core_stats;
136c136
<     init();
---
>         init();
147,174c147,174
<     // per core statistics
<     fprintf(fout, "Power Metrics: \n");
<     for (unsigned i = 0; i < m_config->num_shader(); i++) {
<         fprintf(fout, "core %u:\n", i);
<         fprintf(fout, "\tpipeline duty cycle =%f\n", m_pipeline_duty_cycle[CURRENT_STAT_IDX][i]);
<         fprintf(fout, "\tTotal Deocded Instructions=%u\n", m_num_decoded_insn[CURRENT_STAT_IDX][i]);
<         fprintf(fout, "\tTotal FP Deocded Instructions=%u\n", m_num_FPdecoded_insn[CURRENT_STAT_IDX][i]);
<         fprintf(fout, "\tTotal INT Deocded Instructions=%u\n", m_num_INTdecoded_insn[CURRENT_STAT_IDX][i]);
<         fprintf(fout, "\tTotal LOAD Queued Instructions=%u\n", m_num_loadqueued_insn[CURRENT_STAT_IDX][i]);
<         fprintf(fout, "\tTotal STORE Queued Instructions=%u\n", m_num_storequeued_insn[CURRENT_STAT_IDX][i]);
<         fprintf(fout, "\tTotal IALU Acesses=%u\n", m_num_ialu_acesses[CURRENT_STAT_IDX][i]);
<         fprintf(fout, "\tTotal FP Acesses=%u\n", m_num_fp_acesses[CURRENT_STAT_IDX][i]);
<         fprintf(fout, "\tTotal IMUL Acesses=%u\n", m_num_imul_acesses[CURRENT_STAT_IDX][i]);
<         fprintf(fout, "\tTotal IMUL24 Acesses=%u\n", m_num_imul24_acesses[CURRENT_STAT_IDX][i]);
<         fprintf(fout, "\tTotal IMUL32 Acesses=%u\n", m_num_imul32_acesses[CURRENT_STAT_IDX][i]);
<         fprintf(fout, "\tTotal IDIV Acesses=%u\n", m_num_idiv_acesses[CURRENT_STAT_IDX][i]);
<         fprintf(fout, "\tTotal FPMUL Acesses=%u\n", m_num_fpmul_acesses[CURRENT_STAT_IDX][i]);
<         fprintf(fout, "\tTotal SFU Acesses=%u\n", m_num_trans_acesses[CURRENT_STAT_IDX][i]);
<         fprintf(fout, "\tTotal FPDIV Acesses=%u\n", m_num_fpdiv_acesses[CURRENT_STAT_IDX][i]);
<         fprintf(fout, "\tTotal SFU Acesses=%u\n", m_num_sfu_acesses[CURRENT_STAT_IDX][i]);
<         fprintf(fout, "\tTotal SP Acesses=%u\n", m_num_sp_acesses[CURRENT_STAT_IDX][i]);
<         fprintf(fout, "\tTotal MEM Acesses=%u\n", m_num_mem_acesses[CURRENT_STAT_IDX][i]);
<         fprintf(fout, "\tTotal SFU Commissions=%u\n", m_num_sfu_committed[CURRENT_STAT_IDX][i]);
<         fprintf(fout, "\tTotal SP Commissions=%u\n", m_num_sp_committed[CURRENT_STAT_IDX][i]);
<         fprintf(fout, "\tTotal MEM Commissions=%u\n", m_num_mem_committed[CURRENT_STAT_IDX][i]);
<         fprintf(fout, "\tTotal REG Reads=%u\n", m_read_regfile_acesses[CURRENT_STAT_IDX][i]);
<         fprintf(fout, "\tTotal REG Writes=%u\n", m_write_regfile_acesses[CURRENT_STAT_IDX][i]);
<         fprintf(fout, "\tTotal NON REG=%u\n", m_non_rf_operands[CURRENT_STAT_IDX][i]);
---
> 	// per core statistics
>     fprintf(fout,"Power Metrics: \n");
>     for(unsigned i=0; i<m_config->num_shader();i++){
>         fprintf(fout,"core %u:\n",i);
>         fprintf(fout,"\tpipeline duty cycle =%f\n",m_pipeline_duty_cycle[CURRENT_STAT_IDX][i]);
>         fprintf(fout,"\tTotal Deocded Instructions=%u\n",m_num_decoded_insn[CURRENT_STAT_IDX][i]);
>         fprintf(fout,"\tTotal FP Deocded Instructions=%u\n",m_num_FPdecoded_insn[CURRENT_STAT_IDX][i]);
>         fprintf(fout,"\tTotal INT Deocded Instructions=%u\n",m_num_INTdecoded_insn[CURRENT_STAT_IDX][i]);
>         fprintf(fout,"\tTotal LOAD Queued Instructions=%u\n",m_num_loadqueued_insn[CURRENT_STAT_IDX][i]);
>         fprintf(fout,"\tTotal STORE Queued Instructions=%u\n",m_num_storequeued_insn[CURRENT_STAT_IDX][i]);
>         fprintf(fout,"\tTotal IALU Acesses=%u\n",m_num_ialu_acesses[CURRENT_STAT_IDX][i]);
>         fprintf(fout,"\tTotal FP Acesses=%u\n",m_num_fp_acesses[CURRENT_STAT_IDX][i]);
>         fprintf(fout,"\tTotal IMUL Acesses=%u\n",m_num_imul_acesses[CURRENT_STAT_IDX][i]);
>         fprintf(fout,"\tTotal IMUL24 Acesses=%u\n",m_num_imul24_acesses[CURRENT_STAT_IDX][i]);
>         fprintf(fout,"\tTotal IMUL32 Acesses=%u\n",m_num_imul32_acesses[CURRENT_STAT_IDX][i]);
>         fprintf(fout,"\tTotal IDIV Acesses=%u\n",m_num_idiv_acesses[CURRENT_STAT_IDX][i]);
>         fprintf(fout,"\tTotal FPMUL Acesses=%u\n",m_num_fpmul_acesses[CURRENT_STAT_IDX][i]);
>         fprintf(fout,"\tTotal SFU Acesses=%u\n",m_num_trans_acesses[CURRENT_STAT_IDX][i]);
>         fprintf(fout,"\tTotal FPDIV Acesses=%u\n",m_num_fpdiv_acesses[CURRENT_STAT_IDX][i]);
>         fprintf(fout,"\tTotal SFU Acesses=%u\n",m_num_sfu_acesses[CURRENT_STAT_IDX][i]);
>         fprintf(fout,"\tTotal SP Acesses=%u\n",m_num_sp_acesses[CURRENT_STAT_IDX][i]);
>         fprintf(fout,"\tTotal MEM Acesses=%u\n",m_num_mem_acesses[CURRENT_STAT_IDX][i]);
>         fprintf(fout,"\tTotal SFU Commissions=%u\n",m_num_sfu_committed[CURRENT_STAT_IDX][i]);
>         fprintf(fout,"\tTotal SP Commissions=%u\n",m_num_sp_committed[CURRENT_STAT_IDX][i]);
>         fprintf(fout,"\tTotal MEM Commissions=%u\n",m_num_mem_committed[CURRENT_STAT_IDX][i]);
>         fprintf(fout,"\tTotal REG Reads=%u\n",m_read_regfile_acesses[CURRENT_STAT_IDX][i]);
>         fprintf(fout,"\tTotal REG Writes=%u\n",m_write_regfile_acesses[CURRENT_STAT_IDX][i]);
>         fprintf(fout,"\tTotal NON REG=%u\n",m_non_rf_operands[CURRENT_STAT_IDX][i]);
179,278c179,278
<     m_pipeline_duty_cycle[CURRENT_STAT_IDX] = m_core_stats->m_pipeline_duty_cycle;
<     m_num_decoded_insn[CURRENT_STAT_IDX] = m_core_stats->m_num_decoded_insn;
<     m_num_FPdecoded_insn[CURRENT_STAT_IDX] = m_core_stats->m_num_FPdecoded_insn;
<     m_num_INTdecoded_insn[CURRENT_STAT_IDX] = m_core_stats->m_num_INTdecoded_insn;
<     m_num_storequeued_insn[CURRENT_STAT_IDX] = m_core_stats->m_num_storequeued_insn;
<     m_num_loadqueued_insn[CURRENT_STAT_IDX] = m_core_stats->m_num_loadqueued_insn;
<     m_num_ialu_acesses[CURRENT_STAT_IDX] = m_core_stats->m_num_ialu_acesses;
<     m_num_fp_acesses[CURRENT_STAT_IDX] = m_core_stats->m_num_fp_acesses;
<     m_num_imul_acesses[CURRENT_STAT_IDX] = m_core_stats->m_num_imul_acesses;
<     m_num_imul24_acesses[CURRENT_STAT_IDX] = m_core_stats->m_num_imul24_acesses;
<     m_num_imul32_acesses[CURRENT_STAT_IDX] = m_core_stats->m_num_imul32_acesses;
<     m_num_fpmul_acesses[CURRENT_STAT_IDX] = m_core_stats->m_num_fpmul_acesses;
<     m_num_idiv_acesses[CURRENT_STAT_IDX] = m_core_stats->m_num_idiv_acesses;
<     m_num_fpdiv_acesses[CURRENT_STAT_IDX] = m_core_stats->m_num_fpdiv_acesses;
<     m_num_sp_acesses[CURRENT_STAT_IDX] = m_core_stats->m_num_sp_acesses;
<     m_num_sfu_acesses[CURRENT_STAT_IDX] = m_core_stats->m_num_sfu_acesses;
<     m_num_trans_acesses[CURRENT_STAT_IDX] = m_core_stats->m_num_trans_acesses;
<     m_num_mem_acesses[CURRENT_STAT_IDX] = m_core_stats->m_num_mem_acesses;
<     m_num_sp_committed[CURRENT_STAT_IDX] = m_core_stats->m_num_sp_committed;
<     m_num_sfu_committed[CURRENT_STAT_IDX] = m_core_stats->m_num_sfu_committed;
<     m_num_mem_committed[CURRENT_STAT_IDX] = m_core_stats->m_num_mem_committed;
<     m_read_regfile_acesses[CURRENT_STAT_IDX] = m_core_stats->m_read_regfile_acesses;
<     m_write_regfile_acesses[CURRENT_STAT_IDX] = m_core_stats->m_write_regfile_acesses;
<     m_non_rf_operands[CURRENT_STAT_IDX] = m_core_stats->m_non_rf_operands;
<     m_active_sp_lanes[CURRENT_STAT_IDX] = m_core_stats->m_active_sp_lanes;
<     m_active_sfu_lanes[CURRENT_STAT_IDX] = m_core_stats->m_active_sfu_lanes;
<     m_num_tex_inst[CURRENT_STAT_IDX] = m_core_stats->m_num_tex_inst;
< 
< 
<     m_pipeline_duty_cycle[PREV_STAT_IDX] = (float*)calloc(m_config->num_shader(), sizeof(float));
<     m_num_decoded_insn[PREV_STAT_IDX] = (unsigned *)calloc(m_config->num_shader(), sizeof(unsigned));
<     m_num_FPdecoded_insn[PREV_STAT_IDX] = (unsigned *)calloc(m_config->num_shader(), sizeof(unsigned));
<     m_num_INTdecoded_insn[PREV_STAT_IDX] = (unsigned *)calloc(m_config->num_shader(), sizeof(unsigned));
<     m_num_storequeued_insn[PREV_STAT_IDX] = (unsigned *)calloc(m_config->num_shader(), sizeof(unsigned));
<     m_num_loadqueued_insn[PREV_STAT_IDX] = (unsigned *)calloc(m_config->num_shader(), sizeof(unsigned));
<     m_num_ialu_acesses[PREV_STAT_IDX] = (unsigned *)calloc(m_config->num_shader(), sizeof(unsigned));
<     m_num_fp_acesses[PREV_STAT_IDX] = (unsigned *)calloc(m_config->num_shader(), sizeof(unsigned));
<     m_num_tex_inst[PREV_STAT_IDX] = (unsigned *)calloc(m_config->num_shader(), sizeof(unsigned));
<     m_num_imul_acesses[PREV_STAT_IDX] = (unsigned *)calloc(m_config->num_shader(), sizeof(unsigned));
<     m_num_imul24_acesses[PREV_STAT_IDX] = (unsigned *)calloc(m_config->num_shader(), sizeof(unsigned));
<     m_num_imul32_acesses[PREV_STAT_IDX] = (unsigned *)calloc(m_config->num_shader(), sizeof(unsigned));
<     m_num_fpmul_acesses[PREV_STAT_IDX] = (unsigned *)calloc(m_config->num_shader(), sizeof(unsigned));
<     m_num_idiv_acesses[PREV_STAT_IDX] = (unsigned *)calloc(m_config->num_shader(), sizeof(unsigned));
<     m_num_fpdiv_acesses[PREV_STAT_IDX] = (unsigned *)calloc(m_config->num_shader(), sizeof(unsigned));
<     m_num_sp_acesses[PREV_STAT_IDX] = (unsigned *)calloc(m_config->num_shader(), sizeof(unsigned));
<     m_num_sfu_acesses[PREV_STAT_IDX] = (unsigned *)calloc(m_config->num_shader(), sizeof(unsigned));
<     m_num_trans_acesses[PREV_STAT_IDX] = (unsigned *)calloc(m_config->num_shader(), sizeof(unsigned));
<     m_num_mem_acesses[PREV_STAT_IDX] = (unsigned *)calloc(m_config->num_shader(), sizeof(unsigned));
<     m_num_sp_committed[PREV_STAT_IDX] = (unsigned *)calloc(m_config->num_shader(), sizeof(unsigned));
<     m_num_sfu_committed[PREV_STAT_IDX] = (unsigned *)calloc(m_config->num_shader(), sizeof(unsigned));
<     m_num_mem_committed[PREV_STAT_IDX] = (unsigned *)calloc(m_config->num_shader(), sizeof(unsigned));
<     m_read_regfile_acesses[PREV_STAT_IDX] = (unsigned *)calloc(m_config->num_shader(), sizeof(unsigned));
<     m_write_regfile_acesses[PREV_STAT_IDX] = (unsigned *)calloc(m_config->num_shader(), sizeof(unsigned));
<     m_non_rf_operands[PREV_STAT_IDX] = (unsigned *)calloc(m_config->num_shader(), sizeof(unsigned));
<     m_active_sp_lanes[PREV_STAT_IDX] = (unsigned *)calloc(m_config->num_shader(), sizeof(unsigned));
<     m_active_sfu_lanes[PREV_STAT_IDX] = (unsigned *)calloc(m_config->num_shader(), sizeof(unsigned));
< }
< 
< void power_core_stat_t::save_stats() {
<     for (unsigned i = 0; i < m_config->num_shader(); ++i) {
<         m_pipeline_duty_cycle[PREV_STAT_IDX][i] = m_pipeline_duty_cycle[CURRENT_STAT_IDX][i];
<         m_num_decoded_insn[PREV_STAT_IDX][i] =   m_num_decoded_insn[CURRENT_STAT_IDX][i];
<         m_num_FPdecoded_insn[PREV_STAT_IDX][i] = m_num_FPdecoded_insn[CURRENT_STAT_IDX][i];
<         m_num_INTdecoded_insn[PREV_STAT_IDX][i] = m_num_INTdecoded_insn[CURRENT_STAT_IDX][i];
<         m_num_storequeued_insn[PREV_STAT_IDX][i] = m_num_storequeued_insn[CURRENT_STAT_IDX][i];
<         m_num_loadqueued_insn[PREV_STAT_IDX][i] = m_num_loadqueued_insn[CURRENT_STAT_IDX][i];
<         m_num_ialu_acesses[PREV_STAT_IDX][i] = m_num_ialu_acesses[CURRENT_STAT_IDX][i];
<         m_num_fp_acesses[PREV_STAT_IDX][i] = m_num_fp_acesses[CURRENT_STAT_IDX][i];
<         m_num_tex_inst[PREV_STAT_IDX][i] = m_num_tex_inst[CURRENT_STAT_IDX][i];
<         m_num_imul_acesses[PREV_STAT_IDX][i] = m_num_imul_acesses[CURRENT_STAT_IDX][i];
<         m_num_imul24_acesses[PREV_STAT_IDX][i] = m_num_imul24_acesses[CURRENT_STAT_IDX][i];
<         m_num_imul32_acesses[PREV_STAT_IDX][i] = m_num_imul32_acesses[CURRENT_STAT_IDX][i];
<         m_num_fpmul_acesses[PREV_STAT_IDX][i] = m_num_fpmul_acesses[CURRENT_STAT_IDX][i];
<         m_num_idiv_acesses[PREV_STAT_IDX][i] = m_num_idiv_acesses[CURRENT_STAT_IDX][i];
<         m_num_fpdiv_acesses[PREV_STAT_IDX][i] = m_num_fpdiv_acesses[CURRENT_STAT_IDX][i];
<         m_num_sp_acesses[PREV_STAT_IDX][i] = m_num_sp_acesses[CURRENT_STAT_IDX][i];
<         m_num_sfu_acesses[PREV_STAT_IDX][i] = m_num_sfu_acesses[CURRENT_STAT_IDX][i];
<         m_num_trans_acesses[PREV_STAT_IDX][i] = m_num_trans_acesses[CURRENT_STAT_IDX][i];
<         m_num_mem_acesses[PREV_STAT_IDX][i] = m_num_mem_acesses[CURRENT_STAT_IDX][i];
<         m_num_sp_committed[PREV_STAT_IDX][i] = m_num_sp_committed[CURRENT_STAT_IDX][i];
<         m_num_sfu_committed[PREV_STAT_IDX][i] = m_num_sfu_committed[CURRENT_STAT_IDX][i];
<         m_num_mem_committed[PREV_STAT_IDX][i] = m_num_mem_committed[CURRENT_STAT_IDX][i];
<         m_read_regfile_acesses[PREV_STAT_IDX][i] = m_read_regfile_acesses[CURRENT_STAT_IDX][i];
<         m_write_regfile_acesses[PREV_STAT_IDX][i] = m_write_regfile_acesses[CURRENT_STAT_IDX][i];
<         m_non_rf_operands[PREV_STAT_IDX][i] = m_non_rf_operands[CURRENT_STAT_IDX][i];
<         m_active_sp_lanes[PREV_STAT_IDX][i] = m_active_sp_lanes[CURRENT_STAT_IDX][i];
<         m_active_sfu_lanes[PREV_STAT_IDX][i] = m_active_sfu_lanes[CURRENT_STAT_IDX][i];
<     }
< }
< 
< power_stat_t::power_stat_t( const struct shader_core_config *shader_config, float * average_pipeline_duty_cycle, float *active_sms, shader_core_stats * shader_stats, const struct memory_config *mem_config, memory_stats_t * memory_stats)
< {
<     assert( shader_config->m_valid );
<     assert( mem_config->m_valid );
<     pwr_core_stat = new power_core_stat_t(shader_config, shader_stats);
<     pwr_mem_stat = new power_mem_stat_t(mem_config, shader_config, memory_stats, shader_stats);
<     m_average_pipeline_duty_cycle = average_pipeline_duty_cycle;
<     m_active_sms = active_sms;
<     m_config = shader_config;
<     m_mem_config = mem_config;
---
>     m_pipeline_duty_cycle[CURRENT_STAT_IDX]=m_core_stats->m_pipeline_duty_cycle;
>     m_num_decoded_insn[CURRENT_STAT_IDX]=m_core_stats->m_num_decoded_insn;
>     m_num_FPdecoded_insn[CURRENT_STAT_IDX]=m_core_stats->m_num_FPdecoded_insn;
>     m_num_INTdecoded_insn[CURRENT_STAT_IDX]=m_core_stats->m_num_INTdecoded_insn;
>     m_num_storequeued_insn[CURRENT_STAT_IDX]=m_core_stats->m_num_storequeued_insn;
>     m_num_loadqueued_insn[CURRENT_STAT_IDX]=m_core_stats->m_num_loadqueued_insn;
>     m_num_ialu_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_ialu_acesses;
>     m_num_fp_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_fp_acesses;
>     m_num_imul_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_imul_acesses;
>     m_num_imul24_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_imul24_acesses;
>     m_num_imul32_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_imul32_acesses;
>     m_num_fpmul_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_fpmul_acesses;
>     m_num_idiv_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_idiv_acesses;
>     m_num_fpdiv_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_fpdiv_acesses;
>     m_num_sp_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_sp_acesses;
>     m_num_sfu_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_sfu_acesses;
>     m_num_trans_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_trans_acesses;
>     m_num_mem_acesses[CURRENT_STAT_IDX]=m_core_stats->m_num_mem_acesses;
>     m_num_sp_committed[CURRENT_STAT_IDX]=m_core_stats->m_num_sp_committed;
>     m_num_sfu_committed[CURRENT_STAT_IDX]=m_core_stats->m_num_sfu_committed;
>     m_num_mem_committed[CURRENT_STAT_IDX]=m_core_stats->m_num_mem_committed;
>     m_read_regfile_acesses[CURRENT_STAT_IDX]=m_core_stats->m_read_regfile_acesses;
>     m_write_regfile_acesses[CURRENT_STAT_IDX]=m_core_stats->m_write_regfile_acesses;
>     m_non_rf_operands[CURRENT_STAT_IDX]=m_core_stats->m_non_rf_operands;
>     m_active_sp_lanes[CURRENT_STAT_IDX]=m_core_stats->m_active_sp_lanes;
>     m_active_sfu_lanes[CURRENT_STAT_IDX]=m_core_stats->m_active_sfu_lanes;
>     m_num_tex_inst[CURRENT_STAT_IDX]=m_core_stats->m_num_tex_inst;
> 
> 
>     m_pipeline_duty_cycle[PREV_STAT_IDX]=(float*)calloc(m_config->num_shader(),sizeof(float));
>     m_num_decoded_insn[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
>     m_num_FPdecoded_insn[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
>     m_num_INTdecoded_insn[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
>     m_num_storequeued_insn[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
>     m_num_loadqueued_insn[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
>     m_num_ialu_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
>     m_num_fp_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
>     m_num_tex_inst[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
>     m_num_imul_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
>     m_num_imul24_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
>     m_num_imul32_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
>     m_num_fpmul_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
>     m_num_idiv_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
>     m_num_fpdiv_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
>     m_num_sp_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
>     m_num_sfu_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
>     m_num_trans_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
>     m_num_mem_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
>     m_num_sp_committed[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
>     m_num_sfu_committed[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
>     m_num_mem_committed[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
>     m_read_regfile_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
>     m_write_regfile_acesses[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
>     m_non_rf_operands[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
>     m_active_sp_lanes[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
>     m_active_sfu_lanes[PREV_STAT_IDX]=(unsigned *)calloc(m_config->num_shader(),sizeof(unsigned));
> }
> 
> void power_core_stat_t::save_stats(){
> for(unsigned i=0; i<m_config->num_shader(); ++i){
>     m_pipeline_duty_cycle[PREV_STAT_IDX][i]=m_pipeline_duty_cycle[CURRENT_STAT_IDX][i];
>     m_num_decoded_insn[PREV_STAT_IDX][i]=	m_num_decoded_insn[CURRENT_STAT_IDX][i];
>     m_num_FPdecoded_insn[PREV_STAT_IDX][i]=m_num_FPdecoded_insn[CURRENT_STAT_IDX][i];
>     m_num_INTdecoded_insn[PREV_STAT_IDX][i]=m_num_INTdecoded_insn[CURRENT_STAT_IDX][i];
>     m_num_storequeued_insn[PREV_STAT_IDX][i]=m_num_storequeued_insn[CURRENT_STAT_IDX][i];
>     m_num_loadqueued_insn[PREV_STAT_IDX][i]=m_num_loadqueued_insn[CURRENT_STAT_IDX][i];
>     m_num_ialu_acesses[PREV_STAT_IDX][i]=m_num_ialu_acesses[CURRENT_STAT_IDX][i];
>     m_num_fp_acesses[PREV_STAT_IDX][i]=m_num_fp_acesses[CURRENT_STAT_IDX][i];
>     m_num_tex_inst[PREV_STAT_IDX][i]=m_num_tex_inst[CURRENT_STAT_IDX][i];
>     m_num_imul_acesses[PREV_STAT_IDX][i]=m_num_imul_acesses[CURRENT_STAT_IDX][i];
>     m_num_imul24_acesses[PREV_STAT_IDX][i]=m_num_imul24_acesses[CURRENT_STAT_IDX][i];
>     m_num_imul32_acesses[PREV_STAT_IDX][i]=m_num_imul32_acesses[CURRENT_STAT_IDX][i];
>     m_num_fpmul_acesses[PREV_STAT_IDX][i]=m_num_fpmul_acesses[CURRENT_STAT_IDX][i];
>     m_num_idiv_acesses[PREV_STAT_IDX][i]=m_num_idiv_acesses[CURRENT_STAT_IDX][i];
>     m_num_fpdiv_acesses[PREV_STAT_IDX][i]=m_num_fpdiv_acesses[CURRENT_STAT_IDX][i];
>     m_num_sp_acesses[PREV_STAT_IDX][i]=m_num_sp_acesses[CURRENT_STAT_IDX][i];
>     m_num_sfu_acesses[PREV_STAT_IDX][i]=m_num_sfu_acesses[CURRENT_STAT_IDX][i];
>     m_num_trans_acesses[PREV_STAT_IDX][i]=m_num_trans_acesses[CURRENT_STAT_IDX][i];
>     m_num_mem_acesses[PREV_STAT_IDX][i]=m_num_mem_acesses[CURRENT_STAT_IDX][i];
>     m_num_sp_committed[PREV_STAT_IDX][i]=m_num_sp_committed[CURRENT_STAT_IDX][i];
>     m_num_sfu_committed[PREV_STAT_IDX][i]=m_num_sfu_committed[CURRENT_STAT_IDX][i];
>     m_num_mem_committed[PREV_STAT_IDX][i]=m_num_mem_committed[CURRENT_STAT_IDX][i];
>     m_read_regfile_acesses[PREV_STAT_IDX][i]=m_read_regfile_acesses[CURRENT_STAT_IDX][i];
>     m_write_regfile_acesses[PREV_STAT_IDX][i]=m_write_regfile_acesses[CURRENT_STAT_IDX][i];
>     m_non_rf_operands[PREV_STAT_IDX][i]=m_non_rf_operands[CURRENT_STAT_IDX][i];
>     m_active_sp_lanes[PREV_STAT_IDX][i]=m_active_sp_lanes[CURRENT_STAT_IDX][i];
>     m_active_sfu_lanes[PREV_STAT_IDX][i]=m_active_sfu_lanes[CURRENT_STAT_IDX][i];
>     }
> }
> 
> power_stat_t::power_stat_t( const struct shader_core_config *shader_config,float * average_pipeline_duty_cycle,float *active_sms,shader_core_stats * shader_stats, const struct memory_config *mem_config,memory_stats_t * memory_stats)
> {
> 	assert( shader_config->m_valid );
> 	assert( mem_config->m_valid );
> 	pwr_core_stat= new power_core_stat_t(shader_config,shader_stats);
> 	pwr_mem_stat= new power_mem_stat_t(mem_config,shader_config, memory_stats, shader_stats);
> 	m_average_pipeline_duty_cycle=average_pipeline_duty_cycle;
> 	m_active_sms=active_sms;
> 	m_config = shader_config;
> 	m_mem_config = mem_config;
283,284c283,284
<     pwr_core_stat->visualizer_print(visualizer_file);
<     pwr_mem_stat->visualizer_print(visualizer_file);
---
> 	pwr_core_stat->visualizer_print(visualizer_file);
> 	pwr_mem_stat->visualizer_print(visualizer_file);
289,291c289,291
<     fprintf(fout, "average_pipeline_duty_cycle=%f\n", *m_average_pipeline_duty_cycle);
<     pwr_core_stat->print(fout);
<     pwr_mem_stat->print(fout);
---
> 	fprintf(fout,"average_pipeline_duty_cycle=%f\n",*m_average_pipeline_duty_cycle);
> 	pwr_core_stat->print(fout);
> 	pwr_mem_stat->print(fout);
diff -r gpgpu-sim/shader.cc ../../../gpgpu-sim_distribution/src/gpgpu-sim/shader.cc
2c2
< // George L. Yuan, Andrew Turner, Inderpreet Singh
---
> // George L. Yuan, Andrew Turner, Inderpreet Singh 
33c33
< //#include "dram.h"
---
> #include "dram.h"
53c53
< 
---
>     
59,65c59,65
<   std::list<unsigned> result;
<   for ( unsigned op = 0; op < MAX_REG_OPERANDS; op++ ) {
<     int reg_num = fvt.arch_reg.dst[op]; // this math needs to match that used in function_info::ptx_decode_inst
<     if ( reg_num >= 0 ) // valid register
<       result.push_back(reg_num);
<   }
<   return result;
---
>    std::list<unsigned> result;
>    for( unsigned op=0; op < MAX_REG_OPERANDS; op++ ) {
>       int reg_num = fvt.arch_reg.dst[op]; // this math needs to match that used in function_info::ptx_decode_inst
>       if( reg_num >= 0 ) // valid register
>          result.push_back(reg_num);
>    }
>    return result;
68c68
< shader_core_ctx::shader_core_ctx( class gpgpu_sim *gpu,
---
> shader_core_ctx::shader_core_ctx( class gpgpu_sim *gpu, 
75,303c75,94
<   : core_t( gpu, NULL, config->warp_size, config->n_thread_per_shader ),
<     m_barriers( this, config->max_warps_per_shader, config->max_cta_per_core, config->max_barriers_per_cta, config->warp_size ),
<     m_dynamic_warp_id(0)
< {
<   m_cluster = cluster;
<   m_config = config;
<   m_memory_config = mem_config;
<   m_stats = stats;
<   unsigned warp_size = config->warp_size;
< 
<   m_sid = shader_id;
<   m_tpc = tpc_id;
< 
<   m_pipeline_reg.reserve(N_PIPELINE_STAGES);
<   for (int j = 0; j < N_PIPELINE_STAGES; j++) {
<     m_pipeline_reg.push_back(register_set(m_config->pipe_widths[j], pipeline_stage_name_decode[j]));
<   }
< 
<   m_threadState = (thread_ctx_t*) calloc(sizeof(thread_ctx_t), config->n_thread_per_shader);
< 
<   m_not_completed = 0;
<   m_active_threads.reset();
<   m_n_active_cta = 0;
<   for ( unsigned i = 0; i < MAX_CTA_PER_SHADER; i++ )
<     m_cta_status[i] = 0;
<   for (unsigned i = 0; i < config->n_thread_per_shader; i++) {
<     m_thread[i] = NULL;
<     m_threadState[i].m_cta_id = -1;
<     m_threadState[i].m_active = false;
<   }
< 
<   // m_icnt = new shader_memory_interface(this,cluster);
<   if ( m_config->gpgpu_perfect_mem ) {
<     m_icnt = new perfect_memory_interface(this, cluster);
<   } else {
<     m_icnt = new shader_memory_interface(this, cluster);
<   }
<   m_mem_fetch_allocator = new shader_core_mem_fetch_allocator(shader_id, tpc_id, mem_config);
< 
<   // fetch
<   m_last_warp_fetched = 0;
< 
< #define STRSIZE 1024
<   char name[STRSIZE];
<   snprintf(name, STRSIZE, "L1I_%03d", m_sid);
<   m_L1I = new read_only_cache( name, m_config->m_L1I_config, m_sid, get_shader_instruction_cache_id(), m_icnt, IN_L1I_MISS_QUEUE);
< 
<   m_warp.resize(m_config->max_warps_per_shader, shd_warp_t(this, warp_size));
<   m_scoreboard = new Scoreboard(m_sid, m_config->max_warps_per_shader);
< 
<   //scedulers
<   //must currently occur after all inputs have been initialized.
<   std::string sched_config = m_config->gpgpu_scheduler_string;
<   const concrete_scheduler scheduler = sched_config.find("lrr") != std::string::npos ?
<                                        CONCRETE_SCHEDULER_LRR :
<                                        sched_config.find("two_level_active") != std::string::npos ?
<                                        CONCRETE_SCHEDULER_TWO_LEVEL_ACTIVE :
<                                        sched_config.find("gto") != std::string::npos ?
<                                        CONCRETE_SCHEDULER_GTO :
<                                        sched_config.find("warp_limiting") != std::string::npos ?
<                                        CONCRETE_SCHEDULER_WARP_LIMITING :
<                                        NUM_CONCRETE_SCHEDULERS;
<   assert ( scheduler != NUM_CONCRETE_SCHEDULERS );
< 
<   for (int i = 0; i < m_config->gpgpu_num_sched_per_core; i++) {
<     switch ( scheduler )
<     {
<     case CONCRETE_SCHEDULER_LRR:
<       schedulers.push_back(
<         new lrr_scheduler( m_stats,
<                            this,
<                            m_scoreboard,
<                            m_simt_stack,
<                            &m_warp,
<                            &m_pipeline_reg[ID_OC_SP],
<                            &m_pipeline_reg[ID_OC_SFU],
<                            &m_pipeline_reg[ID_OC_MEM],
<                            i
<                          )
<       );
<       break;
<     case CONCRETE_SCHEDULER_TWO_LEVEL_ACTIVE:
<       schedulers.push_back(
<         new two_level_active_scheduler( m_stats,
<                                         this,
<                                         m_scoreboard,
<                                         m_simt_stack,
<                                         &m_warp,
<                                         &m_pipeline_reg[ID_OC_SP],
<                                         &m_pipeline_reg[ID_OC_SFU],
<                                         &m_pipeline_reg[ID_OC_MEM],
<                                         i,
<                                         config->gpgpu_scheduler_string
<                                       )
<       );
<       break;
<     case CONCRETE_SCHEDULER_GTO:
<       schedulers.push_back(
<         new gto_scheduler( m_stats,
<                            this,
<                            m_scoreboard,
<                            m_simt_stack,
<                            &m_warp,
<                            &m_pipeline_reg[ID_OC_SP],
<                            &m_pipeline_reg[ID_OC_SFU],
<                            &m_pipeline_reg[ID_OC_MEM],
<                            i
<                          )
<       );
<       break;
<     case CONCRETE_SCHEDULER_WARP_LIMITING:
<       schedulers.push_back(
<         new swl_scheduler( m_stats,
<                            this,
<                            m_scoreboard,
<                            m_simt_stack,
<                            &m_warp,
<                            &m_pipeline_reg[ID_OC_SP],
<                            &m_pipeline_reg[ID_OC_SFU],
<                            &m_pipeline_reg[ID_OC_MEM],
<                            i,
<                            config->gpgpu_scheduler_string
<                          )
<       );
<       break;
<     default:
<       abort();
<     };
<   }
< 
<   for (unsigned i = 0; i < m_warp.size(); i++) {
<     //distribute i's evenly though schedulers;
<     schedulers[i % m_config->gpgpu_num_sched_per_core]->add_supervised_warp_id(i);
<   }
<   for ( int i = 0; i < m_config->gpgpu_num_sched_per_core; ++i ) {
<     schedulers[i]->done_adding_supervised_warps();
<   }
< 
<   //op collector configuration
<   enum { SP_CUS, SFU_CUS, MEM_CUS, GEN_CUS };
<   m_operand_collector.add_cu_set(SP_CUS, m_config->gpgpu_operand_collector_num_units_sp, m_config->gpgpu_operand_collector_num_out_ports_sp);
<   m_operand_collector.add_cu_set(SFU_CUS, m_config->gpgpu_operand_collector_num_units_sfu, m_config->gpgpu_operand_collector_num_out_ports_sfu);
<   m_operand_collector.add_cu_set(MEM_CUS, m_config->gpgpu_operand_collector_num_units_mem, m_config->gpgpu_operand_collector_num_out_ports_mem);
<   m_operand_collector.add_cu_set(GEN_CUS, m_config->gpgpu_operand_collector_num_units_gen, m_config->gpgpu_operand_collector_num_out_ports_gen);
< 
<   opndcoll_rfu_t::port_vector_t in_ports;
<   opndcoll_rfu_t::port_vector_t out_ports;
<   opndcoll_rfu_t::uint_vector_t cu_sets;
<   for (unsigned i = 0; i < m_config->gpgpu_operand_collector_num_in_ports_sp; i++) {
<     in_ports.push_back(&m_pipeline_reg[ID_OC_SP]);
<     out_ports.push_back(&m_pipeline_reg[OC_EX_SP]);
<     cu_sets.push_back((unsigned)SP_CUS);
<     cu_sets.push_back((unsigned)GEN_CUS);
<     m_operand_collector.add_port(in_ports, out_ports, cu_sets);
<     in_ports.clear(), out_ports.clear(), cu_sets.clear();
<   }
< 
<   for (unsigned i = 0; i < m_config->gpgpu_operand_collector_num_in_ports_sfu; i++) {
<     in_ports.push_back(&m_pipeline_reg[ID_OC_SFU]);
<     out_ports.push_back(&m_pipeline_reg[OC_EX_SFU]);
<     cu_sets.push_back((unsigned)SFU_CUS);
<     cu_sets.push_back((unsigned)GEN_CUS);
<     m_operand_collector.add_port(in_ports, out_ports, cu_sets);
<     in_ports.clear(), out_ports.clear(), cu_sets.clear();
<   }
< 
<   for (unsigned i = 0; i < m_config->gpgpu_operand_collector_num_in_ports_mem; i++) {
<     in_ports.push_back(&m_pipeline_reg[ID_OC_MEM]);
<     out_ports.push_back(&m_pipeline_reg[OC_EX_MEM]);
<     cu_sets.push_back((unsigned)MEM_CUS);
<     cu_sets.push_back((unsigned)GEN_CUS);
<     m_operand_collector.add_port(in_ports, out_ports, cu_sets);
<     in_ports.clear(), out_ports.clear(), cu_sets.clear();
<   }
< 
< 
<   for (unsigned i = 0; i < m_config->gpgpu_operand_collector_num_in_ports_gen; i++) {
<     in_ports.push_back(&m_pipeline_reg[ID_OC_SP]);
<     in_ports.push_back(&m_pipeline_reg[ID_OC_SFU]);
<     in_ports.push_back(&m_pipeline_reg[ID_OC_MEM]);
<     out_ports.push_back(&m_pipeline_reg[OC_EX_SP]);
<     out_ports.push_back(&m_pipeline_reg[OC_EX_SFU]);
<     out_ports.push_back(&m_pipeline_reg[OC_EX_MEM]);
<     cu_sets.push_back((unsigned)GEN_CUS);
<     m_operand_collector.add_port(in_ports, out_ports, cu_sets);
<     in_ports.clear(), out_ports.clear(), cu_sets.clear();
<   }
< 
<   m_operand_collector.init( m_config->gpgpu_num_reg_banks, this );
< 
<   // execute
<   m_num_function_units = m_config->gpgpu_num_sp_units + m_config->gpgpu_num_sfu_units + 1; // sp_unit, sfu, ldst_unit
<   //m_dispatch_port = new enum pipeline_stage_name_t[ m_num_function_units ];
<   //m_issue_port = new enum pipeline_stage_name_t[ m_num_function_units ];
< 
<   //m_fu = new simd_function_unit*[m_num_function_units];
< 
<   for (int k = 0; k < m_config->gpgpu_num_sp_units; k++) {
<     m_fu.push_back(new sp_unit( &m_pipeline_reg[EX_WB], m_config, this ));
<     m_dispatch_port.push_back(ID_OC_SP);
<     m_issue_port.push_back(OC_EX_SP);
<   }
< 
<   for (int k = 0; k < m_config->gpgpu_num_sfu_units; k++) {
<     m_fu.push_back(new sfu( &m_pipeline_reg[EX_WB], m_config, this ));
<     m_dispatch_port.push_back(ID_OC_SFU);
<     m_issue_port.push_back(OC_EX_SFU);
<   }
< 
<   m_ldst_unit = new ldst_unit( m_icnt, m_mem_fetch_allocator, this, &m_operand_collector, m_scoreboard, config, mem_config, stats, shader_id, tpc_id );
<   m_fu.push_back(m_ldst_unit);
<   m_dispatch_port.push_back(ID_OC_MEM);
<   m_issue_port.push_back(OC_EX_MEM);
< 
<   assert(m_num_function_units == m_fu.size() and m_fu.size() == m_dispatch_port.size() and m_fu.size() == m_issue_port.size());
< 
<   //there are as many result buses as the width of the EX_WB stage
<   num_result_bus = config->pipe_widths[EX_WB];
<   for (unsigned i = 0; i < num_result_bus; i++) {
<     this->m_result_bus.push_back(new std::bitset<MAX_ALU_LATENCY>());
<   }
< 
<   m_last_inst_gpu_sim_cycle = 0;
<   m_last_inst_gpu_tot_sim_cycle = 0;
< }
< 
< void shader_core_ctx::reinit(unsigned start_thread, unsigned end_thread, bool reset_not_completed )
< {
<   if ( reset_not_completed ) {
---
>    : core_t( gpu, NULL, config->warp_size, config->n_thread_per_shader ),
>      m_barriers( this, config->max_warps_per_shader, config->max_cta_per_core, config->max_barriers_per_cta, config->warp_size ),
>      m_dynamic_warp_id(0)
> {
>     m_cluster = cluster;
>     m_config = config;
>     m_memory_config = mem_config;
>     m_stats = stats;
>     unsigned warp_size=config->warp_size;
>     
>     m_sid = shader_id;
>     m_tpc = tpc_id;
>     
>     m_pipeline_reg.reserve(N_PIPELINE_STAGES);
>     for (int j = 0; j<N_PIPELINE_STAGES; j++) {
>         m_pipeline_reg.push_back(register_set(m_config->pipe_widths[j],pipeline_stage_name_decode[j]));
>     }
>     
>     m_threadState = (thread_ctx_t*) calloc(sizeof(thread_ctx_t), config->n_thread_per_shader);
>     
306,314c97,314
<   }
<   for (unsigned i = start_thread; i < end_thread; i++) {
<     m_threadState[i].n_insn = 0;
<     m_threadState[i].m_cta_id = -1;
<   }
<   for (unsigned i = start_thread / m_config->warp_size; i < end_thread / m_config->warp_size; ++i) {
<     m_warp[i].reset();
<     m_simt_stack[i]->reset();
<   }
---
>     m_n_active_cta = 0;
>     for ( unsigned i = 0; i<MAX_CTA_PER_SHADER; i++ ) 
>         m_cta_status[i]=0;
>     for (unsigned i = 0; i<config->n_thread_per_shader; i++) {
>         m_thread[i]= NULL;
>         m_threadState[i].m_cta_id = -1;
>         m_threadState[i].m_active = false;
>     }
>     
>     // m_icnt = new shader_memory_interface(this,cluster);
>     if ( m_config->gpgpu_perfect_mem ) {
>         m_icnt = new perfect_memory_interface(this,cluster);
>     } else {
>         m_icnt = new shader_memory_interface(this,cluster);
>     }
>     m_mem_fetch_allocator = new shader_core_mem_fetch_allocator(shader_id,tpc_id,mem_config);
>     
>     // fetch
>     m_last_warp_fetched = 0;
>     
>     #define STRSIZE 1024
>     char name[STRSIZE];
>     snprintf(name, STRSIZE, "L1I_%03d", m_sid);
>     m_L1I = new read_only_cache( name,m_config->m_L1I_config,m_sid,get_shader_instruction_cache_id(),m_icnt,IN_L1I_MISS_QUEUE);
>     
>     m_warp.resize(m_config->max_warps_per_shader, shd_warp_t(this, warp_size));
>     m_scoreboard = new Scoreboard(m_sid, m_config->max_warps_per_shader);
>     
>     //scedulers
>     //must currently occur after all inputs have been initialized.
>     std::string sched_config = m_config->gpgpu_scheduler_string;
>     const concrete_scheduler scheduler = sched_config.find("lrr") != std::string::npos ?
>                                          CONCRETE_SCHEDULER_LRR :
>                                          sched_config.find("two_level_active") != std::string::npos ?
>                                          CONCRETE_SCHEDULER_TWO_LEVEL_ACTIVE :
>                                          sched_config.find("gto") != std::string::npos ?
>                                          CONCRETE_SCHEDULER_GTO :
>                                          sched_config.find("warp_limiting") != std::string::npos ?
>                                          CONCRETE_SCHEDULER_WARP_LIMITING:
>                                          NUM_CONCRETE_SCHEDULERS;
>     assert ( scheduler != NUM_CONCRETE_SCHEDULERS );
>     
>     for (int i = 0; i < m_config->gpgpu_num_sched_per_core; i++) {
>         switch( scheduler )
>         {
>             case CONCRETE_SCHEDULER_LRR:
>                 schedulers.push_back(
>                     new lrr_scheduler( m_stats,
>                                        this,
>                                        m_scoreboard,
>                                        m_simt_stack,
>                                        &m_warp,
>                                        &m_pipeline_reg[ID_OC_SP],
>                                        &m_pipeline_reg[ID_OC_SFU],
>                                        &m_pipeline_reg[ID_OC_MEM],
>                                        i
>                                      )
>                 );
>                 break;
>             case CONCRETE_SCHEDULER_TWO_LEVEL_ACTIVE:
>                 schedulers.push_back(
>                     new two_level_active_scheduler( m_stats,
>                                                     this,
>                                                     m_scoreboard,
>                                                     m_simt_stack,
>                                                     &m_warp,
>                                                     &m_pipeline_reg[ID_OC_SP],
>                                                     &m_pipeline_reg[ID_OC_SFU],
>                                                     &m_pipeline_reg[ID_OC_MEM],
>                                                     i,
>                                                     config->gpgpu_scheduler_string
>                                                   )
>                 );
>                 break;
>             case CONCRETE_SCHEDULER_GTO:
>                 schedulers.push_back(
>                     new gto_scheduler( m_stats,
>                                        this,
>                                        m_scoreboard,
>                                        m_simt_stack,
>                                        &m_warp,
>                                        &m_pipeline_reg[ID_OC_SP],
>                                        &m_pipeline_reg[ID_OC_SFU],
>                                        &m_pipeline_reg[ID_OC_MEM],
>                                        i
>                                      )
>                 );
>                 break;
>             case CONCRETE_SCHEDULER_WARP_LIMITING:
>                 schedulers.push_back(
>                     new swl_scheduler( m_stats,
>                                        this,
>                                        m_scoreboard,
>                                        m_simt_stack,
>                                        &m_warp,
>                                        &m_pipeline_reg[ID_OC_SP],
>                                        &m_pipeline_reg[ID_OC_SFU],
>                                        &m_pipeline_reg[ID_OC_MEM],
>                                        i,
>                                        config->gpgpu_scheduler_string
>                                      )
>                 );
>                 break;
>             default:
>                 abort();
>         };
>     }
>     
>     for (unsigned i = 0; i < m_warp.size(); i++) {
>         //distribute i's evenly though schedulers;
>         schedulers[i%m_config->gpgpu_num_sched_per_core]->add_supervised_warp_id(i);
>     }
>     for ( int i = 0; i < m_config->gpgpu_num_sched_per_core; ++i ) {
>         schedulers[i]->done_adding_supervised_warps();
>     }
>     
>     //op collector configuration
>     enum { SP_CUS, SFU_CUS, MEM_CUS, GEN_CUS };
>     m_operand_collector.add_cu_set(SP_CUS, m_config->gpgpu_operand_collector_num_units_sp, m_config->gpgpu_operand_collector_num_out_ports_sp);
>     m_operand_collector.add_cu_set(SFU_CUS, m_config->gpgpu_operand_collector_num_units_sfu, m_config->gpgpu_operand_collector_num_out_ports_sfu);
>     m_operand_collector.add_cu_set(MEM_CUS, m_config->gpgpu_operand_collector_num_units_mem, m_config->gpgpu_operand_collector_num_out_ports_mem);
>     m_operand_collector.add_cu_set(GEN_CUS, m_config->gpgpu_operand_collector_num_units_gen, m_config->gpgpu_operand_collector_num_out_ports_gen);
>     
>     opndcoll_rfu_t::port_vector_t in_ports;
>     opndcoll_rfu_t::port_vector_t out_ports;
>     opndcoll_rfu_t::uint_vector_t cu_sets;
>     for (unsigned i = 0; i < m_config->gpgpu_operand_collector_num_in_ports_sp; i++) {
>         in_ports.push_back(&m_pipeline_reg[ID_OC_SP]);
>         out_ports.push_back(&m_pipeline_reg[OC_EX_SP]);
>         cu_sets.push_back((unsigned)SP_CUS);
>         cu_sets.push_back((unsigned)GEN_CUS);
>         m_operand_collector.add_port(in_ports,out_ports,cu_sets);
>         in_ports.clear(),out_ports.clear(),cu_sets.clear();
>     }
>     
>     for (unsigned i = 0; i < m_config->gpgpu_operand_collector_num_in_ports_sfu; i++) {
>         in_ports.push_back(&m_pipeline_reg[ID_OC_SFU]);
>         out_ports.push_back(&m_pipeline_reg[OC_EX_SFU]);
>         cu_sets.push_back((unsigned)SFU_CUS);
>         cu_sets.push_back((unsigned)GEN_CUS);
>         m_operand_collector.add_port(in_ports,out_ports,cu_sets);
>         in_ports.clear(),out_ports.clear(),cu_sets.clear();
>     }
>     
>     for (unsigned i = 0; i < m_config->gpgpu_operand_collector_num_in_ports_mem; i++) {
>         in_ports.push_back(&m_pipeline_reg[ID_OC_MEM]);
>         out_ports.push_back(&m_pipeline_reg[OC_EX_MEM]);
>         cu_sets.push_back((unsigned)MEM_CUS);
>         cu_sets.push_back((unsigned)GEN_CUS);                       
>         m_operand_collector.add_port(in_ports,out_ports,cu_sets);
>         in_ports.clear(),out_ports.clear(),cu_sets.clear();
>     }   
>     
>     
>     for (unsigned i = 0; i < m_config->gpgpu_operand_collector_num_in_ports_gen; i++) {
>         in_ports.push_back(&m_pipeline_reg[ID_OC_SP]);
>         in_ports.push_back(&m_pipeline_reg[ID_OC_SFU]);
>         in_ports.push_back(&m_pipeline_reg[ID_OC_MEM]);
>         out_ports.push_back(&m_pipeline_reg[OC_EX_SP]);
>         out_ports.push_back(&m_pipeline_reg[OC_EX_SFU]);
>         out_ports.push_back(&m_pipeline_reg[OC_EX_MEM]);
>         cu_sets.push_back((unsigned)GEN_CUS);   
>         m_operand_collector.add_port(in_ports,out_ports,cu_sets);
>         in_ports.clear(),out_ports.clear(),cu_sets.clear();
>     }
>     
>     m_operand_collector.init( m_config->gpgpu_num_reg_banks, this );
>     
>     // execute
>     m_num_function_units = m_config->gpgpu_num_sp_units + m_config->gpgpu_num_sfu_units + 1; // sp_unit, sfu, ldst_unit
>     //m_dispatch_port = new enum pipeline_stage_name_t[ m_num_function_units ];
>     //m_issue_port = new enum pipeline_stage_name_t[ m_num_function_units ];
>     
>     //m_fu = new simd_function_unit*[m_num_function_units];
>     
>     for (int k = 0; k < m_config->gpgpu_num_sp_units; k++) {
>         m_fu.push_back(new sp_unit( &m_pipeline_reg[EX_WB], m_config, this ));
>         m_dispatch_port.push_back(ID_OC_SP);
>         m_issue_port.push_back(OC_EX_SP);
>     }
>     
>     for (int k = 0; k < m_config->gpgpu_num_sfu_units; k++) {
>         m_fu.push_back(new sfu( &m_pipeline_reg[EX_WB], m_config, this ));
>         m_dispatch_port.push_back(ID_OC_SFU);
>         m_issue_port.push_back(OC_EX_SFU);
>     }
>     
>     m_ldst_unit = new ldst_unit( m_icnt, m_mem_fetch_allocator, this, &m_operand_collector, m_scoreboard, config, mem_config, stats, shader_id, tpc_id );
>     m_fu.push_back(m_ldst_unit);
>     m_dispatch_port.push_back(ID_OC_MEM);
>     m_issue_port.push_back(OC_EX_MEM);
>     
>     assert(m_num_function_units == m_fu.size() and m_fu.size() == m_dispatch_port.size() and m_fu.size() == m_issue_port.size());
>     
>     //there are as many result buses as the width of the EX_WB stage
>     num_result_bus = config->pipe_widths[EX_WB];
>     for(unsigned i=0; i<num_result_bus; i++){
>         this->m_result_bus.push_back(new std::bitset<MAX_ALU_LATENCY>());
>     }
>     
>     m_last_inst_gpu_sim_cycle = 0;
>     m_last_inst_gpu_tot_sim_cycle = 0;
> }
> 
> void shader_core_ctx::reinit(unsigned start_thread, unsigned end_thread, bool reset_not_completed ) 
> {
>    if( reset_not_completed ) {
>        m_not_completed = 0;
>        m_active_threads.reset();
>    }
>    for (unsigned i = start_thread; i<end_thread; i++) {
>       m_threadState[i].n_insn = 0;
>       m_threadState[i].m_cta_id = -1;
>    }
>    for (unsigned i = start_thread / m_config->warp_size; i < end_thread / m_config->warp_size; ++i) {
>       m_warp[i].reset();
>       m_simt_stack[i]->reset();
>    }
319,333c319,338
<   address_type start_pc = next_pc(start_thread);
<   if (m_config->model == POST_DOMINATOR) {
<     unsigned start_warp = start_thread / m_config->warp_size;
<     unsigned end_warp = end_thread / m_config->warp_size + ((end_thread % m_config->warp_size) ? 1 : 0);
<     for (unsigned i = start_warp; i < end_warp; ++i) {
<       unsigned n_active = 0;
<       simt_mask_t active_threads;
<       for (unsigned t = 0; t < m_config->warp_size; t++) {
<         unsigned hwtid = i * m_config->warp_size + t;
<         if ( hwtid < end_thread ) {
<           n_active++;
<           assert( !m_active_threads.test(hwtid) );
<           m_active_threads.set( hwtid );
<           active_threads.set(t);
<         }
---
>     address_type start_pc = next_pc(start_thread);
>     if (m_config->model == POST_DOMINATOR) {
>         unsigned start_warp = start_thread / m_config->warp_size;
>         unsigned end_warp = end_thread / m_config->warp_size + ((end_thread % m_config->warp_size)? 1 : 0);
>         for (unsigned i = start_warp; i < end_warp; ++i) {
>             unsigned n_active=0;
>             simt_mask_t active_threads;
>             for (unsigned t = 0; t < m_config->warp_size; t++) {
>                 unsigned hwtid = i * m_config->warp_size + t;
>                 if ( hwtid < end_thread ) {
>                     n_active++;
>                     assert( !m_active_threads.test(hwtid) );
>                     m_active_threads.set( hwtid );
>                     active_threads.set(t);
>                 }
>             }
>             m_simt_stack[i]->launch(start_pc,active_threads);
>             m_warp[i].init(start_pc,cta_id,i,active_threads, m_dynamic_warp_id);
>             ++m_dynamic_warp_id;
>             m_not_completed += n_active;
335,340c340
<       m_simt_stack[i]->launch(start_pc, active_threads);
<       m_warp[i].init(start_pc, cta_id, i, active_threads, m_dynamic_warp_id);
<       ++m_dynamic_warp_id;
<       m_not_completed += n_active;
<     }
<   }
---
>    }
343c343
< // return the next pc of a thread
---
> // return the next pc of a thread 
346,351c346,351
<   if ( tid == -1 )
<     return -1;
<   ptx_thread_info *the_thread = m_thread[tid];
<   if ( the_thread == NULL )
<     return -1;
<   return the_thread->get_pc(); // PC should already be updatd to next PC at this point (was set in shader_decode() last time thread ran)
---
>     if( tid == -1 ) 
>         return -1;
>     ptx_thread_info *the_thread = m_thread[tid];
>     if ( the_thread == NULL )
>         return -1;
>     return the_thread->get_pc(); // PC should already be updatd to next PC at this point (was set in shader_decode() last time thread ran)
356,357c356,357
<   unsigned cluster_id = m_shader_config->sid_to_cluster(sid);
<   m_cluster[cluster_id]->get_pdom_stack_top_info(sid, tid, pc, rpc);
---
>     unsigned cluster_id = m_shader_config->sid_to_cluster(sid);
>     m_cluster[cluster_id]->get_pdom_stack_top_info(sid,tid,pc,rpc);
362,363c362,363
<   unsigned warp_id = tid / m_config->warp_size;
<   m_simt_stack[warp_id]->get_pdom_stack_top_info(pc, rpc);
---
>     unsigned warp_id = tid/m_config->warp_size;
>     m_simt_stack[warp_id]->get_pdom_stack_top_info(pc,rpc);
368,369c368,369
<   unsigned long long  thread_icount_uarch = 0;
<   unsigned long long  warp_icount_uarch = 0;
---
> 	unsigned long long  thread_icount_uarch=0;
> 	unsigned long long  warp_icount_uarch=0;
371,450c371,450
<   for (unsigned i = 0; i < m_config->num_shader(); i++) {
<     thread_icount_uarch += m_num_sim_insn[i];
<     warp_icount_uarch += m_num_sim_winsn[i];
<   }
<   fprintf(fout, "gpgpu_n_tot_thrd_icount = %lld\n", thread_icount_uarch);
<   fprintf(fout, "gpgpu_n_tot_w_icount = %lld\n", warp_icount_uarch);
< 
<   fprintf(fout, "gpgpu_n_stall_shd_mem = %d\n", gpgpu_n_stall_shd_mem );
<   fprintf(fout, "gpgpu_n_mem_read_local = %d\n", gpgpu_n_mem_read_local);
<   fprintf(fout, "gpgpu_n_mem_write_local = %d\n", gpgpu_n_mem_write_local);
<   fprintf(fout, "gpgpu_n_mem_read_global = %d\n", gpgpu_n_mem_read_global);
<   fprintf(fout, "gpgpu_n_mem_write_global = %d\n", gpgpu_n_mem_write_global);
<   fprintf(fout, "gpgpu_n_mem_texture = %d\n", gpgpu_n_mem_texture);
<   fprintf(fout, "gpgpu_n_mem_const = %d\n", gpgpu_n_mem_const);
< 
<   fprintf(fout, "gpgpu_n_load_insn  = %d\n", gpgpu_n_load_insn);
<   fprintf(fout, "gpgpu_n_store_insn = %d\n", gpgpu_n_store_insn);
<   fprintf(fout, "gpgpu_n_shmem_insn = %d\n", gpgpu_n_shmem_insn);
<   fprintf(fout, "gpgpu_n_tex_insn = %d\n", gpgpu_n_tex_insn);
<   fprintf(fout, "gpgpu_n_const_mem_insn = %d\n", gpgpu_n_const_insn);
<   fprintf(fout, "gpgpu_n_param_mem_insn = %d\n", gpgpu_n_param_insn);
< 
<   fprintf(fout, "gpgpu_n_shmem_bkconflict = %d\n", gpgpu_n_shmem_bkconflict);
<   fprintf(fout, "gpgpu_n_cache_bkconflict = %d\n", gpgpu_n_cache_bkconflict);
< 
<   fprintf(fout, "gpgpu_n_intrawarp_mshr_merge = %d\n", gpgpu_n_intrawarp_mshr_merge);
<   fprintf(fout, "gpgpu_n_cmem_portconflict = %d\n", gpgpu_n_cmem_portconflict);
< 
<   fprintf(fout, "gpgpu_stall_shd_mem[c_mem][bk_conf] = %d\n", gpu_stall_shd_mem_breakdown[C_MEM][BK_CONF]);
<   fprintf(fout, "gpgpu_stall_shd_mem[c_mem][mshr_rc] = %d\n", gpu_stall_shd_mem_breakdown[C_MEM][MSHR_RC_FAIL]);
<   fprintf(fout, "gpgpu_stall_shd_mem[c_mem][icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[C_MEM][ICNT_RC_FAIL]);
<   fprintf(fout, "gpgpu_stall_shd_mem[c_mem][data_port_stall] = %d\n", gpu_stall_shd_mem_breakdown[C_MEM][DATA_PORT_STALL]);
<   fprintf(fout, "gpgpu_stall_shd_mem[t_mem][mshr_rc] = %d\n", gpu_stall_shd_mem_breakdown[T_MEM][MSHR_RC_FAIL]);
<   fprintf(fout, "gpgpu_stall_shd_mem[t_mem][icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[T_MEM][ICNT_RC_FAIL]);
<   fprintf(fout, "gpgpu_stall_shd_mem[t_mem][data_port_stall] = %d\n", gpu_stall_shd_mem_breakdown[T_MEM][DATA_PORT_STALL]);
<   fprintf(fout, "gpgpu_stall_shd_mem[s_mem][bk_conf] = %d\n", gpu_stall_shd_mem_breakdown[S_MEM][BK_CONF]);
<   fprintf(fout, "gpgpu_stall_shd_mem[gl_mem][bk_conf] = %d\n",
<           gpu_stall_shd_mem_breakdown[G_MEM_LD][BK_CONF] +
<           gpu_stall_shd_mem_breakdown[G_MEM_ST][BK_CONF] +
<           gpu_stall_shd_mem_breakdown[L_MEM_LD][BK_CONF] +
<           gpu_stall_shd_mem_breakdown[L_MEM_ST][BK_CONF]
<          ); // coalescing stall at data cache
<   fprintf(fout, "gpgpu_stall_shd_mem[gl_mem][coal_stall] = %d\n",
<           gpu_stall_shd_mem_breakdown[G_MEM_LD][COAL_STALL] +
<           gpu_stall_shd_mem_breakdown[G_MEM_ST][COAL_STALL] +
<           gpu_stall_shd_mem_breakdown[L_MEM_LD][COAL_STALL] +
<           gpu_stall_shd_mem_breakdown[L_MEM_ST][COAL_STALL]
<          ); // coalescing stall + bank conflict at data cache
<   fprintf(fout, "gpgpu_stall_shd_mem[gl_mem][data_port_stall] = %d\n",
<           gpu_stall_shd_mem_breakdown[G_MEM_LD][DATA_PORT_STALL] +
<           gpu_stall_shd_mem_breakdown[G_MEM_ST][DATA_PORT_STALL] +
<           gpu_stall_shd_mem_breakdown[L_MEM_LD][DATA_PORT_STALL] +
<           gpu_stall_shd_mem_breakdown[L_MEM_ST][DATA_PORT_STALL]
<          ); // data port stall at data cache
<   fprintf(fout, "gpgpu_stall_shd_mem[g_mem_ld][mshr_rc] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_LD][MSHR_RC_FAIL]);
<   fprintf(fout, "gpgpu_stall_shd_mem[g_mem_ld][icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_LD][ICNT_RC_FAIL]);
<   fprintf(fout, "gpgpu_stall_shd_mem[g_mem_ld][wb_icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_LD][WB_ICNT_RC_FAIL]);
<   fprintf(fout, "gpgpu_stall_shd_mem[g_mem_ld][wb_rsrv_fail] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_LD][WB_CACHE_RSRV_FAIL]);
<   fprintf(fout, "gpgpu_stall_shd_mem[g_mem_st][mshr_rc] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_ST][MSHR_RC_FAIL]);
<   fprintf(fout, "gpgpu_stall_shd_mem[g_mem_st][icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_ST][ICNT_RC_FAIL]);
<   fprintf(fout, "gpgpu_stall_shd_mem[g_mem_st][wb_icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_ST][WB_ICNT_RC_FAIL]);
<   fprintf(fout, "gpgpu_stall_shd_mem[g_mem_st][wb_rsrv_fail] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_ST][WB_CACHE_RSRV_FAIL]);
<   fprintf(fout, "gpgpu_stall_shd_mem[l_mem_ld][mshr_rc] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_LD][MSHR_RC_FAIL]);
<   fprintf(fout, "gpgpu_stall_shd_mem[l_mem_ld][icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_LD][ICNT_RC_FAIL]);
<   fprintf(fout, "gpgpu_stall_shd_mem[l_mem_ld][wb_icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_LD][WB_ICNT_RC_FAIL]);
<   fprintf(fout, "gpgpu_stall_shd_mem[l_mem_ld][wb_rsrv_fail] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_LD][WB_CACHE_RSRV_FAIL]);
<   fprintf(fout, "gpgpu_stall_shd_mem[l_mem_st][mshr_rc] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_ST][MSHR_RC_FAIL]);
<   fprintf(fout, "gpgpu_stall_shd_mem[l_mem_st][icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_ST][ICNT_RC_FAIL]);
<   fprintf(fout, "gpgpu_stall_shd_mem[l_mem_ld][wb_icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_ST][WB_ICNT_RC_FAIL]);
<   fprintf(fout, "gpgpu_stall_shd_mem[l_mem_ld][wb_rsrv_fail] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_ST][WB_CACHE_RSRV_FAIL]);
< 
<   fprintf(fout, "gpu_reg_bank_conflict_stalls = %d\n", gpu_reg_bank_conflict_stalls);
< 
<   fprintf(fout, "Warp Occupancy Distribution:\n");
<   fprintf(fout, "Stall:%d\t", shader_cycle_distro[2]);
<   fprintf(fout, "W0_Idle:%d\t", shader_cycle_distro[0]);
<   fprintf(fout, "W0_Scoreboard:%d", shader_cycle_distro[1]);
<   for (unsigned i = 3; i < m_config->warp_size + 3; i++)
<     fprintf(fout, "\tW%d:%d", i - 2, shader_cycle_distro[i]);
<   fprintf(fout, "\n");
---
>     for(unsigned i=0; i < m_config->num_shader(); i++) {
>         thread_icount_uarch += m_num_sim_insn[i];
>         warp_icount_uarch += m_num_sim_winsn[i];
>     }
>     fprintf(fout,"gpgpu_n_tot_thrd_icount = %lld\n", thread_icount_uarch);
>     fprintf(fout,"gpgpu_n_tot_w_icount = %lld\n", warp_icount_uarch);
> 
>     fprintf(fout,"gpgpu_n_stall_shd_mem = %d\n", gpgpu_n_stall_shd_mem );
>     fprintf(fout,"gpgpu_n_mem_read_local = %d\n", gpgpu_n_mem_read_local);
>     fprintf(fout,"gpgpu_n_mem_write_local = %d\n", gpgpu_n_mem_write_local);
>     fprintf(fout,"gpgpu_n_mem_read_global = %d\n", gpgpu_n_mem_read_global);
>     fprintf(fout,"gpgpu_n_mem_write_global = %d\n", gpgpu_n_mem_write_global);
>     fprintf(fout,"gpgpu_n_mem_texture = %d\n", gpgpu_n_mem_texture);
>     fprintf(fout,"gpgpu_n_mem_const = %d\n", gpgpu_n_mem_const);
> 
>    fprintf(fout, "gpgpu_n_load_insn  = %d\n", gpgpu_n_load_insn);
>    fprintf(fout, "gpgpu_n_store_insn = %d\n", gpgpu_n_store_insn);
>    fprintf(fout, "gpgpu_n_shmem_insn = %d\n", gpgpu_n_shmem_insn);
>    fprintf(fout, "gpgpu_n_tex_insn = %d\n", gpgpu_n_tex_insn);
>    fprintf(fout, "gpgpu_n_const_mem_insn = %d\n", gpgpu_n_const_insn);
>    fprintf(fout, "gpgpu_n_param_mem_insn = %d\n", gpgpu_n_param_insn);
> 
>    fprintf(fout, "gpgpu_n_shmem_bkconflict = %d\n", gpgpu_n_shmem_bkconflict);
>    fprintf(fout, "gpgpu_n_cache_bkconflict = %d\n", gpgpu_n_cache_bkconflict);   
> 
>    fprintf(fout, "gpgpu_n_intrawarp_mshr_merge = %d\n", gpgpu_n_intrawarp_mshr_merge);
>    fprintf(fout, "gpgpu_n_cmem_portconflict = %d\n", gpgpu_n_cmem_portconflict);
> 
>    fprintf(fout, "gpgpu_stall_shd_mem[c_mem][bk_conf] = %d\n", gpu_stall_shd_mem_breakdown[C_MEM][BK_CONF]);
>    fprintf(fout, "gpgpu_stall_shd_mem[c_mem][mshr_rc] = %d\n", gpu_stall_shd_mem_breakdown[C_MEM][MSHR_RC_FAIL]);
>    fprintf(fout, "gpgpu_stall_shd_mem[c_mem][icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[C_MEM][ICNT_RC_FAIL]);
>    fprintf(fout, "gpgpu_stall_shd_mem[c_mem][data_port_stall] = %d\n", gpu_stall_shd_mem_breakdown[C_MEM][DATA_PORT_STALL]);
>    fprintf(fout, "gpgpu_stall_shd_mem[t_mem][mshr_rc] = %d\n", gpu_stall_shd_mem_breakdown[T_MEM][MSHR_RC_FAIL]);
>    fprintf(fout, "gpgpu_stall_shd_mem[t_mem][icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[T_MEM][ICNT_RC_FAIL]);
>    fprintf(fout, "gpgpu_stall_shd_mem[t_mem][data_port_stall] = %d\n", gpu_stall_shd_mem_breakdown[T_MEM][DATA_PORT_STALL]);
>    fprintf(fout, "gpgpu_stall_shd_mem[s_mem][bk_conf] = %d\n", gpu_stall_shd_mem_breakdown[S_MEM][BK_CONF]);
>    fprintf(fout, "gpgpu_stall_shd_mem[gl_mem][bk_conf] = %d\n", 
>            gpu_stall_shd_mem_breakdown[G_MEM_LD][BK_CONF] + 
>            gpu_stall_shd_mem_breakdown[G_MEM_ST][BK_CONF] + 
>            gpu_stall_shd_mem_breakdown[L_MEM_LD][BK_CONF] + 
>            gpu_stall_shd_mem_breakdown[L_MEM_ST][BK_CONF]   
>            ); // coalescing stall at data cache 
>    fprintf(fout, "gpgpu_stall_shd_mem[gl_mem][coal_stall] = %d\n", 
>            gpu_stall_shd_mem_breakdown[G_MEM_LD][COAL_STALL] + 
>            gpu_stall_shd_mem_breakdown[G_MEM_ST][COAL_STALL] + 
>            gpu_stall_shd_mem_breakdown[L_MEM_LD][COAL_STALL] + 
>            gpu_stall_shd_mem_breakdown[L_MEM_ST][COAL_STALL]    
>            ); // coalescing stall + bank conflict at data cache 
>    fprintf(fout, "gpgpu_stall_shd_mem[gl_mem][data_port_stall] = %d\n", 
>            gpu_stall_shd_mem_breakdown[G_MEM_LD][DATA_PORT_STALL] + 
>            gpu_stall_shd_mem_breakdown[G_MEM_ST][DATA_PORT_STALL] + 
>            gpu_stall_shd_mem_breakdown[L_MEM_LD][DATA_PORT_STALL] + 
>            gpu_stall_shd_mem_breakdown[L_MEM_ST][DATA_PORT_STALL]    
>            ); // data port stall at data cache 
>    fprintf(fout, "gpgpu_stall_shd_mem[g_mem_ld][mshr_rc] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_LD][MSHR_RC_FAIL]);
>    fprintf(fout, "gpgpu_stall_shd_mem[g_mem_ld][icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_LD][ICNT_RC_FAIL]);
>    fprintf(fout, "gpgpu_stall_shd_mem[g_mem_ld][wb_icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_LD][WB_ICNT_RC_FAIL]);
>    fprintf(fout, "gpgpu_stall_shd_mem[g_mem_ld][wb_rsrv_fail] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_LD][WB_CACHE_RSRV_FAIL]);
>    fprintf(fout, "gpgpu_stall_shd_mem[g_mem_st][mshr_rc] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_ST][MSHR_RC_FAIL]);
>    fprintf(fout, "gpgpu_stall_shd_mem[g_mem_st][icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_ST][ICNT_RC_FAIL]);
>    fprintf(fout, "gpgpu_stall_shd_mem[g_mem_st][wb_icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_ST][WB_ICNT_RC_FAIL]);
>    fprintf(fout, "gpgpu_stall_shd_mem[g_mem_st][wb_rsrv_fail] = %d\n", gpu_stall_shd_mem_breakdown[G_MEM_ST][WB_CACHE_RSRV_FAIL]);
>    fprintf(fout, "gpgpu_stall_shd_mem[l_mem_ld][mshr_rc] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_LD][MSHR_RC_FAIL]);
>    fprintf(fout, "gpgpu_stall_shd_mem[l_mem_ld][icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_LD][ICNT_RC_FAIL]);
>    fprintf(fout, "gpgpu_stall_shd_mem[l_mem_ld][wb_icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_LD][WB_ICNT_RC_FAIL]);
>    fprintf(fout, "gpgpu_stall_shd_mem[l_mem_ld][wb_rsrv_fail] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_LD][WB_CACHE_RSRV_FAIL]);
>    fprintf(fout, "gpgpu_stall_shd_mem[l_mem_st][mshr_rc] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_ST][MSHR_RC_FAIL]);
>    fprintf(fout, "gpgpu_stall_shd_mem[l_mem_st][icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_ST][ICNT_RC_FAIL]);
>    fprintf(fout, "gpgpu_stall_shd_mem[l_mem_ld][wb_icnt_rc] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_ST][WB_ICNT_RC_FAIL]);
>    fprintf(fout, "gpgpu_stall_shd_mem[l_mem_ld][wb_rsrv_fail] = %d\n", gpu_stall_shd_mem_breakdown[L_MEM_ST][WB_CACHE_RSRV_FAIL]);
> 
>    fprintf(fout, "gpu_reg_bank_conflict_stalls = %d\n", gpu_reg_bank_conflict_stalls);
> 
>    fprintf(fout, "Warp Occupancy Distribution:\n");
>    fprintf(fout, "Stall:%d\t", shader_cycle_distro[2]);
>    fprintf(fout, "W0_Idle:%d\t", shader_cycle_distro[0]);
>    fprintf(fout, "W0_Scoreboard:%d", shader_cycle_distro[1]);
>    for (unsigned i = 3; i < m_config->warp_size + 3; i++) 
>       fprintf(fout, "\tW%d:%d", i-2, shader_cycle_distro[i]);
>    fprintf(fout, "\n");
452,453c452,453
<   m_outgoing_traffic_stats->print(fout);
<   m_incoming_traffic_stats->print(fout);
---
>    m_outgoing_traffic_stats->print(fout); 
>    m_incoming_traffic_stats->print(fout); 
457,464c457,466
<   assert( warp_id <= m_config->max_warps_per_shader );
<   for ( unsigned i = 0; i < num_issued; ++i ) {
<     if ( m_shader_dynamic_warp_issue_distro[ s_id ].size() <= dynamic_warp_id ) {
<       m_shader_dynamic_warp_issue_distro[ s_id ].resize(dynamic_warp_id + 1);
<     }
<     ++m_shader_dynamic_warp_issue_distro[ s_id ][ dynamic_warp_id ];
<     if ( m_shader_warp_slot_issue_distro[ s_id ].size() <= warp_id ) {
<       m_shader_warp_slot_issue_distro[ s_id ].resize(warp_id + 1);
---
>     assert( warp_id <= m_config->max_warps_per_shader );
>     for ( unsigned i = 0; i < num_issued; ++i ) {
>         if ( m_shader_dynamic_warp_issue_distro[ s_id ].size() <= dynamic_warp_id ) {
>             m_shader_dynamic_warp_issue_distro[ s_id ].resize(dynamic_warp_id + 1);
>         }
>         ++m_shader_dynamic_warp_issue_distro[ s_id ][ dynamic_warp_id ];
>         if ( m_shader_warp_slot_issue_distro[ s_id ].size() <= warp_id ) {
>             m_shader_warp_slot_issue_distro[ s_id ].resize(warp_id + 1);
>         }
>         ++m_shader_warp_slot_issue_distro[ s_id ][ warp_id ];
466,467d467
<     ++m_shader_warp_slot_issue_distro[ s_id ][ warp_id ];
<   }
472,527c472,487
<   // warp divergence breakdown
<   gzprintf(visualizer_file, "WarpDivergenceBreakdown:");
<   unsigned int total = 0;
<   unsigned int cf = (m_config->gpgpu_warpdistro_shader == -1) ? m_config->num_shader() : 1;
<   gzprintf(visualizer_file, " %d", (shader_cycle_distro[0] - last_shader_cycle_distro[0]) / cf );
<   gzprintf(visualizer_file, " %d", (shader_cycle_distro[1] - last_shader_cycle_distro[1]) / cf );
<   gzprintf(visualizer_file, " %d", (shader_cycle_distro[2] - last_shader_cycle_distro[2]) / cf );
<   for (unsigned i = 0; i < m_config->warp_size + 3; i++) {
<     if ( i >= 3 ) {
<       total += (shader_cycle_distro[i] - last_shader_cycle_distro[i]);
<       if ( ((i - 3) % (m_config->warp_size / 8)) == ((m_config->warp_size / 8) - 1) ) {
<         gzprintf(visualizer_file, " %d", total / cf );
<         total = 0;
<       }
<     }
<     last_shader_cycle_distro[i] = shader_cycle_distro[i];
<   }
<   gzprintf(visualizer_file, "\n");
< 
<   // warp issue breakdown
<   unsigned sid = m_config->gpgpu_warp_issue_shader;
<   unsigned count = 0;
<   unsigned warp_id_issued_sum = 0;
<   gzprintf(visualizer_file, "WarpIssueSlotBreakdown:");
<   if (m_shader_warp_slot_issue_distro[sid].size() > 0) {
<     for ( std::vector<unsigned>::const_iterator iter = m_shader_warp_slot_issue_distro[ sid ].begin();
<           iter != m_shader_warp_slot_issue_distro[ sid ].end(); iter++, count++ ) {
<       unsigned diff = count < m_last_shader_warp_slot_issue_distro.size() ?
<                       *iter - m_last_shader_warp_slot_issue_distro[ count ] :
<                       *iter;
<       gzprintf( visualizer_file, " %d", diff );
<       warp_id_issued_sum += diff;
<     }
<     m_last_shader_warp_slot_issue_distro = m_shader_warp_slot_issue_distro[ sid ];
<   } else {
<     gzprintf( visualizer_file, " 0");
<   }
<   gzprintf(visualizer_file, "\n");
< 
< #define DYNAMIC_WARP_PRINT_RESOLUTION 32
<   unsigned total_issued_this_resolution = 0;
<   unsigned dynamic_id_issued_sum = 0;
<   count = 0;
<   gzprintf(visualizer_file, "WarpIssueDynamicIdBreakdown:");
<   if (m_shader_dynamic_warp_issue_distro[sid].size() > 0) {
<     for ( std::vector<unsigned>::const_iterator iter = m_shader_dynamic_warp_issue_distro[ sid ].begin();
<           iter != m_shader_dynamic_warp_issue_distro[ sid ].end(); iter++, count++ ) {
<       unsigned diff = count < m_last_shader_dynamic_warp_issue_distro.size() ?
<                       *iter - m_last_shader_dynamic_warp_issue_distro[ count ] :
<                       *iter;
<       total_issued_this_resolution += diff;
<       if ( ( count + 1 ) % DYNAMIC_WARP_PRINT_RESOLUTION == 0 ) {
<         gzprintf( visualizer_file, " %d", total_issued_this_resolution );
<         dynamic_id_issued_sum += total_issued_this_resolution;
<         total_issued_this_resolution = 0;
<       }
---
>     // warp divergence breakdown
>     gzprintf(visualizer_file, "WarpDivergenceBreakdown:");
>     unsigned int total=0;
>     unsigned int cf = (m_config->gpgpu_warpdistro_shader==-1)?m_config->num_shader():1;
>     gzprintf(visualizer_file, " %d", (shader_cycle_distro[0] - last_shader_cycle_distro[0]) / cf );
>     gzprintf(visualizer_file, " %d", (shader_cycle_distro[1] - last_shader_cycle_distro[1]) / cf );
>     gzprintf(visualizer_file, " %d", (shader_cycle_distro[2] - last_shader_cycle_distro[2]) / cf );
>     for (unsigned i=0; i<m_config->warp_size+3; i++) {
>        if ( i>=3 ) {
>           total += (shader_cycle_distro[i] - last_shader_cycle_distro[i]);
>           if ( ((i-3) % (m_config->warp_size/8)) == ((m_config->warp_size/8)-1) ) {
>              gzprintf(visualizer_file, " %d", total / cf );
>              total=0;
>           }
>        }
>        last_shader_cycle_distro[i] = shader_cycle_distro[i];
529,538c489
<     if ( count % DYNAMIC_WARP_PRINT_RESOLUTION != 0 ) {
<       gzprintf( visualizer_file, " %d", total_issued_this_resolution );
<       dynamic_id_issued_sum += total_issued_this_resolution;
<     }
<     m_last_shader_dynamic_warp_issue_distro = m_shader_dynamic_warp_issue_distro[ sid ];
<     assert( warp_id_issued_sum == dynamic_id_issued_sum );
<   } else {
<     gzprintf( visualizer_file, " 0");
<   }
<   gzprintf(visualizer_file, "\n");
---
>     gzprintf(visualizer_file,"\n");
540,559c491,559
<   // overall cache miss rates
<   gzprintf(visualizer_file, "gpgpu_n_cache_bkconflict: %d\n", gpgpu_n_cache_bkconflict);
<   gzprintf(visualizer_file, "gpgpu_n_shmem_bkconflict: %d\n", gpgpu_n_shmem_bkconflict);
< 
< 
<   // instruction count per shader core
<   gzprintf(visualizer_file, "shaderinsncount:  ");
<   for (unsigned i = 0; i < m_config->num_shader(); i++)
<     gzprintf(visualizer_file, "%u ", m_num_sim_insn[i] );
<   gzprintf(visualizer_file, "\n");
<   // warp instruction count per shader core
<   gzprintf(visualizer_file, "shaderwarpinsncount:  ");
<   for (unsigned i = 0; i < m_config->num_shader(); i++)
<     gzprintf(visualizer_file, "%u ", m_num_sim_winsn[i] );
<   gzprintf(visualizer_file, "\n");
<   // warp divergence per shader core
<   gzprintf(visualizer_file, "shaderwarpdiv: ");
<   for (unsigned i = 0; i < m_config->num_shader(); i++)
<     gzprintf(visualizer_file, "%u ", m_n_diverge[i] );
<   gzprintf(visualizer_file, "\n");
---
>     // warp issue breakdown
>     unsigned sid = m_config->gpgpu_warp_issue_shader;
>     unsigned count = 0;
>     unsigned warp_id_issued_sum = 0;
>     gzprintf(visualizer_file, "WarpIssueSlotBreakdown:");
>     if(m_shader_warp_slot_issue_distro[sid].size() > 0){
>         for ( std::vector<unsigned>::const_iterator iter = m_shader_warp_slot_issue_distro[ sid ].begin();
>               iter != m_shader_warp_slot_issue_distro[ sid ].end(); iter++, count++ ) {
>             unsigned diff = count < m_last_shader_warp_slot_issue_distro.size() ?
>                             *iter - m_last_shader_warp_slot_issue_distro[ count ] :
>                             *iter;
>             gzprintf( visualizer_file, " %d", diff );
>             warp_id_issued_sum += diff;
>         }
>         m_last_shader_warp_slot_issue_distro = m_shader_warp_slot_issue_distro[ sid ];
>     }else{
>         gzprintf( visualizer_file, " 0");
>     }
>     gzprintf(visualizer_file,"\n");
> 
>     #define DYNAMIC_WARP_PRINT_RESOLUTION 32
>     unsigned total_issued_this_resolution = 0;
>     unsigned dynamic_id_issued_sum = 0;
>     count = 0;
>     gzprintf(visualizer_file, "WarpIssueDynamicIdBreakdown:");
>     if(m_shader_dynamic_warp_issue_distro[sid].size() > 0){
>         for ( std::vector<unsigned>::const_iterator iter = m_shader_dynamic_warp_issue_distro[ sid ].begin();
>               iter != m_shader_dynamic_warp_issue_distro[ sid ].end(); iter++, count++ ) {
>             unsigned diff = count < m_last_shader_dynamic_warp_issue_distro.size() ?
>                             *iter - m_last_shader_dynamic_warp_issue_distro[ count ] :
>                             *iter;
>             total_issued_this_resolution += diff;
>             if ( ( count + 1 ) % DYNAMIC_WARP_PRINT_RESOLUTION == 0 ) {
>                 gzprintf( visualizer_file, " %d", total_issued_this_resolution );
>                 dynamic_id_issued_sum += total_issued_this_resolution;
>                 total_issued_this_resolution = 0;
>             }
>         }
>         if ( count % DYNAMIC_WARP_PRINT_RESOLUTION != 0 ) {
>             gzprintf( visualizer_file, " %d", total_issued_this_resolution );
>             dynamic_id_issued_sum += total_issued_this_resolution;
>         }
>         m_last_shader_dynamic_warp_issue_distro = m_shader_dynamic_warp_issue_distro[ sid ];
>         assert( warp_id_issued_sum == dynamic_id_issued_sum );
>     }else{
>         gzprintf( visualizer_file, " 0");
>     }
>     gzprintf(visualizer_file,"\n");
> 
>     // overall cache miss rates
>     gzprintf(visualizer_file, "gpgpu_n_cache_bkconflict: %d\n", gpgpu_n_cache_bkconflict);
>     gzprintf(visualizer_file, "gpgpu_n_shmem_bkconflict: %d\n", gpgpu_n_shmem_bkconflict);     
> 
> 
>    // instruction count per shader core
>    gzprintf(visualizer_file, "shaderinsncount:  ");
>    for (unsigned i=0;i<m_config->num_shader();i++) 
>       gzprintf(visualizer_file, "%u ", m_num_sim_insn[i] );
>    gzprintf(visualizer_file, "\n");
>    // warp instruction count per shader core
>    gzprintf(visualizer_file, "shaderwarpinsncount:  ");
>    for (unsigned i=0;i<m_config->num_shader();i++)
>       gzprintf(visualizer_file, "%u ", m_num_sim_winsn[i] );
>    gzprintf(visualizer_file, "\n");
>    // warp divergence per shader core
>    gzprintf(visualizer_file, "shaderwarpdiv: ");
>    for (unsigned i=0;i<m_config->num_shader();i++) 
>       gzprintf(visualizer_file, "%u ", m_n_diverge[i] );
>    gzprintf(visualizer_file, "\n");
567,582c567,571
<   if ( m_inst_fetch_buffer.m_valid ) {
<     // decode 1 or 2 instructions and place them into ibuffer
<     address_type pc = m_inst_fetch_buffer.m_pc;
<     const warp_inst_t* pI1 = ptx_fetch_inst(pc);
<     m_warp[m_inst_fetch_buffer.m_warp_id].ibuffer_fill(0, pI1);
<     m_warp[m_inst_fetch_buffer.m_warp_id].inc_inst_in_pipeline();
<     if ( pI1 ) {
<       m_stats->m_num_decoded_insn[m_sid]++;
<       if (pI1->oprnd_type == INT_OP) {
<         m_stats->m_num_INTdecoded_insn[m_sid]++;
<       } else if (pI1->oprnd_type == FP_OP) {
<         m_stats->m_num_FPdecoded_insn[m_sid]++;
<       }
<       const warp_inst_t* pI2 = ptx_fetch_inst(pc + pI1->isize);
<       if ( pI2 ) {
<         m_warp[m_inst_fetch_buffer.m_warp_id].ibuffer_fill(1, pI2);
---
>     if( m_inst_fetch_buffer.m_valid ) {
>         // decode 1 or 2 instructions and place them into ibuffer
>         address_type pc = m_inst_fetch_buffer.m_pc;
>         const warp_inst_t* pI1 = ptx_fetch_inst(pc);
>         m_warp[m_inst_fetch_buffer.m_warp_id].ibuffer_fill(0,pI1);
584,588c573,590
<         m_stats->m_num_decoded_insn[m_sid]++;
<         if (pI2->oprnd_type == INT_OP) {
<           m_stats->m_num_INTdecoded_insn[m_sid]++;
<         } else if (pI2->oprnd_type == FP_OP) {
<           m_stats->m_num_FPdecoded_insn[m_sid]++;
---
>         if( pI1 ) {
>             m_stats->m_num_decoded_insn[m_sid]++;
>             if(pI1->oprnd_type==INT_OP){
>                 m_stats->m_num_INTdecoded_insn[m_sid]++;
>             }else if(pI1->oprnd_type==FP_OP) {
>             	m_stats->m_num_FPdecoded_insn[m_sid]++;
>             }
>            const warp_inst_t* pI2 = ptx_fetch_inst(pc+pI1->isize);
>            if( pI2 ) {
>                m_warp[m_inst_fetch_buffer.m_warp_id].ibuffer_fill(1,pI2);
>                m_warp[m_inst_fetch_buffer.m_warp_id].inc_inst_in_pipeline();
>                m_stats->m_num_decoded_insn[m_sid]++;
>                if(pI2->oprnd_type==INT_OP){
>                    m_stats->m_num_INTdecoded_insn[m_sid]++;
>                }else if(pI2->oprnd_type==FP_OP) {
>             	   m_stats->m_num_FPdecoded_insn[m_sid]++;
>                }
>            }
590c592
<       }
---
>         m_inst_fetch_buffer.m_valid = false;
592,593d593
<     m_inst_fetch_buffer.m_valid = false;
<   }
598,621c598,621
<   if ( !m_inst_fetch_buffer.m_valid ) {
<     // find an active warp with space in instruction buffer that is not already waiting on a cache miss
<     // and get next 1-2 instructions from i-cache...
<     for ( unsigned i = 0; i < m_config->max_warps_per_shader; i++ ) {
<       unsigned warp_id = (m_last_warp_fetched + 1 + i) % m_config->max_warps_per_shader;
< 
<       // this code checks if this warp has finished executing and can be reclaimed
<       if ( m_warp[warp_id].hardware_done() && !m_scoreboard->pendingWrites(warp_id) && !m_warp[warp_id].done_exit() ) {
<         bool did_exit = false;
<         for ( unsigned t = 0; t < m_config->warp_size; t++) {
<           unsigned tid = warp_id * m_config->warp_size + t;
<           if ( m_threadState[tid].m_active == true ) {
<             m_threadState[tid].m_active = false;
<             unsigned cta_id = m_warp[warp_id].get_cta_id();
<             register_cta_thread_exit(cta_id);
<             m_not_completed -= 1;
<             m_active_threads.reset(tid);
<             assert( m_thread[tid] != NULL );
<             did_exit = true;
<           }
<         }
<         if ( did_exit )
<           m_warp[warp_id].set_done_exit();
<       }
---
>     if( !m_inst_fetch_buffer.m_valid ) {
>         // find an active warp with space in instruction buffer that is not already waiting on a cache miss
>         // and get next 1-2 instructions from i-cache...
>         for( unsigned i=0; i < m_config->max_warps_per_shader; i++ ) {
>             unsigned warp_id = (m_last_warp_fetched+1+i) % m_config->max_warps_per_shader;
> 
>             // this code checks if this warp has finished executing and can be reclaimed
>             if( m_warp[warp_id].hardware_done() && !m_scoreboard->pendingWrites(warp_id) && !m_warp[warp_id].done_exit() ) {
>                 bool did_exit=false;
>                 for( unsigned t=0; t<m_config->warp_size;t++) {
>                     unsigned tid=warp_id*m_config->warp_size+t;
>                     if( m_threadState[tid].m_active == true ) {
>                         m_threadState[tid].m_active = false; 
>                         unsigned cta_id = m_warp[warp_id].get_cta_id();
>                         register_cta_thread_exit(cta_id);
>                         m_not_completed -= 1;
>                         m_active_threads.reset(tid);
>                         assert( m_thread[tid]!= NULL );
>                         did_exit=true;
>                     }
>                 }
>                 if( did_exit ) 
>                     m_warp[warp_id].set_done_exit();
>             }
623,656c623,659
<       // this code fetches instructions from the i-cache or generates memory requests
<       if ( !m_warp[warp_id].functional_done() && !m_warp[warp_id].imiss_pending() && m_warp[warp_id].ibuffer_empty() ) {
<         address_type pc  = m_warp[warp_id].get_pc();
<         address_type ppc = pc + PROGRAM_MEM_START;
<         unsigned nbytes = 16;
<         unsigned offset_in_block = pc & (m_config->m_L1I_config.get_line_sz() - 1);
<         if ( (offset_in_block + nbytes) > m_config->m_L1I_config.get_line_sz() )
<           nbytes = (m_config->m_L1I_config.get_line_sz() - offset_in_block);
< 
<         // TODO: replace with use of allocator
<         // mem_fetch *mf = m_mem_fetch_allocator->alloc()
<         mem_access_t acc(INST_ACC_R, ppc, nbytes, false);
<         mem_fetch *mf = new mem_fetch(acc,
<                                       NULL/*we don't have an instruction yet*/,
<                                       READ_PACKET_SIZE,
<                                       warp_id,
<                                       m_sid,
<                                       m_tpc,
<                                       m_memory_config );
<         std::list<cache_event> events;
<         enum cache_request_status status = m_L1I->access( (new_addr_type)ppc, mf, gpu_sim_cycle + gpu_tot_sim_cycle, events);
<         if ( status == MISS ) {
<           m_last_warp_fetched = warp_id;
<           m_warp[warp_id].set_imiss_pending();
<           m_warp[warp_id].set_last_fetch(gpu_sim_cycle);
<         } else if ( status == HIT ) {
<           m_last_warp_fetched = warp_id;
<           m_inst_fetch_buffer = ifetch_buffer_t(pc, nbytes, warp_id);
<           m_warp[warp_id].set_last_fetch(gpu_sim_cycle);
<           delete mf;
<         } else {
<           m_last_warp_fetched = warp_id;
<           assert( status == RESERVATION_FAIL );
<           delete mf;
---
>             // this code fetches instructions from the i-cache or generates memory requests
>             if( !m_warp[warp_id].functional_done() && !m_warp[warp_id].imiss_pending() && m_warp[warp_id].ibuffer_empty() ) {
>                 address_type pc  = m_warp[warp_id].get_pc();
>                 address_type ppc = pc + PROGRAM_MEM_START;
>                 unsigned nbytes=16; 
>                 unsigned offset_in_block = pc & (m_config->m_L1I_config.get_line_sz()-1);
>                 if( (offset_in_block+nbytes) > m_config->m_L1I_config.get_line_sz() )
>                     nbytes = (m_config->m_L1I_config.get_line_sz()-offset_in_block);
> 
>                 // TODO: replace with use of allocator
>                 // mem_fetch *mf = m_mem_fetch_allocator->alloc()
>                 mem_access_t acc(INST_ACC_R,ppc,nbytes,false);
>                 mem_fetch *mf = new mem_fetch(acc,
>                                               NULL/*we don't have an instruction yet*/,
>                                               READ_PACKET_SIZE,
>                                               warp_id,
>                                               m_sid,
>                                               m_tpc,
>                                               m_memory_config );
>                 std::list<cache_event> events;
>                 enum cache_request_status status = m_L1I->access( (new_addr_type)ppc, mf, gpu_sim_cycle+gpu_tot_sim_cycle,events);
>                 if( status == MISS ) {
>                     m_last_warp_fetched=warp_id;
>                     m_warp[warp_id].set_imiss_pending();
>                     m_warp[warp_id].set_last_fetch(gpu_sim_cycle);
>                 } else if( status == HIT ) {
>                     m_last_warp_fetched=warp_id;
>                     m_inst_fetch_buffer = ifetch_buffer_t(pc,nbytes,warp_id);
>                     m_warp[warp_id].set_last_fetch(gpu_sim_cycle);
>                     delete mf;
>                 } else {
>                     m_last_warp_fetched=warp_id;
>                     assert( status == RESERVATION_FAIL );
>                     delete mf;
>                 }
>                 break;
>             }
658,659d660
<         break;
<       }
661d661
<   }
663c663
<   m_L1I->cycle();
---
>     m_L1I->cycle();
665,669c665,669
<   if ( m_L1I->access_ready() ) {
<     mem_fetch *mf = m_L1I->next_access();
<     m_warp[mf->get_wid()].clear_imiss_pending();
<     delete mf;
<   }
---
>     if( m_L1I->access_ready() ) {
>         mem_fetch *mf = m_L1I->next_access();
>         m_warp[mf->get_wid()].clear_imiss_pending();
>         delete mf;
>     }
674,676c674,676
<   execute_warp_inst_t(inst);
<   if ( inst.is_load() || inst.is_store() )
<     inst.generate_mem_accesses();
---
>     execute_warp_inst_t(inst);
>     if( inst.is_load() || inst.is_store() )
>         inst.generate_mem_accesses();
681,682c681,692
<   warp_inst_t** pipe_reg = pipe_reg_set.get_free();
<   assert(pipe_reg);
---
>     warp_inst_t** pipe_reg = pipe_reg_set.get_free();
>     assert(pipe_reg);
>     
>     m_warp[warp_id].ibuffer_free();
>     assert(next_inst->valid());
>     **pipe_reg = *next_inst; // static instruction information
>     (*pipe_reg)->issue( active_mask, warp_id, gpu_tot_sim_cycle + gpu_sim_cycle, m_warp[warp_id].get_dynamic_warp_id() ); // dynamic instruction information
>     m_stats->shader_cycle_distro[2+(*pipe_reg)->active_count()]++;
>     func_exec_inst( **pipe_reg );
>     if( next_inst->op == BARRIER_OP ){
>     	m_warp[warp_id].store_info_of_last_inst_at_barrier(*pipe_reg);
>         m_barriers.warp_reaches_barrier(m_warp[warp_id].get_cta_id(),warp_id,const_cast<warp_inst_t*> (next_inst));
684,696c694,696
<   m_warp[warp_id].ibuffer_free();
<   assert(next_inst->valid());
<   **pipe_reg = *next_inst; // static instruction information
<   (*pipe_reg)->issue( active_mask, warp_id, gpu_tot_sim_cycle + gpu_sim_cycle, m_warp[warp_id].get_dynamic_warp_id() ); // dynamic instruction information
<   m_stats->shader_cycle_distro[2 + (*pipe_reg)->active_count()]++;
<   func_exec_inst( **pipe_reg );
<   if ( next_inst->op == BARRIER_OP ) {
<     m_warp[warp_id].store_info_of_last_inst_at_barrier(*pipe_reg);
<     m_barriers.warp_reaches_barrier(m_warp[warp_id].get_cta_id(), warp_id, const_cast<warp_inst_t*> (next_inst));
< 
<   } else if ( next_inst->op == MEMORY_BARRIER_OP ) {
<     m_warp[warp_id].set_membar();
<   }
---
>     }else if( next_inst->op == MEMORY_BARRIER_OP ){
>         m_warp[warp_id].set_membar();
>     }
698,700c698,700
<   updateSIMTStack(warp_id, *pipe_reg);
<   m_scoreboard->reserveRegisters(*pipe_reg);
<   m_warp[warp_id].set_next_pc(next_inst->pc + next_inst->isize);
---
>     updateSIMTStack(warp_id,*pipe_reg);
>     m_scoreboard->reserveRegisters(*pipe_reg);
>     m_warp[warp_id].set_next_pc(next_inst->pc + next_inst->isize);
703,707c703,707
< void shader_core_ctx::issue() {
<   //really is issue;
<   for (unsigned i = 0; i < schedulers.size(); i++) {
<     schedulers[i]->cycle();
<   }
---
> void shader_core_ctx::issue(){
>     //really is issue;
>     for (unsigned i = 0; i < schedulers.size(); i++) {
>         schedulers[i]->cycle();
>     }
710,711c710,711
< shd_warp_t& scheduler_unit::warp(int i) {
<   return (*m_warp)[i];
---
> shd_warp_t& scheduler_unit::warp(int i){
>     return (*m_warp)[i];
740,750c740,752
<   assert( num_warps_to_add <= input_list.size() );
<   result_list.clear();
<   typename std::vector< T >::const_iterator iter
<     = ( last_issued_from_input ==  input_list.end() ) ? input_list.begin()
<       : last_issued_from_input + 1;
< 
<   for ( unsigned count = 0;
<         count < num_warps_to_add;
<         ++iter, ++count) {
<     if ( iter ==  input_list.end() ) {
<       iter = input_list.begin();
---
>     assert( num_warps_to_add <= input_list.size() );
>     result_list.clear();
>     typename std::vector< T >::const_iterator iter
>         = ( last_issued_from_input ==  input_list.end() ) ? input_list.begin()
>                                                           : last_issued_from_input + 1;
> 
>     for ( unsigned count = 0;
>           count < num_warps_to_add;
>           ++iter, ++count) {
>         if ( iter ==  input_list.end() ) {
>             iter = input_list.begin();
>         }
>         result_list.push_back( *iter );
752,753d753
<     result_list.push_back( *iter );
<   }
775,788c775,798
<   assert( num_warps_to_add <= input_list.size() );
<   result_list.clear();
<   typename std::vector< T > temp = input_list;
< 
<   if ( ORDERING_GREEDY_THEN_PRIORITY_FUNC == ordering ) {
<     T greedy_value = *last_issued_from_input;
<     result_list.push_back( greedy_value );
< 
<     std::sort( temp.begin(), temp.end(), priority_func );
<     typename std::vector< T >::iterator iter = temp.begin();
<     for ( unsigned count = 0; count < num_warps_to_add; ++count, ++iter ) {
<       if ( *iter != greedy_value ) {
<         result_list.push_back( *iter );
<       }
---
>     assert( num_warps_to_add <= input_list.size() );
>     result_list.clear();
>     typename std::vector< T > temp = input_list;
> 
>     if ( ORDERING_GREEDY_THEN_PRIORITY_FUNC == ordering ) {
>         T greedy_value = *last_issued_from_input;
>         result_list.push_back( greedy_value );
> 
>         std::sort( temp.begin(), temp.end(), priority_func );
>         typename std::vector< T >::iterator iter = temp.begin();
>         for ( unsigned count = 0; count < num_warps_to_add; ++count, ++iter ) {
>             if ( *iter != greedy_value ) {
>                 result_list.push_back( *iter );
>             }
>         }
>     } else if ( ORDERED_PRIORITY_FUNC_ONLY == ordering ) {
>         std::sort( temp.begin(), temp.end(), priority_func );
>         typename std::vector< T >::iterator iter = temp.begin();
>         for ( unsigned count = 0; count < num_warps_to_add; ++count, ++iter ) {
>             result_list.push_back( *iter );
>         }
>     } else {
>         fprintf( stderr, "Unknown ordering - %d\n", ordering );
>         abort();
790,799d799
<   } else if ( ORDERED_PRIORITY_FUNC_ONLY == ordering ) {
<     std::sort( temp.begin(), temp.end(), priority_func );
<     typename std::vector< T >::iterator iter = temp.begin();
<     for ( unsigned count = 0; count < num_warps_to_add; ++count, ++iter ) {
<       result_list.push_back( *iter );
<     }
<   } else {
<     fprintf( stderr, "Unknown ordering - %d\n", ordering );
<     abort();
<   }
804,876c804,815
<   SCHED_DPRINTF( "scheduler_unit::cycle()\n" );
<   bool valid_inst = false;  // there was one warp with a valid instruction to issue (didn't require flush due to control hazard)
<   bool ready_inst = false;  // of the valid instructions, there was one not waiting for pending register writes
<   bool issued_inst = false; // of these we issued one
< 
<   order_warps();
<   for ( std::vector< shd_warp_t* >::const_iterator iter = m_next_cycle_prioritized_warps.begin();
<         iter != m_next_cycle_prioritized_warps.end();
<         iter++ ) {
<     // Don't consider warps that are not yet valid
<     if ( (*iter) == NULL || (*iter)->done_exit() ) {
<       continue;
<     }
<     SCHED_DPRINTF( "Testing (warp_id %u, dynamic_warp_id %u)\n",
<                    (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id() );
<     unsigned warp_id = (*iter)->get_warp_id();
<     unsigned checked = 0;
<     unsigned issued = 0;
<     unsigned max_issue = m_shader->m_config->gpgpu_max_insn_issue_per_warp;
<     while ( !warp(warp_id).waiting() && !warp(warp_id).ibuffer_empty() && (checked < max_issue) && (checked <= issued) && (issued < max_issue) ) {
<       const warp_inst_t *pI = warp(warp_id).ibuffer_next_inst();
<       bool valid = warp(warp_id).ibuffer_next_valid();
<       bool warp_inst_issued = false;
<       unsigned pc, rpc;
<       m_simt_stack[warp_id]->get_pdom_stack_top_info(&pc, &rpc);
<       SCHED_DPRINTF( "Warp (warp_id %u, dynamic_warp_id %u) has valid instruction (%s)\n",
<                      (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id(),
<                      ptx_get_insn_str( pc).c_str() );
<       if ( pI ) {
<         assert(valid);
<         if ( pc != pI->pc ) {
<           SCHED_DPRINTF( "Warp (warp_id %u, dynamic_warp_id %u) control hazard instruction flush\n",
<                          (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id() );
<           // control hazard
<           warp(warp_id).set_next_pc(pc);
<           warp(warp_id).ibuffer_flush();
<         } else {
<           valid_inst = true;
<           if ( !m_scoreboard->checkCollision(warp_id, pI) ) {
<             SCHED_DPRINTF( "Warp (warp_id %u, dynamic_warp_id %u) passes scoreboard\n",
<                            (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id() );
<             ready_inst = true;
<             const active_mask_t &active_mask = m_simt_stack[warp_id]->get_active_mask();
<             assert( warp(warp_id).inst_in_pipeline() );
<             if ( (pI->op == LOAD_OP) || (pI->op == STORE_OP) || (pI->op == MEMORY_BARRIER_OP) ) {
<               if ( m_mem_out->has_free() ) {
<                 m_shader->issue_warp(*m_mem_out, pI, active_mask, warp_id);
<                 issued++;
<                 issued_inst = true;
<                 warp_inst_issued = true;
<               }
<             } else {
<               bool sp_pipe_avail = m_sp_out->has_free();
<               bool sfu_pipe_avail = m_sfu_out->has_free();
<               if ( sp_pipe_avail && (pI->op != SFU_OP) ) {
<                 // always prefer SP pipe for operations that can use both SP and SFU pipelines
<                 m_shader->issue_warp(*m_sp_out, pI, active_mask, warp_id);
<                 issued++;
<                 issued_inst = true;
<                 warp_inst_issued = true;
<               } else if ( (pI->op == SFU_OP) || (pI->op == ALU_SFU_OP) ) {
<                 if ( sfu_pipe_avail ) {
<                   m_shader->issue_warp(*m_sfu_out, pI, active_mask, warp_id);
<                   issued++;
<                   issued_inst = true;
<                   warp_inst_issued = true;
<                 }
<               }
<             }
<           } else {
<             SCHED_DPRINTF( "Warp (warp_id %u, dynamic_warp_id %u) fails scoreboard\n",
<                            (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id() );
<           }
---
>     SCHED_DPRINTF( "scheduler_unit::cycle()\n" );
>     bool valid_inst = false;  // there was one warp with a valid instruction to issue (didn't require flush due to control hazard)
>     bool ready_inst = false;  // of the valid instructions, there was one not waiting for pending register writes
>     bool issued_inst = false; // of these we issued one
> 
>     order_warps();
>     for ( std::vector< shd_warp_t* >::const_iterator iter = m_next_cycle_prioritized_warps.begin();
>           iter != m_next_cycle_prioritized_warps.end();
>           iter++ ) {
>         // Don't consider warps that are not yet valid
>         if ( (*iter) == NULL || (*iter)->done_exit() ) {
>             continue;
878,880c817
<       } else if ( valid ) {
<         // this case can happen after a return instruction in diverged warp
<         SCHED_DPRINTF( "Warp (warp_id %u, dynamic_warp_id %u) return from diverged warp flush\n",
---
>         SCHED_DPRINTF( "Testing (warp_id %u, dynamic_warp_id %u)\n",
882,904c819,891
<         warp(warp_id).set_next_pc(pc);
<         warp(warp_id).ibuffer_flush();
<       }
<       if (warp_inst_issued) {
<         SCHED_DPRINTF( "Warp (warp_id %u, dynamic_warp_id %u) issued %u instructions\n",
<                        (*iter)->get_warp_id(),
<                        (*iter)->get_dynamic_warp_id(),
<                        issued );
<         do_on_warp_issued( warp_id, issued, iter );
<       }
<       checked++;
<     }
<     if ( issued ) {
<       // This might be a bit inefficient, but we need to maintain
<       // two ordered list for proper scheduler execution.
<       // We could remove the need for this loop by associating a
<       // supervised_is index with each entry in the m_next_cycle_prioritized_warps
<       // vector. For now, just run through until you find the right warp_id
<       for ( std::vector< shd_warp_t* >::const_iterator supervised_iter = m_supervised_warps.begin();
<             supervised_iter != m_supervised_warps.end();
<             ++supervised_iter ) {
<         if ( *iter == *supervised_iter ) {
<           m_last_supervised_issued = supervised_iter;
---
>         unsigned warp_id = (*iter)->get_warp_id();
>         unsigned checked=0;
>         unsigned issued=0;
>         unsigned max_issue = m_shader->m_config->gpgpu_max_insn_issue_per_warp;
>         while( !warp(warp_id).waiting() && !warp(warp_id).ibuffer_empty() && (checked < max_issue) && (checked <= issued) && (issued < max_issue) ) {
>             const warp_inst_t *pI = warp(warp_id).ibuffer_next_inst();
>             bool valid = warp(warp_id).ibuffer_next_valid();
>             bool warp_inst_issued = false;
>             unsigned pc,rpc;
>             m_simt_stack[warp_id]->get_pdom_stack_top_info(&pc,&rpc);
>             SCHED_DPRINTF( "Warp (warp_id %u, dynamic_warp_id %u) has valid instruction (%s)\n",
>                            (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id(),
>                            ptx_get_insn_str( pc).c_str() );
>             if( pI ) {
>                 assert(valid);
>                 if( pc != pI->pc ) {
>                     SCHED_DPRINTF( "Warp (warp_id %u, dynamic_warp_id %u) control hazard instruction flush\n",
>                                    (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id() );
>                     // control hazard
>                     warp(warp_id).set_next_pc(pc);
>                     warp(warp_id).ibuffer_flush();
>                 } else {
>                     valid_inst = true;
>                     if ( !m_scoreboard->checkCollision(warp_id, pI) ) {
>                         SCHED_DPRINTF( "Warp (warp_id %u, dynamic_warp_id %u) passes scoreboard\n",
>                                        (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id() );
>                         ready_inst = true;
>                         const active_mask_t &active_mask = m_simt_stack[warp_id]->get_active_mask();
>                         assert( warp(warp_id).inst_in_pipeline() );
>                         if ( (pI->op == LOAD_OP) || (pI->op == STORE_OP) || (pI->op == MEMORY_BARRIER_OP) ) {
>                             if( m_mem_out->has_free() ) {
>                                 m_shader->issue_warp(*m_mem_out,pI,active_mask,warp_id);
>                                 issued++;
>                                 issued_inst=true;
>                                 warp_inst_issued = true;
>                             }
>                         } else {
>                             bool sp_pipe_avail = m_sp_out->has_free();
>                             bool sfu_pipe_avail = m_sfu_out->has_free();
>                             if( sp_pipe_avail && (pI->op != SFU_OP) ) {
>                                 // always prefer SP pipe for operations that can use both SP and SFU pipelines
>                                 m_shader->issue_warp(*m_sp_out,pI,active_mask,warp_id);
>                                 issued++;
>                                 issued_inst=true;
>                                 warp_inst_issued = true;
>                             } else if ( (pI->op == SFU_OP) || (pI->op == ALU_SFU_OP) ) {
>                                 if( sfu_pipe_avail ) {
>                                     m_shader->issue_warp(*m_sfu_out,pI,active_mask,warp_id);
>                                     issued++;
>                                     issued_inst=true;
>                                     warp_inst_issued = true;
>                                 }
>                             }                         }
>                     } else {
>                         SCHED_DPRINTF( "Warp (warp_id %u, dynamic_warp_id %u) fails scoreboard\n",
>                                        (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id() );
>                     }
>                 }
>             } else if( valid ) {
>                // this case can happen after a return instruction in diverged warp
>                SCHED_DPRINTF( "Warp (warp_id %u, dynamic_warp_id %u) return from diverged warp flush\n",
>                               (*iter)->get_warp_id(), (*iter)->get_dynamic_warp_id() );
>                warp(warp_id).set_next_pc(pc);
>                warp(warp_id).ibuffer_flush();
>             }
>             if(warp_inst_issued) {
>                 SCHED_DPRINTF( "Warp (warp_id %u, dynamic_warp_id %u) issued %u instructions\n",
>                                (*iter)->get_warp_id(),
>                                (*iter)->get_dynamic_warp_id(),
>                                issued );
>                 do_on_warp_issued( warp_id, issued, iter );
>             }
>             checked++;
906,907c893,907
<       }
<       break;
---
>         if ( issued ) {
>             // This might be a bit inefficient, but we need to maintain
>             // two ordered list for proper scheduler execution.
>             // We could remove the need for this loop by associating a
>             // supervised_is index with each entry in the m_next_cycle_prioritized_warps
>             // vector. For now, just run through until you find the right warp_id
>             for ( std::vector< shd_warp_t* >::const_iterator supervised_iter = m_supervised_warps.begin();
>                   supervised_iter != m_supervised_warps.end();
>                   ++supervised_iter ) {
>                 if ( *iter == *supervised_iter ) {
>                     m_last_supervised_issued = supervised_iter;
>                 }
>             }
>             break;
>         } 
909d908
<   }
911,917c910,916
<   // issue stall statistics:
<   if ( !valid_inst )
<     m_stats->shader_cycle_distro[0]++; // idle or control hazard
<   else if ( !ready_inst )
<     m_stats->shader_cycle_distro[1]++; // waiting for RAW hazards (possibly due to memory)
<   else if ( !issued_inst )
<     m_stats->shader_cycle_distro[2]++; // pipeline stalled
---
>     // issue stall statistics:
>     if( !valid_inst ) 
>         m_stats->shader_cycle_distro[0]++; // idle or control hazard
>     else if( !ready_inst ) 
>         m_stats->shader_cycle_distro[1]++; // waiting for RAW hazards (possibly due to memory) 
>     else if( !issued_inst ) 
>         m_stats->shader_cycle_distro[2]++; // pipeline stalled
924,928c923,927
<   m_stats->event_warp_issued( m_shader->get_sid(),
<                               warp_id,
<                               num_issued,
<                               warp(warp_id).get_dynamic_warp_id() );
<   warp(warp_id).ibuffer_step();
---
>     m_stats->event_warp_issued( m_shader->get_sid(),
>                                 warp_id,
>                                 num_issued,
>                                 warp(warp_id).get_dynamic_warp_id() );
>     warp(warp_id).ibuffer_step();
933,937c932,939
<   if (rhs && lhs) {
<     if ( lhs->done_exit() || lhs->waiting() ) {
<       return false;
<     } else if ( rhs->done_exit() || rhs->waiting() ) {
<       return true;
---
>     if (rhs && lhs) {
>         if ( lhs->done_exit() || lhs->waiting() ) {
>             return false;
>         } else if ( rhs->done_exit() || rhs->waiting() ) {
>             return true;
>         } else {
>             return lhs->get_dynamic_warp_id() < rhs->get_dynamic_warp_id();
>         }
939c941
<       return lhs->get_dynamic_warp_id() < rhs->get_dynamic_warp_id();
---
>         return lhs < rhs;
941,943d942
<   } else {
<     return lhs < rhs;
<   }
948,951c947,950
<   order_lrr( m_next_cycle_prioritized_warps,
<              m_supervised_warps,
<              m_last_supervised_issued,
<              m_supervised_warps.size() );
---
>     order_lrr( m_next_cycle_prioritized_warps,
>                m_supervised_warps,
>                m_last_supervised_issued,
>                m_supervised_warps.size() );
956,961c955,960
<   order_by_priority( m_next_cycle_prioritized_warps,
<                      m_supervised_warps,
<                      m_last_supervised_issued,
<                      m_supervised_warps.size(),
<                      ORDERING_GREEDY_THEN_PRIORITY_FUNC,
<                      scheduler_unit::sort_warps_by_oldest_dynamic_id );
---
>     order_by_priority( m_next_cycle_prioritized_warps,
>                        m_supervised_warps,
>                        m_last_supervised_issued,
>                        m_supervised_warps.size(),
>                        ORDERING_GREEDY_THEN_PRIORITY_FUNC,
>                        scheduler_unit::sort_warps_by_oldest_dynamic_id );
966,967c965,966
<     unsigned num_issued,
<     const std::vector< shd_warp_t* >::const_iterator& prioritized_iter )
---
>                                                unsigned num_issued,
>                                                const std::vector< shd_warp_t* >::const_iterator& prioritized_iter )
969,982c968,981
<   scheduler_unit::do_on_warp_issued( warp_id, num_issued, prioritized_iter );
<   if ( SCHEDULER_PRIORITIZATION_LRR == m_inner_level_prioritization ) {
<     std::vector< shd_warp_t* > new_active;
<     order_lrr( new_active,
<                m_next_cycle_prioritized_warps,
<                prioritized_iter,
<                m_next_cycle_prioritized_warps.size() );
<     m_next_cycle_prioritized_warps = new_active;
<   } else {
<     fprintf( stderr,
<              "Unimplemented m_inner_level_prioritization: %d\n",
<              m_inner_level_prioritization );
<     abort();
<   }
---
>     scheduler_unit::do_on_warp_issued( warp_id, num_issued, prioritized_iter );
>     if ( SCHEDULER_PRIORITIZATION_LRR == m_inner_level_prioritization ) {
>         std::vector< shd_warp_t* > new_active; 
>         order_lrr( new_active,
>                    m_next_cycle_prioritized_warps,
>                    prioritized_iter,
>                    m_next_cycle_prioritized_warps.size() );
>         m_next_cycle_prioritized_warps = new_active;
>     } else {
>         fprintf( stderr,
>                  "Unimplemented m_inner_level_prioritization: %d\n",
>                  m_inner_level_prioritization );
>         abort();
>     }
987,997c986,1008
<   //Move waiting warps to m_pending_warps
<   unsigned num_demoted = 0;
<   for (   std::vector< shd_warp_t* >::iterator iter = m_next_cycle_prioritized_warps.begin();
<           iter != m_next_cycle_prioritized_warps.end(); ) {
<     bool waiting = (*iter)->waiting();
<     for (int i = 0; i < 4; i++) {
<       const warp_inst_t* inst = (*iter)->ibuffer_next_inst();
<       //Is the instruction waiting on a long operation?
<       if ( inst && inst->in[i] > 0 && this->m_scoreboard->islongop((*iter)->get_warp_id(), inst->in[i])) {
<         waiting = true;
<       }
---
>     //Move waiting warps to m_pending_warps
>     unsigned num_demoted = 0;
>     for (   std::vector< shd_warp_t* >::iterator iter = m_next_cycle_prioritized_warps.begin();
>             iter != m_next_cycle_prioritized_warps.end(); ) {
>         bool waiting = (*iter)->waiting();
>         for (int i=0; i<4; i++){
>             const warp_inst_t* inst = (*iter)->ibuffer_next_inst();
>             //Is the instruction waiting on a long operation?
>             if ( inst && inst->in[i] > 0 && this->m_scoreboard->islongop((*iter)->get_warp_id(), inst->in[i])){
>                 waiting = true;
>             }
>         }
> 
>         if( waiting ) {
>             m_pending_warps.push_back(*iter);
>             iter = m_next_cycle_prioritized_warps.erase(iter);
>             SCHED_DPRINTF( "DEMOTED warp_id=%d, dynamic_warp_id=%d\n",
>                            (*iter)->get_warp_id(),
>                            (*iter)->get_dynamic_warp_id() );
>             ++num_demoted;
>         } else {
>             ++iter;
>         }
1000,1006c1011,1021
<     if ( waiting ) {
<       m_pending_warps.push_back(*iter);
<       iter = m_next_cycle_prioritized_warps.erase(iter);
<       SCHED_DPRINTF( "DEMOTED warp_id=%d, dynamic_warp_id=%d\n",
<                      (*iter)->get_warp_id(),
<                      (*iter)->get_dynamic_warp_id() );
<       ++num_demoted;
---
>     //If there is space in m_next_cycle_prioritized_warps, promote the next m_pending_warps
>     unsigned num_promoted = 0;
>     if ( SCHEDULER_PRIORITIZATION_SRR == m_outer_level_prioritization ) {
>         while ( m_next_cycle_prioritized_warps.size() < m_max_active_warps ) {
>             m_next_cycle_prioritized_warps.push_back(m_pending_warps.front());
>             m_pending_warps.pop_front();
>             SCHED_DPRINTF( "PROMOTED warp_id=%d, dynamic_warp_id=%d\n",
>                            (m_next_cycle_prioritized_warps.back())->get_warp_id(),
>                            (m_next_cycle_prioritized_warps.back())->get_dynamic_warp_id() );
>             ++num_promoted;
>         }
1008c1023,1026
<       ++iter;
---
>         fprintf( stderr,
>                  "Unimplemented m_outer_level_prioritization: %d\n",
>                  m_outer_level_prioritization );
>         abort();
1010,1029c1028
<   }
< 
<   //If there is space in m_next_cycle_prioritized_warps, promote the next m_pending_warps
<   unsigned num_promoted = 0;
<   if ( SCHEDULER_PRIORITIZATION_SRR == m_outer_level_prioritization ) {
<     while ( m_next_cycle_prioritized_warps.size() < m_max_active_warps ) {
<       m_next_cycle_prioritized_warps.push_back(m_pending_warps.front());
<       m_pending_warps.pop_front();
<       SCHED_DPRINTF( "PROMOTED warp_id=%d, dynamic_warp_id=%d\n",
<                      (m_next_cycle_prioritized_warps.back())->get_warp_id(),
<                      (m_next_cycle_prioritized_warps.back())->get_dynamic_warp_id() );
<       ++num_promoted;
<     }
<   } else {
<     fprintf( stderr,
<              "Unimplemented m_outer_level_prioritization: %d\n",
<              m_outer_level_prioritization );
<     abort();
<   }
<   assert( num_promoted == num_demoted );
---
>     assert( num_promoted == num_demoted );
1040c1039
<   : scheduler_unit ( stats, shader, scoreboard, simt, warp, sp_out, sfu_out, mem_out, id )
---
>     : scheduler_unit ( stats, shader, scoreboard, simt, warp, sp_out, sfu_out, mem_out, id )
1042,1052c1041,1051
<   unsigned m_prioritization_readin;
<   int ret = sscanf( config_string,
<                     "warp_limiting:%d:%d",
<                     &m_prioritization_readin,
<                     &m_num_warps_to_limit
<                   );
<   assert( 2 == ret );
<   m_prioritization = (scheduler_prioritization_type)m_prioritization_readin;
<   // Currently only GTO is implemented
<   assert( m_prioritization == SCHEDULER_PRIORITIZATION_GTO );
<   assert( m_num_warps_to_limit <= shader->get_config()->max_warps_per_shader );
---
>     unsigned m_prioritization_readin;
>     int ret = sscanf( config_string,
>                       "warp_limiting:%d:%d",
>                       &m_prioritization_readin,
>                       &m_num_warps_to_limit
>                      );
>     assert( 2 == ret );
>     m_prioritization = (scheduler_prioritization_type)m_prioritization_readin;
>     // Currently only GTO is implemented
>     assert( m_prioritization == SCHEDULER_PRIORITIZATION_GTO );
>     assert( m_num_warps_to_limit <= shader->get_config()->max_warps_per_shader );
1057,1067c1056,1066
<   if ( SCHEDULER_PRIORITIZATION_GTO == m_prioritization ) {
<     order_by_priority( m_next_cycle_prioritized_warps,
<                        m_supervised_warps,
<                        m_last_supervised_issued,
<                        MIN( m_num_warps_to_limit, m_supervised_warps.size() ),
<                        ORDERING_GREEDY_THEN_PRIORITY_FUNC,
<                        scheduler_unit::sort_warps_by_oldest_dynamic_id );
<   } else {
<     fprintf(stderr, "swl_scheduler m_prioritization = %d\n", m_prioritization);
<     abort();
<   }
---
>     if ( SCHEDULER_PRIORITIZATION_GTO == m_prioritization ) {
>         order_by_priority( m_next_cycle_prioritized_warps,
>                            m_supervised_warps,
>                            m_last_supervised_issued,
>                            MIN( m_num_warps_to_limit, m_supervised_warps.size() ),
>                            ORDERING_GREEDY_THEN_PRIORITY_FUNC,
>                            scheduler_unit::sort_warps_by_oldest_dynamic_id );
>     } else {
>         fprintf(stderr, "swl_scheduler m_prioritization = %d\n", m_prioritization);
>         abort();
>     }
1076c1075
<   return  (addr >> segment_size_lg2bytes);
---
>    return  (addr >> segment_size_lg2bytes);
1082,1083c1081,1082
<   // During functional execution, each thread sees its own memory space for local memory, but these
<   // need to be mapped to a shared address space for timing simulation.  We do that mapping here.
---
>    // During functional execution, each thread sees its own memory space for local memory, but these
>    // need to be mapped to a shared address space for timing simulation.  We do that mapping here.
1085,1109c1084,1108
<   address_type thread_base = 0;
<   unsigned max_concurrent_threads = 0;
<   if (m_config->gpgpu_local_mem_map) {
<     // Dnew = D*N + T%nTpC + nTpC*C
<     // N = nTpC*nCpS*nS (max concurent threads)
<     // C = nS*K + S (hw cta number per gpu)
<     // K = T/nTpC   (hw cta number per core)
<     // D = data index
<     // T = thread
<     // nTpC = number of threads per CTA
<     // nCpS = number of CTA per shader
<     //
<     // for a given local memory address threads in a CTA map to contiguous addresses,
<     // then distribute across memory space by CTAs from successive shader cores first,
<     // then by successive CTA in same shader core
<     thread_base = 4 * (kernel_padded_threads_per_cta * (m_sid + num_shader * (tid / kernel_padded_threads_per_cta))
<                        + tid % kernel_padded_threads_per_cta);
<     max_concurrent_threads = kernel_padded_threads_per_cta * kernel_max_cta_per_shader * num_shader;
<   } else {
<     // legacy mapping that maps the same address in the local memory space of all threads
<     // to a single contiguous address region
<     thread_base = 4 * (m_config->n_thread_per_shader * m_sid + tid);
<     max_concurrent_threads = num_shader * m_config->n_thread_per_shader;
<   }
<   assert( thread_base < 4/*word size*/*max_concurrent_threads );
---
>    address_type thread_base = 0;
>    unsigned max_concurrent_threads=0;
>    if (m_config->gpgpu_local_mem_map) {
>       // Dnew = D*N + T%nTpC + nTpC*C
>       // N = nTpC*nCpS*nS (max concurent threads)
>       // C = nS*K + S (hw cta number per gpu)
>       // K = T/nTpC   (hw cta number per core)
>       // D = data index
>       // T = thread
>       // nTpC = number of threads per CTA
>       // nCpS = number of CTA per shader
>       // 
>       // for a given local memory address threads in a CTA map to contiguous addresses,
>       // then distribute across memory space by CTAs from successive shader cores first, 
>       // then by successive CTA in same shader core
>       thread_base = 4*(kernel_padded_threads_per_cta * (m_sid + num_shader * (tid / kernel_padded_threads_per_cta))
>                        + tid % kernel_padded_threads_per_cta); 
>       max_concurrent_threads = kernel_padded_threads_per_cta * kernel_max_cta_per_shader * num_shader;
>    } else {
>       // legacy mapping that maps the same address in the local memory space of all threads 
>       // to a single contiguous address region 
>       thread_base = 4*(m_config->n_thread_per_shader * m_sid + tid);
>       max_concurrent_threads = num_shader * m_config->n_thread_per_shader;
>    }
>    assert( thread_base < 4/*word size*/*max_concurrent_threads );
1111,1136c1110,1135
<   // If requested datasize > 4B, split into multiple 4B accesses
<   // otherwise do one sub-4 byte memory access
<   unsigned num_accesses = 0;
< 
<   if (datasize >= 4) {
<     // >4B access, split into 4B chunks
<     assert(datasize % 4 == 0); // Must be a multiple of 4B
<     num_accesses = datasize / 4;
<     assert(num_accesses <= MAX_ACCESSES_PER_INSN_PER_THREAD); // max 32B
<     assert(localaddr % 4 == 0); // Address must be 4B aligned - required if accessing 4B per request, otherwise access will overflow into next thread's space
<     for (unsigned i = 0; i < num_accesses; i++) {
<       address_type local_word = localaddr / 4 + i;
<       address_type linear_address = local_word * max_concurrent_threads * 4 + thread_base + LOCAL_GENERIC_START;
<       translated_addrs[i] = linear_address;
<     }
<   } else {
<     // Sub-4B access, do only one access
<     assert(datasize > 0);
<     num_accesses = 1;
<     address_type local_word = localaddr / 4;
<     address_type local_word_offset = localaddr % 4;
<     assert( (localaddr + datasize - 1) / 4  == local_word ); // Make sure access doesn't overflow into next 4B chunk
<     address_type linear_address = local_word * max_concurrent_threads * 4 + local_word_offset + thread_base + LOCAL_GENERIC_START;
<     translated_addrs[0] = linear_address;
<   }
<   return num_accesses;
---
>    // If requested datasize > 4B, split into multiple 4B accesses
>    // otherwise do one sub-4 byte memory access
>    unsigned num_accesses = 0;
> 
>    if(datasize >= 4) {
>       // >4B access, split into 4B chunks
>       assert(datasize%4 == 0);   // Must be a multiple of 4B
>       num_accesses = datasize/4;
>       assert(num_accesses <= MAX_ACCESSES_PER_INSN_PER_THREAD); // max 32B
>       assert(localaddr%4 == 0); // Address must be 4B aligned - required if accessing 4B per request, otherwise access will overflow into next thread's space
>       for(unsigned i=0; i<num_accesses; i++) {
>           address_type local_word = localaddr/4 + i;
>           address_type linear_address = local_word*max_concurrent_threads*4 + thread_base + LOCAL_GENERIC_START;
>           translated_addrs[i] = linear_address;
>       }
>    } else {
>       // Sub-4B access, do only one access
>       assert(datasize > 0);
>       num_accesses = 1;
>       address_type local_word = localaddr/4;
>       address_type local_word_offset = localaddr%4;
>       assert( (localaddr+datasize-1)/4  == local_word ); // Make sure access doesn't overflow into next 4B chunk
>       address_type linear_address = local_word*max_concurrent_threads*4 + local_word_offset + thread_base + LOCAL_GENERIC_START;
>       translated_addrs[0] = linear_address;
>    }
>    return num_accesses;
1140,1144c1139,1143
< int shader_core_ctx::test_res_bus(int latency) {
<   for (unsigned i = 0; i < num_result_bus; i++) {
<     if (!m_result_bus[i]->test(latency)) {return i;}
<   }
<   return -1;
---
> int shader_core_ctx::test_res_bus(int latency){
> 	for(unsigned i=0; i<num_result_bus; i++){
> 		if(!m_result_bus[i]->test(latency)){return i;}
> 	}
> 	return -1;
1149,1171c1148,1171
<   for (unsigned i = 0; i < num_result_bus; i++) {
<     *(m_result_bus[i]) >>= 1;
<   }
<   for ( unsigned n = 0; n < m_num_function_units; n++ ) {
<     unsigned multiplier = m_fu[n]->clock_multiplier();
<     for ( unsigned c = 0; c < multiplier; c++ )
<       m_fu[n]->cycle();
<     m_fu[n]->active_lanes_in_pipeline();
<     enum pipeline_stage_name_t issue_port = m_issue_port[n];
<     register_set& issue_inst = m_pipeline_reg[ issue_port ];
<     warp_inst_t** ready_reg = issue_inst.get_ready();
<     if ( issue_inst.has_ready() && m_fu[n]->can_issue( **ready_reg ) ) {
<       bool schedule_wb_now = !m_fu[n]->stallable();
<       int resbus = -1;
<       if ( schedule_wb_now && (resbus = test_res_bus( (*ready_reg)->latency )) != -1 ) {
<         assert( (*ready_reg)->latency < MAX_ALU_LATENCY );
<         m_result_bus[resbus]->set( (*ready_reg)->latency );
<         m_fu[n]->issue( issue_inst );
<       } else if ( !schedule_wb_now ) {
<         m_fu[n]->issue( issue_inst );
<       } else {
<         // stall issue (cannot reserve result bus)
<       }
---
> 	for(unsigned i=0; i<num_result_bus; i++){
> 		*(m_result_bus[i]) >>=1;
> 	}
>     for( unsigned n=0; n < m_num_function_units; n++ ) {
>         unsigned multiplier = m_fu[n]->clock_multiplier();
>         for( unsigned c=0; c < multiplier; c++ ) 
>             m_fu[n]->cycle();
>         m_fu[n]->active_lanes_in_pipeline();
>         enum pipeline_stage_name_t issue_port = m_issue_port[n];
>         register_set& issue_inst = m_pipeline_reg[ issue_port ];
>         warp_inst_t** ready_reg = issue_inst.get_ready();
>         if( issue_inst.has_ready() && m_fu[n]->can_issue( **ready_reg ) ) {
>             bool schedule_wb_now = !m_fu[n]->stallable();
>             int resbus = -1;
>             if( schedule_wb_now && (resbus=test_res_bus( (*ready_reg)->latency ))!=-1 ) {
>                 assert( (*ready_reg)->latency < MAX_ALU_LATENCY );
>                 m_result_bus[resbus]->set( (*ready_reg)->latency );
>                 m_fu[n]->issue( issue_inst );
>             } else if( !schedule_wb_now ) {
>                 m_fu[n]->issue( issue_inst );
>             } else {
>                 // stall issue (cannot reserve result bus)
>             }
>         }
1173d1172
<   }
1177,1179c1176,1178
<   if ( m_L1D ) {
<     m_L1D->print( fp, dl1_accesses, dl1_misses );
<   }
---
>    if( m_L1D ) {
>        m_L1D->print( fp, dl1_accesses, dl1_misses );
>    }
1183,1189c1182,1188
<   // Adds stats to 'cs' from each cache
<   if (m_L1D)
<     cs += m_L1D->get_stats();
<   if (m_L1C)
<     cs += m_L1C->get_stats();
<   if (m_L1T)
<     cs += m_L1T->get_stats();
---
>     // Adds stats to 'cs' from each cache
>     if(m_L1D)
>         cs += m_L1D->get_stats();
>     if(m_L1C)
>         cs += m_L1C->get_stats();
>     if(m_L1T)
>         cs += m_L1T->get_stats();
1193,1203c1192,1202
< void ldst_unit::get_L1D_sub_stats(struct cache_sub_stats &css) const {
<   if (m_L1D)
<     m_L1D->get_sub_stats(css);
< }
< void ldst_unit::get_L1C_sub_stats(struct cache_sub_stats &css) const {
<   if (m_L1C)
<     m_L1C->get_sub_stats(css);
< }
< void ldst_unit::get_L1T_sub_stats(struct cache_sub_stats &css) const {
<   if (m_L1T)
<     m_L1T->get_sub_stats(css);
---
> void ldst_unit::get_L1D_sub_stats(struct cache_sub_stats &css) const{
>     if(m_L1D)
>         m_L1D->get_sub_stats(css);
> }
> void ldst_unit::get_L1C_sub_stats(struct cache_sub_stats &css) const{
>     if(m_L1C)
>         m_L1C->get_sub_stats(css);
> }
> void ldst_unit::get_L1T_sub_stats(struct cache_sub_stats &css) const{
>     if(m_L1T)
>         m_L1T->get_sub_stats(css);
1208,1217c1207,1216
< #if 0
<   printf("[warp_inst_complete] uid=%u core=%u warp=%u pc=%#x @ time=%llu issued@%llu\n",
<          inst.get_uid(), m_sid, inst.warp_id(), inst.pc, gpu_tot_sim_cycle + gpu_sim_cycle, inst.get_issue_cycle());
< #endif
<   if (inst.op_pipe == SP__OP)
<     m_stats->m_num_sp_committed[m_sid]++;
<   else if (inst.op_pipe == SFU__OP)
<     m_stats->m_num_sfu_committed[m_sid]++;
<   else if (inst.op_pipe == MEM__OP)
<     m_stats->m_num_mem_committed[m_sid]++;
---
>    #if 0
>       printf("[warp_inst_complete] uid=%u core=%u warp=%u pc=%#x @ time=%llu issued@%llu\n", 
>              inst.get_uid(), m_sid, inst.warp_id(), inst.pc, gpu_tot_sim_cycle + gpu_sim_cycle, inst.get_issue_cycle()); 
>    #endif
>   if(inst.op_pipe==SP__OP)
> 	  m_stats->m_num_sp_committed[m_sid]++;
>   else if(inst.op_pipe==SFU__OP)
> 	  m_stats->m_num_sfu_committed[m_sid]++;
>   else if(inst.op_pipe==MEM__OP)
> 	  m_stats->m_num_mem_committed[m_sid]++;
1219,1220c1218,1219
<   if (m_config->gpgpu_clock_gated_lanes == false)
<     m_stats->m_num_sim_insn[m_sid] += m_config->warp_size;
---
>   if(m_config->gpgpu_clock_gated_lanes==false)
> 	  m_stats->m_num_sim_insn[m_sid] += m_config->warp_size;
1222c1221
<     m_stats->m_num_sim_insn[m_sid] += inst.active_count();
---
> 	  m_stats->m_num_sim_insn[m_sid] += inst.active_count();
1232,1233c1231,1232
<   unsigned max_committed_thread_instructions = m_config->warp_size * (m_config->pipe_widths[EX_WB]); //from the functional units
<   m_stats->m_pipeline_duty_cycle[m_sid] = ((float)(m_stats->m_num_sim_insn[m_sid] - m_stats->m_last_num_sim_insn[m_sid])) / max_committed_thread_instructions;
---
> 	unsigned max_committed_thread_instructions=m_config->warp_size * (m_config->pipe_widths[EX_WB]); //from the functional units
> 	m_stats->m_pipeline_duty_cycle[m_sid]=((float)(m_stats->m_num_sim_insn[m_sid]-m_stats->m_last_num_sim_insn[m_sid]))/max_committed_thread_instructions;
1235,1236c1234,1235
<   m_stats->m_last_num_sim_insn[m_sid] = m_stats->m_num_sim_insn[m_sid];
<   m_stats->m_last_num_sim_winsn[m_sid] = m_stats->m_num_sim_winsn[m_sid];
---
>     m_stats->m_last_num_sim_insn[m_sid]=m_stats->m_num_sim_insn[m_sid];
>     m_stats->m_last_num_sim_winsn[m_sid]=m_stats->m_num_sim_winsn[m_sid];
1238,1270c1237,1269
<   warp_inst_t** preg = m_pipeline_reg[EX_WB].get_ready();
<   warp_inst_t* pipe_reg = (preg == NULL) ? NULL : *preg;
<   while ( preg and !pipe_reg->empty()) {
<     /*
<      * Right now, the writeback stage drains all waiting instructions
<      * assuming there are enough ports in the register file or the
<      * conflicts are resolved at issue.
<      */
<     /*
<      * The operand collector writeback can generally generate a stall
<      * However, here, the pipelines should be un-stallable. This is
<      * guaranteed because this is the first time the writeback function
<      * is called after the operand collector's step function, which
<      * resets the allocations. There is one case which could result in
<      * the writeback function returning false (stall), which is when
<      * an instruction tries to modify two registers (GPR and predicate)
<      * To handle this case, we ignore the return value (thus allowing
<      * no stalling).
<      */
< 
<     m_operand_collector.writeback(*pipe_reg);
<     unsigned warp_id = pipe_reg->warp_id();
<     m_scoreboard->releaseRegisters( pipe_reg );
<     m_warp[warp_id].dec_inst_in_pipeline();
<     warp_inst_complete(*pipe_reg);
<     m_gpu->gpu_sim_insn_last_update_sid = m_sid;
<     m_gpu->gpu_sim_insn_last_update = gpu_sim_cycle;
<     m_last_inst_gpu_sim_cycle = gpu_sim_cycle;
<     m_last_inst_gpu_tot_sim_cycle = gpu_tot_sim_cycle;
<     pipe_reg->clear();
<     preg = m_pipeline_reg[EX_WB].get_ready();
<     pipe_reg = (preg == NULL) ? NULL : *preg;
<   }
---
>     warp_inst_t** preg = m_pipeline_reg[EX_WB].get_ready();
>     warp_inst_t* pipe_reg = (preg==NULL)? NULL:*preg;
>     while( preg and !pipe_reg->empty()) {
>     	/*
>     	 * Right now, the writeback stage drains all waiting instructions
>     	 * assuming there are enough ports in the register file or the
>     	 * conflicts are resolved at issue.
>     	 */
>     	/*
>     	 * The operand collector writeback can generally generate a stall
>     	 * However, here, the pipelines should be un-stallable. This is
>     	 * guaranteed because this is the first time the writeback function
>     	 * is called after the operand collector's step function, which
>     	 * resets the allocations. There is one case which could result in
>     	 * the writeback function returning false (stall), which is when
>     	 * an instruction tries to modify two registers (GPR and predicate)
>     	 * To handle this case, we ignore the return value (thus allowing
>     	 * no stalling).
>     	 */
> 
>         m_operand_collector.writeback(*pipe_reg);
>         unsigned warp_id = pipe_reg->warp_id();
>         m_scoreboard->releaseRegisters( pipe_reg );
>         m_warp[warp_id].dec_inst_in_pipeline();
>         warp_inst_complete(*pipe_reg);
>         m_gpu->gpu_sim_insn_last_update_sid = m_sid;
>         m_gpu->gpu_sim_insn_last_update = gpu_sim_cycle;
>         m_last_inst_gpu_sim_cycle = gpu_sim_cycle;
>         m_last_inst_gpu_tot_sim_cycle = gpu_tot_sim_cycle;
>         pipe_reg->clear();
>         preg = m_pipeline_reg[EX_WB].get_ready();
>         pipe_reg = (preg==NULL)? NULL:*preg;
>     }
1275,1276c1274,1275
<   if ( inst.space.get_type() != shared_space )
<     return true;
---
>    if( inst.space.get_type() != shared_space )
>        return true;
1278,1280c1277,1279
<   if (inst.has_dispatch_delay()) {
<     m_stats->gpgpu_n_shmem_bank_access[m_sid]++;
<   }
---
>    if(inst.has_dispatch_delay()){
> 	   m_stats->gpgpu_n_shmem_bank_access[m_sid]++;
>    }
1282,1288c1281,1287
<   bool stall = inst.dispatch_delay();
<   if ( stall ) {
<     fail_type = S_MEM;
<     rc_fail = BK_CONF;
<   } else
<     rc_fail = NO_RC_FAIL;
<   return !stall;
---
>    bool stall = inst.dispatch_delay();
>    if( stall ) {
>        fail_type = S_MEM;
>        rc_fail = BK_CONF;
>    } else 
>        rc_fail = NO_RC_FAIL;
>    return !stall; 
1299,1326c1298,1325
<   mem_stage_stall_type result = NO_RC_FAIL;
<   bool write_sent = was_write_sent(events);
<   bool read_sent = was_read_sent(events);
<   if ( write_sent )
<     m_core->inc_store_req( inst.warp_id() );
<   if ( status == HIT ) {
<     assert( !read_sent );
<     inst.accessq_pop_back();
<     if ( inst.is_load() ) {
<       for ( unsigned r = 0; r < 4; r++)
<         if (inst.out[r] > 0)
<           m_pending_writes[inst.warp_id()][inst.out[r]]--;
<     }
<     if ( !write_sent )
<       delete mf;
<   } else if ( status == RESERVATION_FAIL ) {
<     result = COAL_STALL;
<     assert( !read_sent );
<     assert( !write_sent );
<     delete mf;
<   } else {
<     assert( status == MISS || status == HIT_RESERVED );
<     //inst.clear_active( access.get_warp_mask() ); // threads in mf writeback when mf returns
<     inst.accessq_pop_back();
<   }
<   if ( !inst.accessq_empty() )
<     result = BK_CONF;
<   return result;
---
>     mem_stage_stall_type result = NO_RC_FAIL;
>     bool write_sent = was_write_sent(events);
>     bool read_sent = was_read_sent(events);
>     if( write_sent ) 
>         m_core->inc_store_req( inst.warp_id() );
>     if ( status == HIT ) {
>         assert( !read_sent );
>         inst.accessq_pop_back();
>         if ( inst.is_load() ) {
>             for ( unsigned r=0; r < 4; r++)
>                 if (inst.out[r] > 0)
>                     m_pending_writes[inst.warp_id()][inst.out[r]]--; 
>         }
>         if( !write_sent ) 
>             delete mf;
>     } else if ( status == RESERVATION_FAIL ) {
>         result = COAL_STALL;
>         assert( !read_sent );
>         assert( !write_sent );
>         delete mf;
>     } else {
>         assert( status == MISS || status == HIT_RESERVED );
>         //inst.clear_active( access.get_warp_mask() ); // threads in mf writeback when mf returns
>         inst.accessq_pop_back();
>     }
>     if( !inst.accessq_empty() )
>         result = BK_CONF;
>     return result;
1331,1342c1330,1341
<   mem_stage_stall_type result = NO_RC_FAIL;
<   if ( inst.accessq_empty() )
<     return result;
< 
<   if ( !cache->data_port_free() )
<     return DATA_PORT_STALL;
< 
<   //const mem_access_t &access = inst.accessq_back();
<   mem_fetch *mf = m_mf_allocator->alloc(inst, inst.accessq_back());
<   std::list<cache_event> events;
<   enum cache_request_status status = cache->access(mf->get_addr(), mf, gpu_sim_cycle + gpu_tot_sim_cycle, events);
<   return process_cache_access( cache, mf->get_addr(), inst, events, mf, status );
---
>     mem_stage_stall_type result = NO_RC_FAIL;
>     if( inst.accessq_empty() )
>         return result;
> 
>     if( !cache->data_port_free() ) 
>         return DATA_PORT_STALL; 
> 
>     //const mem_access_t &access = inst.accessq_back();
>     mem_fetch *mf = m_mf_allocator->alloc(inst,inst.accessq_back());
>     std::list<cache_event> events;
>     enum cache_request_status status = cache->access(mf->get_addr(),mf,gpu_sim_cycle+gpu_tot_sim_cycle,events);
>     return process_cache_access( cache, mf->get_addr(), inst, events, mf, status );
1347,1359c1346,1358
<   if ( inst.empty() || ((inst.space.get_type() != const_space) && (inst.space.get_type() != param_space_kernel)) )
<     return true;
<   if ( inst.active_count() == 0 )
<     return true;
<   mem_stage_stall_type fail = process_memory_access_queue(m_L1C, inst);
<   if (fail != NO_RC_FAIL) {
<     rc_fail = fail; //keep other fails if this didn't fail.
<     fail_type = C_MEM;
<     if (rc_fail == BK_CONF or rc_fail == COAL_STALL) {
<       m_stats->gpgpu_n_cmem_portconflict++; //coal stalls aren't really a bank conflict, but this maintains previous behavior.
<     }
<   }
<   return inst.accessq_empty(); //done if empty.
---
>    if( inst.empty() || ((inst.space.get_type() != const_space) && (inst.space.get_type() != param_space_kernel)) )
>        return true;
>    if( inst.active_count() == 0 ) 
>        return true;
>    mem_stage_stall_type fail = process_memory_access_queue(m_L1C,inst);
>    if (fail != NO_RC_FAIL){ 
>       rc_fail = fail; //keep other fails if this didn't fail.
>       fail_type = C_MEM;
>       if (rc_fail == BK_CONF or rc_fail == COAL_STALL) {
>          m_stats->gpgpu_n_cmem_portconflict++; //coal stalls aren't really a bank conflict, but this maintains previous behavior.
>       }
>    }
>    return inst.accessq_empty(); //done if empty.
1364,1373c1363,1372
<   if ( inst.empty() || inst.space.get_type() != tex_space )
<     return true;
<   if ( inst.active_count() == 0 )
<     return true;
<   mem_stage_stall_type fail = process_memory_access_queue(m_L1T, inst);
<   if (fail != NO_RC_FAIL) {
<     rc_fail = fail; //keep other fails if this didn't fail.
<     fail_type = T_MEM;
<   }
<   return inst.accessq_empty(); //done if empty.
---
>    if( inst.empty() || inst.space.get_type() != tex_space )
>        return true;
>    if( inst.active_count() == 0 ) 
>        return true;
>    mem_stage_stall_type fail = process_memory_access_queue(m_L1T,inst);
>    if (fail != NO_RC_FAIL){ 
>       rc_fail = fail; //keep other fails if this didn't fail.
>       fail_type = T_MEM;
>    }
>    return inst.accessq_empty(); //done if empty.
1378c1377
<   if ( inst.empty() ||
---
>    if( inst.empty() || 
1381,1396c1380,1395
<         (inst.space.get_type() != param_space_local)) )
<     return true;
<   if ( inst.active_count() == 0 )
<     return true;
<   assert( !inst.accessq_empty() );
<   mem_stage_stall_type stall_cond = NO_RC_FAIL;
<   const mem_access_t &access = inst.accessq_back();
< 
<   bool bypassL1D = false;
<   if ( CACHE_GLOBAL == inst.cache_op || (m_L1D == NULL) ) {
<     bypassL1D = true;
<   } else if (inst.space.is_global()) { // global memory access
<     // skip L1 cache if the option is enabled
<     if (m_core->get_config()->gmem_skip_L1D)
<       bypassL1D = true;
<   }
---
>         (inst.space.get_type() != param_space_local)) ) 
>        return true;
>    if( inst.active_count() == 0 ) 
>        return true;
>    assert( !inst.accessq_empty() );
>    mem_stage_stall_type stall_cond = NO_RC_FAIL;
>    const mem_access_t &access = inst.accessq_back();
> 
>    bool bypassL1D = false; 
>    if ( CACHE_GLOBAL == inst.cache_op || (m_L1D == NULL) ) {
>        bypassL1D = true; 
>    } else if (inst.space.is_global()) { // global memory access 
>        // skip L1 cache if the option is enabled
>        if (m_core->get_config()->gmem_skip_L1D) 
>            bypassL1D = true; 
>    }
1398,1430c1397,1429
<   if ( bypassL1D ) {
<     // bypass L1 cache
<     unsigned control_size = inst.is_store() ? WRITE_PACKET_SIZE : READ_PACKET_SIZE;
<     unsigned size = access.get_size() + control_size;
<     if ( m_icnt->full(size, inst.is_store() || inst.isatomic()) ) {
<       stall_cond = ICNT_RC_FAIL;
<     } else {
<       mem_fetch *mf = m_mf_allocator->alloc(inst, access);
<       m_icnt->push(mf);
<       inst.accessq_pop_back();
<       //inst.clear_active( access.get_warp_mask() );
<       if ( inst.is_load() ) {
<         for ( unsigned r = 0; r < 4; r++)
<           if (inst.out[r] > 0)
<             assert( m_pending_writes[inst.warp_id()][inst.out[r]] > 0 );
<       } else if ( inst.is_store() )
<         m_core->inc_store_req( inst.warp_id() );
<     }
<   } else {
<     assert( CACHE_UNDEFINED != inst.cache_op );
<     stall_cond = process_memory_access_queue(m_L1D, inst);
<   }
<   if ( !inst.accessq_empty() )
<     stall_cond = COAL_STALL;
<   if (stall_cond != NO_RC_FAIL) {
<     stall_reason = stall_cond;
<     bool iswrite = inst.is_store();
<     if (inst.space.is_local())
<       access_type = (iswrite) ? L_MEM_ST : L_MEM_LD;
<     else
<       access_type = (iswrite) ? G_MEM_ST : G_MEM_LD;
<   }
<   return inst.accessq_empty();
---
>    if( bypassL1D ) {
>        // bypass L1 cache
>        unsigned control_size = inst.is_store() ? WRITE_PACKET_SIZE : READ_PACKET_SIZE;
>        unsigned size = access.get_size() + control_size;
>        if( m_icnt->full(size, inst.is_store() || inst.isatomic()) ) {
>            stall_cond = ICNT_RC_FAIL;
>        } else {
>            mem_fetch *mf = m_mf_allocator->alloc(inst,access);
>            m_icnt->push(mf);
>            inst.accessq_pop_back();
>            //inst.clear_active( access.get_warp_mask() );
>            if( inst.is_load() ) { 
>               for( unsigned r=0; r < 4; r++) 
>                   if(inst.out[r] > 0) 
>                       assert( m_pending_writes[inst.warp_id()][inst.out[r]] > 0 );
>            } else if( inst.is_store() ) 
>               m_core->inc_store_req( inst.warp_id() );
>        }
>    } else {
>        assert( CACHE_UNDEFINED != inst.cache_op );
>        stall_cond = process_memory_access_queue(m_L1D,inst);
>    }
>    if( !inst.accessq_empty() ) 
>        stall_cond = COAL_STALL;
>    if (stall_cond != NO_RC_FAIL) {
>       stall_reason = stall_cond;
>       bool iswrite = inst.is_store();
>       if (inst.space.is_local()) 
>          access_type = (iswrite)?L_MEM_ST:L_MEM_LD;
>       else 
>          access_type = (iswrite)?G_MEM_ST:G_MEM_LD;
>    }
>    return inst.accessq_empty(); 
1436c1435
<   return m_response_fifo.size() >= m_config->ldst_unit_response_queue_size;
---
>     return m_response_fifo.size() >= m_config->ldst_unit_response_queue_size;
1441,1442c1440,1441
<   mf->set_status(IN_SHADER_LDST_RESPONSE_FIFO, gpu_sim_cycle + gpu_tot_sim_cycle);
<   m_response_fifo.push_back(mf);
---
>     mf->set_status(IN_SHADER_LDST_RESPONSE_FIFO,gpu_sim_cycle+gpu_tot_sim_cycle);
>     m_response_fifo.push_back(mf);
1445,1447c1444,1446
< void ldst_unit::flush() {
<   // Flush L1D cache
<   m_L1D->flush();
---
> void ldst_unit::flush(){
> 	// Flush L1D cache
> 	m_L1D->flush();
1451,1453c1450,1452
< {
<   m_config = config;
<   m_dispatch_reg = new warp_inst_t(config);
---
> { 
>     m_config=config;
>     m_dispatch_reg = new warp_inst_t(config); 
1457,1460c1456,1459
< sfu:: sfu(  register_set* result_port, const shader_core_config *config, shader_core_ctx *core  )
<   : pipelined_simd_unit(result_port, config, config->max_sfu_latency, core)
< {
<   m_name = "SFU";
---
> sfu:: sfu(  register_set* result_port, const shader_core_config *config,shader_core_ctx *core  )
>     : pipelined_simd_unit(result_port,config,config->max_sfu_latency,core)
> { 
>     m_name = "SFU"; 
1465,1492c1464,1465
<   warp_inst_t** ready_reg = source_reg.get_ready();
<   //m_core->incexecstat((*ready_reg));
< 
<   (*ready_reg)->op_pipe = SFU__OP;
<   m_core->incsfu_stat(m_core->get_config()->warp_size, (*ready_reg)->latency);
<   pipelined_simd_unit::issue(source_reg);
< }
< 
< void ldst_unit::active_lanes_in_pipeline() {
<   unsigned active_count = pipelined_simd_unit::get_active_lanes_in_pipeline();
<   assert(active_count <= m_core->get_config()->warp_size);
<   m_core->incfumemactivelanes_stat(active_count);
< }
< void sp_unit::active_lanes_in_pipeline() {
<   unsigned active_count = pipelined_simd_unit::get_active_lanes_in_pipeline();
<   assert(active_count <= m_core->get_config()->warp_size);
<   m_core->incspactivelanes_stat(active_count);
<   m_core->incfuactivelanes_stat(active_count);
<   m_core->incfumemactivelanes_stat(active_count);
< }
< 
< void sfu::active_lanes_in_pipeline() {
<   unsigned active_count = pipelined_simd_unit::get_active_lanes_in_pipeline();
<   assert(active_count <= m_core->get_config()->warp_size);
<   m_core->incsfuactivelanes_stat(active_count);
<   m_core->incfuactivelanes_stat(active_count);
<   m_core->incfumemactivelanes_stat(active_count);
< }
---
>     warp_inst_t** ready_reg = source_reg.get_ready();
> 	//m_core->incexecstat((*ready_reg));
1494,1497c1467,1496
< sp_unit::sp_unit( register_set* result_port, const shader_core_config *config, shader_core_ctx *core)
<   : pipelined_simd_unit(result_port, config, config->max_sp_latency, core)
< {
<   m_name = "SP ";
---
> 	(*ready_reg)->op_pipe=SFU__OP;
> 	m_core->incsfu_stat(m_core->get_config()->warp_size,(*ready_reg)->latency);
> 	pipelined_simd_unit::issue(source_reg);
> }
> 
> void ldst_unit::active_lanes_in_pipeline(){
> 	unsigned active_count=pipelined_simd_unit::get_active_lanes_in_pipeline();
> 	assert(active_count<=m_core->get_config()->warp_size);
> 	m_core->incfumemactivelanes_stat(active_count);
> }
> void sp_unit::active_lanes_in_pipeline(){
> 	unsigned active_count=pipelined_simd_unit::get_active_lanes_in_pipeline();
> 	assert(active_count<=m_core->get_config()->warp_size);
> 	m_core->incspactivelanes_stat(active_count);
> 	m_core->incfuactivelanes_stat(active_count);
> 	m_core->incfumemactivelanes_stat(active_count);
> }
> 
> void sfu::active_lanes_in_pipeline(){
> 	unsigned active_count=pipelined_simd_unit::get_active_lanes_in_pipeline();
> 	assert(active_count<=m_core->get_config()->warp_size);
> 	m_core->incsfuactivelanes_stat(active_count);
> 	m_core->incfuactivelanes_stat(active_count);
> 	m_core->incfumemactivelanes_stat(active_count);
> }
> 
> sp_unit::sp_unit( register_set* result_port, const shader_core_config *config,shader_core_ctx *core)
>     : pipelined_simd_unit(result_port,config,config->max_sp_latency,core)
> { 
>     m_name = "SP "; 
1502,1506c1501,1505
<   warp_inst_t** ready_reg = source_reg.get_ready();
<   //m_core->incexecstat((*ready_reg));
<   (*ready_reg)->op_pipe = SP__OP;
<   m_core->incsp_stat(m_core->get_config()->warp_size, (*ready_reg)->latency);
<   pipelined_simd_unit::issue(source_reg);
---
>     warp_inst_t** ready_reg = source_reg.get_ready();
> 	//m_core->incexecstat((*ready_reg));
> 	(*ready_reg)->op_pipe=SP__OP;
> 	m_core->incsp_stat(m_core->get_config()->warp_size,(*ready_reg)->latency);
> 	pipelined_simd_unit::issue(source_reg);
1510,1518c1509,1517
< pipelined_simd_unit::pipelined_simd_unit( register_set* result_port, const shader_core_config *config, unsigned max_latency, shader_core_ctx *core )
<   : simd_function_unit(config)
< {
<   m_result_port = result_port;
<   m_pipeline_depth = max_latency;
<   m_pipeline_reg = new warp_inst_t*[m_pipeline_depth];
<   for ( unsigned i = 0; i < m_pipeline_depth; i++ )
<     m_pipeline_reg[i] = new warp_inst_t( config );
<   m_core = core;
---
> pipelined_simd_unit::pipelined_simd_unit( register_set* result_port, const shader_core_config *config, unsigned max_latency,shader_core_ctx *core )
>     : simd_function_unit(config) 
> {
>     m_result_port = result_port;
>     m_pipeline_depth = max_latency;
>     m_pipeline_reg = new warp_inst_t*[m_pipeline_depth];
>     for( unsigned i=0; i < m_pipeline_depth; i++ ) 
> 	m_pipeline_reg[i] = new warp_inst_t( config );
>     m_core=core;
1523,1531c1522,1523
<   if ( !m_pipeline_reg[0]->empty() ) {
<     m_result_port->move_in(m_pipeline_reg[0]);
<   }
<   for ( unsigned stage = 0; (stage + 1) < m_pipeline_depth; stage++ )
<     move_warp(m_pipeline_reg[stage], m_pipeline_reg[stage + 1]);
<   if ( !m_dispatch_reg->empty() ) {
<     if ( !m_dispatch_reg->dispatch_delay()) {
<       int start_stage = m_dispatch_reg->latency - m_dispatch_reg->initiation_interval;
<       move_warp(m_pipeline_reg[start_stage], m_dispatch_reg);
---
>     if( !m_pipeline_reg[0]->empty() ){
>         m_result_port->move_in(m_pipeline_reg[0]);
1533,1534c1525,1533
<   }
<   occupied >>= 1;
---
>     for( unsigned stage=0; (stage+1)<m_pipeline_depth; stage++ )
>         move_warp(m_pipeline_reg[stage], m_pipeline_reg[stage+1]);
>     if( !m_dispatch_reg->empty() ) {
>         if( !m_dispatch_reg->dispatch_delay()){
>             int start_stage = m_dispatch_reg->latency - m_dispatch_reg->initiation_interval;
>             move_warp(m_pipeline_reg[start_stage],m_dispatch_reg);
>         }
>     }
>     occupied >>=1;
1540,1544c1539,1543
<   //move_warp(m_dispatch_reg,source_reg);
<   warp_inst_t** ready_reg = source_reg.get_ready();
<   m_core->incexecstat((*ready_reg));
<   //source_reg.move_out_to(m_dispatch_reg);
<   simd_function_unit::issue(source_reg);
---
>     //move_warp(m_dispatch_reg,source_reg);
>     warp_inst_t** ready_reg = source_reg.get_ready();
> 	m_core->incexecstat((*ready_reg));
> 	//source_reg.move_out_to(m_dispatch_reg);
> 	simd_function_unit::issue(source_reg);
1558c1557
<                       shader_core_ctx *core,
---
>                       shader_core_ctx *core, 
1562c1561
<                       const memory_config *mem_config,
---
>                       const memory_config *mem_config,  
1567,1589c1566,1588
<   m_memory_config = mem_config;
<   m_icnt = icnt;
<   m_mf_allocator = mf_allocator;
<   m_core = core;
<   m_operand_collector = operand_collector;
<   m_scoreboard = scoreboard;
<   m_stats = stats;
<   m_sid = sid;
<   m_tpc = tpc;
< #define STRSIZE 1024
<   char L1T_name[STRSIZE];
<   char L1C_name[STRSIZE];
<   snprintf(L1T_name, STRSIZE, "L1T_%03d", m_sid);
<   snprintf(L1C_name, STRSIZE, "L1C_%03d", m_sid);
<   m_L1T = new tex_cache(L1T_name, m_config->m_L1T_config, m_sid, get_shader_texture_cache_id(), icnt, IN_L1T_MISS_QUEUE, IN_SHADER_L1T_ROB);
<   m_L1C = new read_only_cache(L1C_name, m_config->m_L1C_config, m_sid, get_shader_constant_cache_id(), icnt, IN_L1C_MISS_QUEUE);
<   m_L1D = NULL;
<   m_mem_rc = NO_RC_FAIL;
<   m_num_writeback_clients = 5; // = shared memory, global/local (uncached), L1D, L1T, L1C
<   m_writeback_arb = 0;
<   m_next_global = NULL;
<   m_last_inst_gpu_sim_cycle = 0;
<   m_last_inst_gpu_tot_sim_cycle = 0;
---
>     m_memory_config = mem_config;
>     m_icnt = icnt;
>     m_mf_allocator=mf_allocator;
>     m_core = core;
>     m_operand_collector = operand_collector;
>     m_scoreboard = scoreboard;
>     m_stats = stats;
>     m_sid = sid;
>     m_tpc = tpc;
>     #define STRSIZE 1024
>     char L1T_name[STRSIZE];
>     char L1C_name[STRSIZE];
>     snprintf(L1T_name, STRSIZE, "L1T_%03d", m_sid);
>     snprintf(L1C_name, STRSIZE, "L1C_%03d", m_sid);
>     m_L1T = new tex_cache(L1T_name,m_config->m_L1T_config,m_sid,get_shader_texture_cache_id(),icnt,IN_L1T_MISS_QUEUE,IN_SHADER_L1T_ROB);
>     m_L1C = new read_only_cache(L1C_name,m_config->m_L1C_config,m_sid,get_shader_constant_cache_id(),icnt,IN_L1C_MISS_QUEUE);
>     m_L1D = NULL;
>     m_mem_rc = NO_RC_FAIL;
>     m_num_writeback_clients=5; // = shared memory, global/local (uncached), L1D, L1T, L1C
>     m_writeback_arb = 0;
>     m_next_global=NULL;
>     m_last_inst_gpu_sim_cycle=0;
>     m_last_inst_gpu_tot_sim_cycle=0;
1595c1594
<                       shader_core_ctx *core,
---
>                       shader_core_ctx *core, 
1599c1598
<                       const memory_config *mem_config,
---
>                       const memory_config *mem_config,  
1602c1601
<                       unsigned tpc ) : pipelined_simd_unit(NULL, config, 3, core), m_next_wb(config)
---
>                       unsigned tpc ) : pipelined_simd_unit(NULL,config,3,core), m_next_wb(config)
1604,1624c1603,1623
<   init( icnt,
<         mf_allocator,
<         core,
<         operand_collector,
<         scoreboard,
<         config,
<         mem_config,
<         stats,
<         sid,
<         tpc );
<   if ( !m_config->m_L1D_config.disabled() ) {
<     char L1D_name[STRSIZE];
<     snprintf(L1D_name, STRSIZE, "L1D_%03d", m_sid);
<     m_L1D = new l1_cache( L1D_name,
<                           m_config->m_L1D_config,
<                           m_sid,
<                           get_shader_normal_cache_id(),
<                           m_icnt,
<                           m_mf_allocator,
<                           IN_L1D_MISS_QUEUE );
<   }
---
>     init( icnt,
>           mf_allocator,
>           core, 
>           operand_collector,
>           scoreboard,
>           config, 
>           mem_config,  
>           stats, 
>           sid,
>           tpc );
>     if( !m_config->m_L1D_config.disabled() ) {
>         char L1D_name[STRSIZE];
>         snprintf(L1D_name, STRSIZE, "L1D_%03d", m_sid);
>         m_L1D = new l1_cache( L1D_name,
>                               m_config->m_L1D_config,
>                               m_sid,
>                               get_shader_normal_cache_id(),
>                               m_icnt,
>                               m_mf_allocator,
>                               IN_L1D_MISS_QUEUE );
>     }
1629c1628
<                       shader_core_ctx *core,
---
>                       shader_core_ctx *core, 
1633c1632
<                       const memory_config *mem_config,
---
>                       const memory_config *mem_config,  
1638c1637
<   : pipelined_simd_unit(NULL, config, 3, core), m_L1D(new_l1d_cache), m_next_wb(config)
---
>     : pipelined_simd_unit(NULL,config,3,core), m_L1D(new_l1d_cache), m_next_wb(config)
1640,1649c1639,1648
<   init( icnt,
<         mf_allocator,
<         core,
<         operand_collector,
<         scoreboard,
<         config,
<         mem_config,
<         stats,
<         sid,
<         tpc );
---
>     init( icnt,
>           mf_allocator,
>           core, 
>           operand_collector,
>           scoreboard,
>           config, 
>           mem_config,  
>           stats, 
>           sid,
>           tpc );
1654,1778c1653
<   warp_inst_t* inst = *(reg_set.get_ready());
< 
<   // record how many pending register writes/memory accesses there are for this instruction
<   assert(inst->empty() == false);
<   if (inst->is_load() and inst->space.get_type() != shared_space) {
<     unsigned warp_id = inst->warp_id();
<     unsigned n_accesses = inst->accessq_count();
<     for (unsigned r = 0; r < 4; r++) {
<       unsigned reg_id = inst->out[r];
<       if (reg_id > 0) {
<         m_pending_writes[warp_id][reg_id] += n_accesses;
<       }
<     }
<   }
< 
< 
<   inst->op_pipe = MEM__OP;
<   // stat collection
<   m_core->mem_instruction_stats(*inst);
<   m_core->incmem_stat(m_core->get_config()->warp_size, 1);
<   pipelined_simd_unit::issue(reg_set);
< }
< 
< void ldst_unit::writeback()
< {
<   // process next instruction that is going to writeback
<   if ( !m_next_wb.empty() ) {
<     if ( m_operand_collector->writeback(m_next_wb) ) {
<       bool insn_completed = false;
<       for ( unsigned r = 0; r < 4; r++ ) {
<         if ( m_next_wb.out[r] > 0 ) {
<           if ( m_next_wb.space.get_type() != shared_space ) {
<             assert( m_pending_writes[m_next_wb.warp_id()][m_next_wb.out[r]] > 0 );
<             unsigned still_pending = --m_pending_writes[m_next_wb.warp_id()][m_next_wb.out[r]];
<             if ( !still_pending ) {
<               m_pending_writes[m_next_wb.warp_id()].erase(m_next_wb.out[r]);
<               m_scoreboard->releaseRegister( m_next_wb.warp_id(), m_next_wb.out[r] );
<               insn_completed = true;
<             }
<           } else { // shared
<             m_scoreboard->releaseRegister( m_next_wb.warp_id(), m_next_wb.out[r] );
<             insn_completed = true;
<           }
<         }
<       }
<       if ( insn_completed ) {
<         m_core->warp_inst_complete(m_next_wb);
<       }
<       m_next_wb.clear();
<       m_last_inst_gpu_sim_cycle = gpu_sim_cycle;
<       m_last_inst_gpu_tot_sim_cycle = gpu_tot_sim_cycle;
<     }
<   }
< 
<   unsigned serviced_client = -1;
<   for ( unsigned c = 0; m_next_wb.empty() && (c < m_num_writeback_clients); c++ ) {
<     unsigned next_client = (c + m_writeback_arb) % m_num_writeback_clients;
<     switch ( next_client ) {
<     case 0: // shared memory
<       if ( !m_pipeline_reg[0]->empty() ) {
<         m_next_wb = *m_pipeline_reg[0];
<         if (m_next_wb.isatomic()) {
<           m_next_wb.do_atomic();
<           m_core->decrement_atomic_count(m_next_wb.warp_id(), m_next_wb.active_count());
<         }
<         m_core->dec_inst_in_pipeline(m_pipeline_reg[0]->warp_id());
<         m_pipeline_reg[0]->clear();
<         serviced_client = next_client;
<       }
<       break;
<     case 1: // texture response
<       if ( m_L1T->access_ready() ) {
<         mem_fetch *mf = m_L1T->next_access();
<         m_next_wb = mf->get_inst();
<         delete mf;
<         serviced_client = next_client;
<       }
<       break;
<     case 2: // const cache response
<       if ( m_L1C->access_ready() ) {
<         mem_fetch *mf = m_L1C->next_access();
<         m_next_wb = mf->get_inst();
<         delete mf;
<         serviced_client = next_client;
<       }
<       break;
<     case 3: // global/local
<       if ( m_next_global ) {
<         m_next_wb = m_next_global->get_inst();
<         if ( m_next_global->isatomic() )
<           m_core->decrement_atomic_count(m_next_global->get_wid(), m_next_global->get_access_warp_mask().count());
<         delete m_next_global;
<         m_next_global = NULL;
<         serviced_client = next_client;
<       }
<       break;
<     case 4:
<       if ( m_L1D && m_L1D->access_ready() ) {
<         mem_fetch *mf = m_L1D->next_access();
<         m_next_wb = mf->get_inst();
<         delete mf;
<         serviced_client = next_client;
<       }
<       break;
<     default: abort();
<     }
<   }
<   // update arbitration priority only if:
<   // 1. the writeback buffer was available
<   // 2. a client was serviced
<   if (serviced_client != (unsigned) - 1) {
<     m_writeback_arb = (serviced_client + 1) % m_num_writeback_clients;
<   }
< }
< 
< unsigned ldst_unit::clock_multiplier() const
< {
<   return m_config->mem_warp_parts;
< }
< /*
< void ldst_unit::issue( register_set &reg_set )
< {
<   warp_inst_t* inst = *(reg_set.get_ready());
<    // stat collection
<    m_core->mem_instruction_stats(*inst);
---
> 	warp_inst_t* inst = *(reg_set.get_ready());
1793c1668,1673
<    pipelined_simd_unit::issue(reg_set);
---
> 
> 	inst->op_pipe=MEM__OP;
> 	// stat collection
> 	m_core->mem_instruction_stats(*inst);
> 	m_core->incmem_stat(m_core->get_config()->warp_size,1);
> 	pipelined_simd_unit::issue(reg_set);
1795,1823d1674
< */
< void ldst_unit::cycle()
< {
<   writeback();
<   m_operand_collector->step();
<   for ( unsigned stage = 0; (stage + 1) < m_pipeline_depth; stage++ )
<     if ( m_pipeline_reg[stage]->empty() && !m_pipeline_reg[stage + 1]->empty() )
<       move_warp(m_pipeline_reg[stage], m_pipeline_reg[stage + 1]);
< 
<   if ( !m_response_fifo.empty() ) {
<     mem_fetch *mf = m_response_fifo.front();
<     if (mf->istexture()) {
<       if (m_L1T->fill_port_free()) {
<         m_L1T->fill(mf, gpu_sim_cycle + gpu_tot_sim_cycle);
<         m_response_fifo.pop_front();
<       }
<     } else if (mf->isconst())  {
<       if (m_L1C->fill_port_free()) {
<         mf->set_status(IN_SHADER_FETCHED, gpu_sim_cycle + gpu_tot_sim_cycle);
<         m_L1C->fill(mf, gpu_sim_cycle + gpu_tot_sim_cycle);
<         m_response_fifo.pop_front();
<       }
<     } else {
<       if ( mf->get_type() == WRITE_ACK || ( m_config->gpgpu_perfect_mem && mf->get_is_write() )) {
<         m_core->store_ack(mf);
<         m_response_fifo.pop_front();
<         delete mf;
<       } else {
<         assert( !mf->get_is_write() ); // L1 cache is write evict, allocate line on load miss only
1825,1842c1676,1703
<         bool bypassL1D = false;
<         if ( CACHE_GLOBAL == mf->get_inst().cache_op || (m_L1D == NULL) ) {
<           bypassL1D = true;
<         } else if (mf->get_access_type() == GLOBAL_ACC_R || mf->get_access_type() == GLOBAL_ACC_W) { // global memory access
<           if (m_core->get_config()->gmem_skip_L1D)
<             bypassL1D = true;
<         }
<         if ( bypassL1D ) {
<           if ( m_next_global == NULL ) {
<             mf->set_status(IN_SHADER_FETCHED, gpu_sim_cycle + gpu_tot_sim_cycle);
<             m_response_fifo.pop_front();
<             m_next_global = mf;
<           }
<         } else {
<           if (m_L1D->fill_port_free()) {
<             m_L1D->fill(mf, gpu_sim_cycle + gpu_tot_sim_cycle);
<             m_response_fifo.pop_front();
<           }
---
> void ldst_unit::writeback()
> {
>     // process next instruction that is going to writeback
>     if( !m_next_wb.empty() ) {
>         if( m_operand_collector->writeback(m_next_wb) ) {
>             bool insn_completed = false; 
>             for( unsigned r=0; r < 4; r++ ) {
>                 if( m_next_wb.out[r] > 0 ) {
>                     if( m_next_wb.space.get_type() != shared_space ) {
>                         assert( m_pending_writes[m_next_wb.warp_id()][m_next_wb.out[r]] > 0 );
>                         unsigned still_pending = --m_pending_writes[m_next_wb.warp_id()][m_next_wb.out[r]];
>                         if( !still_pending ) {
>                             m_pending_writes[m_next_wb.warp_id()].erase(m_next_wb.out[r]);
>                             m_scoreboard->releaseRegister( m_next_wb.warp_id(), m_next_wb.out[r] );
>                             insn_completed = true; 
>                         }
>                     } else { // shared 
>                         m_scoreboard->releaseRegister( m_next_wb.warp_id(), m_next_wb.out[r] );
>                         insn_completed = true; 
>                     }
>                 }
>             }
>             if( insn_completed ) {
>                 m_core->warp_inst_complete(m_next_wb);
>             }
>             m_next_wb.clear();
>             m_last_inst_gpu_sim_cycle = gpu_sim_cycle;
>             m_last_inst_gpu_tot_sim_cycle = gpu_tot_sim_cycle;
1844d1704
<       }
1846,1867d1705
<   }
< 
<   m_L1T->cycle();
<   m_L1C->cycle();
<   if ( m_L1D ) m_L1D->cycle();
< 
<   warp_inst_t &pipe_reg = *m_dispatch_reg;
<   enum mem_stage_stall_type rc_fail = NO_RC_FAIL;
<   mem_stage_access_type type;
<   bool done = true;
<   done &= shared_cycle(pipe_reg, rc_fail, type);
<   done &= constant_cycle(pipe_reg, rc_fail, type);
<   done &= texture_cycle(pipe_reg, rc_fail, type);
<   done &= memory_cycle(pipe_reg, rc_fail, type);
<   m_mem_rc = rc_fail;
< 
<   if (!done) { // log stall types and return
<     assert(rc_fail != NO_RC_FAIL);
<     m_stats->gpgpu_n_stall_shd_mem++;
<     m_stats->gpu_stall_shd_mem_breakdown[type][rc_fail]++;
<     return;
<   }
1869,1895c1707,1720
<   if ( !pipe_reg.empty() ) {
<     unsigned warp_id = pipe_reg.warp_id();
<     if ( pipe_reg.is_load() ) {
<       if ( pipe_reg.space.get_type() == shared_space ) {
<         if ( m_pipeline_reg[2]->empty() ) {
<           // new shared memory request
<           move_warp(m_pipeline_reg[2], m_dispatch_reg);
<           m_dispatch_reg->clear();
<         }
<       } else {
<         //if( pipe_reg.active_count() > 0 ) {
<         //    if( !m_operand_collector->writeback(pipe_reg) )
<         //        return;
<         //}
< 
<         bool pending_requests = false;
<         for ( unsigned r = 0; r < 4; r++ ) {
<           unsigned reg_id = pipe_reg.out[r];
<           if ( reg_id > 0 ) {
<             if ( m_pending_writes[warp_id].find(reg_id) != m_pending_writes[warp_id].end() ) {
<               if ( m_pending_writes[warp_id][reg_id] > 0 ) {
<                 pending_requests = true;
<                 break;
<               } else {
<                 // this instruction is done already
<                 m_pending_writes[warp_id].erase(reg_id);
<               }
---
>     unsigned serviced_client = -1; 
>     for( unsigned c = 0; m_next_wb.empty() && (c < m_num_writeback_clients); c++ ) {
>         unsigned next_client = (c+m_writeback_arb)%m_num_writeback_clients;
>         switch( next_client ) {
>         case 0: // shared memory 
>             if( !m_pipeline_reg[0]->empty() ) {
>                 m_next_wb = *m_pipeline_reg[0];
>                 if(m_next_wb.isatomic()) {
>                     m_next_wb.do_atomic();
>                     m_core->decrement_atomic_count(m_next_wb.warp_id(), m_next_wb.active_count());
>                 }
>                 m_core->dec_inst_in_pipeline(m_pipeline_reg[0]->warp_id());
>                 m_pipeline_reg[0]->clear();
>                 serviced_client = next_client; 
1897,1901c1722,1757
<           }
<         }
<         if ( !pending_requests ) {
<           m_core->warp_inst_complete(*m_dispatch_reg);
<           m_scoreboard->releaseRegisters(m_dispatch_reg);
---
>             break;
>         case 1: // texture response
>             if( m_L1T->access_ready() ) {
>                 mem_fetch *mf = m_L1T->next_access();
>                 m_next_wb = mf->get_inst();
>                 delete mf;
>                 serviced_client = next_client; 
>             }
>             break;
>         case 2: // const cache response
>             if( m_L1C->access_ready() ) {
>                 mem_fetch *mf = m_L1C->next_access();
>                 m_next_wb = mf->get_inst();
>                 delete mf;
>                 serviced_client = next_client; 
>             }
>             break;
>         case 3: // global/local
>             if( m_next_global ) {
>                 m_next_wb = m_next_global->get_inst();
>                 if( m_next_global->isatomic() ) 
>                     m_core->decrement_atomic_count(m_next_global->get_wid(),m_next_global->get_access_warp_mask().count());
>                 delete m_next_global;
>                 m_next_global = NULL;
>                 serviced_client = next_client; 
>             }
>             break;
>         case 4: 
>             if( m_L1D && m_L1D->access_ready() ) {
>                 mem_fetch *mf = m_L1D->next_access();
>                 m_next_wb = mf->get_inst();
>                 delete mf;
>                 serviced_client = next_client; 
>             }
>             break;
>         default: abort();
1903,1910d1758
<         m_core->dec_inst_in_pipeline(warp_id);
<         m_dispatch_reg->clear();
<       }
<     } else {
<       // stores exit pipeline here
<       m_core->dec_inst_in_pipeline(warp_id);
<       m_core->warp_inst_complete(*m_dispatch_reg);
<       m_dispatch_reg->clear();
1912c1760,1911
<   }
---
>     // update arbitration priority only if: 
>     // 1. the writeback buffer was available 
>     // 2. a client was serviced 
>     if (serviced_client != (unsigned)-1) {
>         m_writeback_arb = (serviced_client + 1) % m_num_writeback_clients; 
>     }
> }
> 
> unsigned ldst_unit::clock_multiplier() const
> { 
>     return m_config->mem_warp_parts; 
> }
> /*
> void ldst_unit::issue( register_set &reg_set )
> {
> 	warp_inst_t* inst = *(reg_set.get_ready());
>    // stat collection
>    m_core->mem_instruction_stats(*inst); 
> 
>    // record how many pending register writes/memory accesses there are for this instruction 
>    assert(inst->empty() == false); 
>    if (inst->is_load() and inst->space.get_type() != shared_space) {
>       unsigned warp_id = inst->warp_id(); 
>       unsigned n_accesses = inst->accessq_count(); 
>       for (unsigned r = 0; r < 4; r++) {
>          unsigned reg_id = inst->out[r]; 
>          if (reg_id > 0) {
>             m_pending_writes[warp_id][reg_id] += n_accesses; 
>          }
>       }
>    }
> 
>    pipelined_simd_unit::issue(reg_set);
> }
> */
> void ldst_unit::cycle()
> {
>    writeback();
>    m_operand_collector->step();
>    for( unsigned stage=0; (stage+1)<m_pipeline_depth; stage++ ) 
>        if( m_pipeline_reg[stage]->empty() && !m_pipeline_reg[stage+1]->empty() )
>             move_warp(m_pipeline_reg[stage], m_pipeline_reg[stage+1]);
> 
>    if( !m_response_fifo.empty() ) {
>        mem_fetch *mf = m_response_fifo.front();
>        if (mf->istexture()) {
>            if (m_L1T->fill_port_free()) {
>                m_L1T->fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle);
>                m_response_fifo.pop_front(); 
>            }
>        } else if (mf->isconst())  {
>            if (m_L1C->fill_port_free()) {
>                mf->set_status(IN_SHADER_FETCHED,gpu_sim_cycle+gpu_tot_sim_cycle);
>                m_L1C->fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle);
>                m_response_fifo.pop_front(); 
>            }
>        } else {
>     	   if( mf->get_type() == WRITE_ACK || ( m_config->gpgpu_perfect_mem && mf->get_is_write() )) {
>                m_core->store_ack(mf);
>                m_response_fifo.pop_front();
>                delete mf;
>            } else {
>                assert( !mf->get_is_write() ); // L1 cache is write evict, allocate line on load miss only
> 
>                bool bypassL1D = false; 
>                if ( CACHE_GLOBAL == mf->get_inst().cache_op || (m_L1D == NULL) ) {
>                    bypassL1D = true; 
>                } else if (mf->get_access_type() == GLOBAL_ACC_R || mf->get_access_type() == GLOBAL_ACC_W) { // global memory access 
>                    if (m_core->get_config()->gmem_skip_L1D)
>                        bypassL1D = true; 
>                }
>                if( bypassL1D ) {
>                    if ( m_next_global == NULL ) {
>                        mf->set_status(IN_SHADER_FETCHED,gpu_sim_cycle+gpu_tot_sim_cycle);
>                        m_response_fifo.pop_front();
>                        m_next_global = mf;
>                    }
>                } else {
>                    if (m_L1D->fill_port_free()) {
>                        m_L1D->fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle);
>                        m_response_fifo.pop_front();
>                    }
>                }
>            }
>        }
>    }
> 
>    m_L1T->cycle();
>    m_L1C->cycle();
>    if( m_L1D ) m_L1D->cycle();
> 
>    warp_inst_t &pipe_reg = *m_dispatch_reg;
>    enum mem_stage_stall_type rc_fail = NO_RC_FAIL;
>    mem_stage_access_type type;
>    bool done = true;
>    done &= shared_cycle(pipe_reg, rc_fail, type);
>    done &= constant_cycle(pipe_reg, rc_fail, type);
>    done &= texture_cycle(pipe_reg, rc_fail, type);
>    done &= memory_cycle(pipe_reg, rc_fail, type);
>    m_mem_rc = rc_fail;
> 
>    if (!done) { // log stall types and return
>       assert(rc_fail != NO_RC_FAIL);
>       m_stats->gpgpu_n_stall_shd_mem++;
>       m_stats->gpu_stall_shd_mem_breakdown[type][rc_fail]++;
>       return;
>    }
> 
>    if( !pipe_reg.empty() ) {
>        unsigned warp_id = pipe_reg.warp_id();
>        if( pipe_reg.is_load() ) {
>            if( pipe_reg.space.get_type() == shared_space ) {
>                if( m_pipeline_reg[2]->empty() ) {
>                    // new shared memory request
>                    move_warp(m_pipeline_reg[2],m_dispatch_reg);
>                    m_dispatch_reg->clear();
>                }
>            } else {
>                //if( pipe_reg.active_count() > 0 ) {
>                //    if( !m_operand_collector->writeback(pipe_reg) ) 
>                //        return;
>                //} 
> 
>                bool pending_requests=false;
>                for( unsigned r=0; r<4; r++ ) {
>                    unsigned reg_id = pipe_reg.out[r];
>                    if( reg_id > 0 ) {
>                        if( m_pending_writes[warp_id].find(reg_id) != m_pending_writes[warp_id].end() ) {
>                            if ( m_pending_writes[warp_id][reg_id] > 0 ) {
>                                pending_requests=true;
>                                break;
>                            } else {
>                                // this instruction is done already
>                                m_pending_writes[warp_id].erase(reg_id); 
>                            }
>                        }
>                    }
>                }
>                if( !pending_requests ) {
>                    m_core->warp_inst_complete(*m_dispatch_reg);
>                    m_scoreboard->releaseRegisters(m_dispatch_reg);
>                }
>                m_core->dec_inst_in_pipeline(warp_id);
>                m_dispatch_reg->clear();
>            }
>        } else {
>            // stores exit pipeline here
>            m_core->dec_inst_in_pipeline(warp_id);
>            m_core->warp_inst_complete(*m_dispatch_reg);
>            m_dispatch_reg->clear();
>        }
>    }
1917,1934c1916,1936
<   assert( m_cta_status[cta_num] > 0 );
<   m_cta_status[cta_num]--;
<   if (!m_cta_status[cta_num]) {
<     m_n_active_cta--;
<     m_barriers.deallocate_barrier(cta_num);
<     shader_CTA_count_unlog(m_sid, 1);
<     printf("GPGPU-Sim uArch: Shader %d finished CTA #%d (%lld,%lld), %u CTAs running\n", m_sid, cta_num, gpu_sim_cycle, gpu_tot_sim_cycle,
<            m_n_active_cta );
<     if ( m_n_active_cta == 0 ) {
<       assert( m_kernel != NULL );
<       m_kernel->dec_running();
<       printf("GPGPU-Sim uArch: Shader %u empty (release kernel %u \'%s\').\n", m_sid, m_kernel->get_uid(),
<              m_kernel->name().c_str() );
<       if ( m_kernel->no_more_ctas_to_run() ) {
<         if ( !m_kernel->running() ) {
<           printf("GPGPU-Sim uArch: GPU detected kernel \'%s\' finished on shader %u.\n", m_kernel->name().c_str(), m_sid );
<           m_gpu->set_kernel_done( m_kernel );
<         }
---
>    assert( m_cta_status[cta_num] > 0 );
>    m_cta_status[cta_num]--;
>    if (!m_cta_status[cta_num]) {
>       m_n_active_cta--;
>       m_barriers.deallocate_barrier(cta_num);
>       shader_CTA_count_unlog(m_sid, 1);
>       //printf("GPGPU-Sim uArch: Shader %d finished CTA #%d (%lld,%lld), %u CTAs running\n", m_sid, cta_num, gpu_sim_cycle, gpu_tot_sim_cycle,
>       //             m_n_active_cta );
>       if( m_n_active_cta == 0 ) {
>           assert( m_kernel != NULL );
>           m_kernel->dec_running();
>           printf("GPGPU-Sim uArch: Shader %u empty (release kernel %u \'%s\').\n", m_sid, m_kernel->get_uid(),
>                  m_kernel->name().c_str() );
>           if( m_kernel->no_more_ctas_to_run() ) {
>               if( !m_kernel->running() ) {
>                   printf("GPGPU-Sim uArch: GPU detected kernel \'%s\' finished on shader %u.\n", m_kernel->name().c_str(), m_sid );
>                   m_gpu->set_kernel_done( m_kernel );
>               }
>           }
>           m_kernel=NULL;
>           fflush(stdout);
1936,1939c1938
<       m_kernel = NULL;
<       fflush(stdout);
<     }
<   }
---
>    }
1942c1941
< void gpgpu_sim::shader_print_runtime_stat( FILE *fout )
---
> void gpgpu_sim::shader_print_runtime_stat( FILE *fout ) 
1944,1962c1943,1961
<   /*
<   fprintf(fout, "SHD_INSN: ");
<   for (unsigned i=0;i<m_n_shader;i++)
<     fprintf(fout, "%u ",m_sc[i]->get_num_sim_insn());
<   fprintf(fout, "\n");
<   fprintf(fout, "SHD_THDS: ");
<   for (unsigned i=0;i<m_n_shader;i++)
<     fprintf(fout, "%u ",m_sc[i]->get_not_completed());
<   fprintf(fout, "\n");
<   fprintf(fout, "SHD_DIVG: ");
<   for (unsigned i=0;i<m_n_shader;i++)
<     fprintf(fout, "%u ",m_sc[i]->get_n_diverge());
<   fprintf(fout, "\n");
< 
<   fprintf(fout, "THD_INSN: ");
<   for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++)
<     fprintf(fout, "%d ", m_sc[0]->get_thread_n_insn(i) );
<   fprintf(fout, "\n");
<   */
---
>     /*
>    fprintf(fout, "SHD_INSN: ");
>    for (unsigned i=0;i<m_n_shader;i++) 
>       fprintf(fout, "%u ",m_sc[i]->get_num_sim_insn());
>    fprintf(fout, "\n");
>    fprintf(fout, "SHD_THDS: ");
>    for (unsigned i=0;i<m_n_shader;i++) 
>       fprintf(fout, "%u ",m_sc[i]->get_not_completed());
>    fprintf(fout, "\n");
>    fprintf(fout, "SHD_DIVG: ");
>    for (unsigned i=0;i<m_n_shader;i++) 
>       fprintf(fout, "%u ",m_sc[i]->get_n_diverge());
>    fprintf(fout, "\n");
> 
>    fprintf(fout, "THD_INSN: ");
>    for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) 
>       fprintf(fout, "%d ", m_sc[0]->get_thread_n_insn(i) );
>    fprintf(fout, "\n");
>    */
1968,2003c1967,2026
<   // Print out the stats from the sampling shader core
<   const unsigned scheduler_sampling_core = m_shader_config->gpgpu_warp_issue_shader;
< #define STR_SIZE 55
<   char name_buff[ STR_SIZE ];
<   name_buff[ STR_SIZE - 1 ] = '\0';
<   const std::vector< unsigned >& distro
<     = print_dynamic_info ?
<       m_shader_stats->get_dynamic_warp_issue()[ scheduler_sampling_core ] :
<       m_shader_stats->get_warp_slot_issue()[ scheduler_sampling_core ];
<   if ( print_dynamic_info ) {
<     snprintf( name_buff, STR_SIZE - 1, "dynamic_warp_id" );
<   } else {
<     snprintf( name_buff, STR_SIZE - 1, "warp_id" );
<   }
<   fprintf( fout,
<            "Shader %d %s issue ditsribution:\n",
<            scheduler_sampling_core,
<            name_buff );
<   const unsigned num_warp_ids = distro.size();
<   // First print out the warp ids
<   fprintf( fout, "%s:\n", name_buff );
<   for ( unsigned warp_id = 0;
<         warp_id < num_warp_ids;
<         ++warp_id  ) {
<     fprintf( fout, "%d, ", warp_id );
<   }
< 
<   fprintf( fout, "\ndistro:\n" );
<   // Then print out the distribution of instuctions issued
<   for ( std::vector< unsigned >::const_iterator iter = distro.begin();
<         iter != distro.end();
<         iter++ ) {
<     fprintf( fout, "%d, ", *iter );
<   }
<   fprintf( fout, "\n" );
< }
---
>     // Print out the stats from the sampling shader core
>     const unsigned scheduler_sampling_core = m_shader_config->gpgpu_warp_issue_shader;
>     #define STR_SIZE 55
>     char name_buff[ STR_SIZE ];
>     name_buff[ STR_SIZE - 1 ] = '\0';
>     const std::vector< unsigned >& distro
>         = print_dynamic_info ?
>           m_shader_stats->get_dynamic_warp_issue()[ scheduler_sampling_core ] :
>           m_shader_stats->get_warp_slot_issue()[ scheduler_sampling_core ];
>     if ( print_dynamic_info ) {
>         snprintf( name_buff, STR_SIZE - 1, "dynamic_warp_id" );
>     } else {
>         snprintf( name_buff, STR_SIZE - 1, "warp_id" );
>     }
>     fprintf( fout,
>              "Shader %d %s issue ditsribution:\n",
>              scheduler_sampling_core,
>              name_buff );
>     const unsigned num_warp_ids = distro.size();
>     // First print out the warp ids
>     fprintf( fout, "%s:\n", name_buff );
>     for ( unsigned warp_id = 0;
>           warp_id < num_warp_ids;
>           ++warp_id  ) {
>         fprintf( fout, "%d, ", warp_id );
>     }
> 
>     fprintf( fout, "\ndistro:\n" );
>     // Then print out the distribution of instuctions issued
>     for ( std::vector< unsigned >::const_iterator iter = distro.begin();
>           iter != distro.end();
>           iter++ ) {
>         fprintf( fout, "%d, ", *iter );
>     }
>     fprintf( fout, "\n" );
> }
> 
> void gpgpu_sim::shader_print_cache_stats( FILE *fout ) const{
> 
>     // L1I
>     struct cache_sub_stats total_css;
>     struct cache_sub_stats css;
> 
>     if(!m_shader_config->m_L1I_config.disabled()){
>         total_css.clear();
>         css.clear();
>         fprintf(fout, "\n========= Core cache stats =========\n");
>         fprintf(fout, "L1I_cache:\n");
>         for ( unsigned i = 0; i < m_shader_config->n_simt_clusters; ++i ) {
>             m_cluster[i]->get_L1I_sub_stats(css);
>             total_css += css;
>         }
>         fprintf(fout, "\tL1I_total_cache_accesses = %u\n", total_css.accesses);
>         fprintf(fout, "\tL1I_total_cache_misses = %u\n", total_css.misses);
>         if(total_css.accesses > 0){
>             fprintf(fout, "\tL1I_total_cache_miss_rate = %.4lf\n", (double)total_css.misses / (double)total_css.accesses);
>         }
>         fprintf(fout, "\tL1I_total_cache_pending_hits = %u\n", total_css.pending_hits);
>         fprintf(fout, "\tL1I_total_cache_reservation_fails = %u\n", total_css.res_fails);
>     }
2005c2028,2034
< void gpgpu_sim::shader_print_cache_stats( FILE *fout ) const {
---
>     // L1D
>     if(!m_shader_config->m_L1D_config.disabled()){
>         total_css.clear();
>         css.clear();
>         fprintf(fout, "L1D_cache:\n");
>         for (unsigned i=0;i<m_shader_config->n_simt_clusters;i++){
>             m_cluster[i]->get_L1D_sub_stats(css);
2007,2009c2036,2037
<   // L1I
<   struct cache_sub_stats total_css;
<   struct cache_sub_stats css;
---
>             fprintf( stdout, "\tL1D_cache_core[%d]: Access = %d, Miss = %d, Miss_rate = %.3lf, Pending_hits = %u, Reservation_fails = %u\n",
>                      i, css.accesses, css.misses, (double)css.misses / (double)css.accesses, css.pending_hits, css.res_fails);
2011,2023c2039,2048
<   if (!m_shader_config->m_L1I_config.disabled()) {
<     total_css.clear();
<     css.clear();
<     fprintf(fout, "\n========= Core cache stats =========\n");
<     fprintf(fout, "L1I_cache:\n");
<     for ( unsigned i = 0; i < m_shader_config->n_simt_clusters; ++i ) {
<       m_cluster[i]->get_L1I_sub_stats(css);
<       total_css += css;
<     }
<     fprintf(fout, "\tL1I_total_cache_accesses = %u\n", total_css.accesses);
<     fprintf(fout, "\tL1I_total_cache_misses = %u\n", total_css.misses);
<     if (total_css.accesses > 0) {
<       fprintf(fout, "\tL1I_total_cache_miss_rate = %.4lf\n", (double)total_css.misses / (double)total_css.accesses);
---
>             total_css += css;
>         }
>         fprintf(fout, "\tL1D_total_cache_accesses = %u\n", total_css.accesses);
>         fprintf(fout, "\tL1D_total_cache_misses = %u\n", total_css.misses);
>         if(total_css.accesses > 0){
>             fprintf(fout, "\tL1D_total_cache_miss_rate = %.4lf\n", (double)total_css.misses / (double)total_css.accesses);
>         }
>         fprintf(fout, "\tL1D_total_cache_pending_hits = %u\n", total_css.pending_hits);
>         fprintf(fout, "\tL1D_total_cache_reservation_fails = %u\n", total_css.res_fails);
>         total_css.print_port_stats(fout, "\tL1D_cache"); 
2025,2027d2049
<     fprintf(fout, "\tL1I_total_cache_pending_hits = %u\n", total_css.pending_hits);
<     fprintf(fout, "\tL1I_total_cache_reservation_fails = %u\n", total_css.res_fails);
<   }
2029,2064c2051,2066
<   // L1D
<   if (!m_shader_config->m_L1D_config.disabled()) {
<     total_css.clear();
<     css.clear();
<     fprintf(fout, "L1D_cache:\n");
<     for (unsigned i = 0; i < m_shader_config->n_simt_clusters; i++) {
<       m_cluster[i]->get_L1D_sub_stats(css);
< 
<       fprintf( stdout, "\tL1D_cache_core[%d]: Access = %d, Miss = %d, Miss_rate = %.3lf, Pending_hits = %u, Reservation_fails = %u\n",
<                i, css.accesses, css.misses, (double)css.misses / (double)css.accesses, css.pending_hits, css.res_fails);
< 
<       total_css += css;
<     }
<     fprintf(fout, "\tL1D_total_cache_accesses = %u\n", total_css.accesses);
<     fprintf(fout, "\tL1D_total_cache_misses = %u\n", total_css.misses);
<     if (total_css.accesses > 0) {
<       fprintf(fout, "\tL1D_total_cache_miss_rate = %.4lf\n", (double)total_css.misses / (double)total_css.accesses);
<     }
<     fprintf(fout, "\tL1D_total_cache_pending_hits = %u\n", total_css.pending_hits);
<     fprintf(fout, "\tL1D_total_cache_reservation_fails = %u\n", total_css.res_fails);
<     total_css.print_port_stats(fout, "\tL1D_cache");
<   }
< 
<   // L1C
<   if (!m_shader_config->m_L1C_config.disabled()) {
<     total_css.clear();
<     css.clear();
<     fprintf(fout, "L1C_cache:\n");
<     for ( unsigned i = 0; i < m_shader_config->n_simt_clusters; ++i ) {
<       m_cluster[i]->get_L1C_sub_stats(css);
<       total_css += css;
<     }
<     fprintf(fout, "\tL1C_total_cache_accesses = %u\n", total_css.accesses);
<     fprintf(fout, "\tL1C_total_cache_misses = %u\n", total_css.misses);
<     if (total_css.accesses > 0) {
<       fprintf(fout, "\tL1C_total_cache_miss_rate = %.4lf\n", (double)total_css.misses / (double)total_css.accesses);
---
>     // L1C
>     if(!m_shader_config->m_L1C_config.disabled()){
>         total_css.clear();
>         css.clear();
>         fprintf(fout, "L1C_cache:\n");
>         for ( unsigned i = 0; i < m_shader_config->n_simt_clusters; ++i ) {
>             m_cluster[i]->get_L1C_sub_stats(css);
>             total_css += css;
>         }
>         fprintf(fout, "\tL1C_total_cache_accesses = %u\n", total_css.accesses);
>         fprintf(fout, "\tL1C_total_cache_misses = %u\n", total_css.misses);
>         if(total_css.accesses > 0){
>             fprintf(fout, "\tL1C_total_cache_miss_rate = %.4lf\n", (double)total_css.misses / (double)total_css.accesses);
>         }
>         fprintf(fout, "\tL1C_total_cache_pending_hits = %u\n", total_css.pending_hits);
>         fprintf(fout, "\tL1C_total_cache_reservation_fails = %u\n", total_css.res_fails);
2066,2068d2067
<     fprintf(fout, "\tL1C_total_cache_pending_hits = %u\n", total_css.pending_hits);
<     fprintf(fout, "\tL1C_total_cache_reservation_fails = %u\n", total_css.res_fails);
<   }
2070,2082c2069,2084
<   // L1T
<   if (!m_shader_config->m_L1T_config.disabled()) {
<     total_css.clear();
<     css.clear();
<     fprintf(fout, "L1T_cache:\n");
<     for ( unsigned i = 0; i < m_shader_config->n_simt_clusters; ++i ) {
<       m_cluster[i]->get_L1T_sub_stats(css);
<       total_css += css;
<     }
<     fprintf(fout, "\tL1T_total_cache_accesses = %u\n", total_css.accesses);
<     fprintf(fout, "\tL1T_total_cache_misses = %u\n", total_css.misses);
<     if (total_css.accesses > 0) {
<       fprintf(fout, "\tL1T_total_cache_miss_rate = %.4lf\n", (double)total_css.misses / (double)total_css.accesses);
---
>     // L1T
>     if(!m_shader_config->m_L1T_config.disabled()){
>         total_css.clear();
>         css.clear();
>         fprintf(fout, "L1T_cache:\n");
>         for ( unsigned i = 0; i < m_shader_config->n_simt_clusters; ++i ) {
>             m_cluster[i]->get_L1T_sub_stats(css);
>             total_css += css;
>         }
>         fprintf(fout, "\tL1T_total_cache_accesses = %u\n", total_css.accesses);
>         fprintf(fout, "\tL1T_total_cache_misses = %u\n", total_css.misses);
>         if(total_css.accesses > 0){
>             fprintf(fout, "\tL1T_total_cache_miss_rate = %.4lf\n", (double)total_css.misses / (double)total_css.accesses);
>         }
>         fprintf(fout, "\tL1T_total_cache_pending_hits = %u\n", total_css.pending_hits);
>         fprintf(fout, "\tL1T_total_cache_reservation_fails = %u\n", total_css.res_fails);
2084,2086d2085
<     fprintf(fout, "\tL1T_total_cache_pending_hits = %u\n", total_css.pending_hits);
<     fprintf(fout, "\tL1T_total_cache_reservation_fails = %u\n", total_css.res_fails);
<   }
2091,2150c2090,2149
<   unsigned total_d1_misses = 0, total_d1_accesses = 0;
<   for ( unsigned i = 0; i < m_shader_config->n_simt_clusters; ++i ) {
<     unsigned custer_d1_misses = 0, cluster_d1_accesses = 0;
<     m_cluster[ i ]->print_cache_stats( fout, cluster_d1_accesses, custer_d1_misses );
<     total_d1_misses += custer_d1_misses;
<     total_d1_accesses += cluster_d1_accesses;
<   }
<   fprintf( fout, "total_dl1_misses=%d\n", total_d1_misses );
<   fprintf( fout, "total_dl1_accesses=%d\n", total_d1_accesses );
<   fprintf( fout, "total_dl1_miss_rate= %f\n", (float)total_d1_misses / (float)total_d1_accesses );
<   /*
<   fprintf(fout, "THD_INSN_AC: ");
<   for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++)
<      fprintf(fout, "%d ", m_sc[0]->get_thread_n_insn_ac(i));
<   fprintf(fout, "\n");
<   fprintf(fout, "T_L1_Mss: "); //l1 miss rate per thread
<   for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++)
<      fprintf(fout, "%d ", m_sc[0]->get_thread_n_l1_mis_ac(i));
<   fprintf(fout, "\n");
<   fprintf(fout, "T_L1_Mgs: "); //l1 merged miss rate per thread
<   for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++)
<      fprintf(fout, "%d ", m_sc[0]->get_thread_n_l1_mis_ac(i) - m_sc[0]->get_thread_n_l1_mrghit_ac(i));
<   fprintf(fout, "\n");
<   fprintf(fout, "T_L1_Acc: "); //l1 access per thread
<   for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++)
<      fprintf(fout, "%d ", m_sc[0]->get_thread_n_l1_access_ac(i));
<   fprintf(fout, "\n");
< 
<   //per warp
<   int temp =0;
<   fprintf(fout, "W_L1_Mss: "); //l1 miss rate per warp
<   for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) {
<      temp += m_sc[0]->get_thread_n_l1_mis_ac(i);
<      if (i%m_shader_config->warp_size == (unsigned)(m_shader_config->warp_size-1)) {
<         fprintf(fout, "%d ", temp);
<         temp = 0;
<      }
<   }
<   fprintf(fout, "\n");
<   temp=0;
<   fprintf(fout, "W_L1_Mgs: "); //l1 merged miss rate per warp
<   for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) {
<      temp += (m_sc[0]->get_thread_n_l1_mis_ac(i) - m_sc[0]->get_thread_n_l1_mrghit_ac(i) );
<      if (i%m_shader_config->warp_size == (unsigned)(m_shader_config->warp_size-1)) {
<         fprintf(fout, "%d ", temp);
<         temp = 0;
<      }
<   }
<   fprintf(fout, "\n");
<   temp =0;
<   fprintf(fout, "W_L1_Acc: "); //l1 access per warp
<   for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) {
<      temp += m_sc[0]->get_thread_n_l1_access_ac(i);
<      if (i%m_shader_config->warp_size == (unsigned)(m_shader_config->warp_size-1)) {
<         fprintf(fout, "%d ", temp);
<         temp = 0;
<      }
<   }
<   fprintf(fout, "\n");
<   */
---
>    unsigned total_d1_misses = 0, total_d1_accesses = 0;
>    for ( unsigned i = 0; i < m_shader_config->n_simt_clusters; ++i ) {
>          unsigned custer_d1_misses = 0, cluster_d1_accesses = 0;
>          m_cluster[ i ]->print_cache_stats( fout, cluster_d1_accesses, custer_d1_misses );
>          total_d1_misses += custer_d1_misses;
>          total_d1_accesses += cluster_d1_accesses;
>    }
>    fprintf( fout, "total_dl1_misses=%d\n", total_d1_misses );
>    fprintf( fout, "total_dl1_accesses=%d\n", total_d1_accesses );
>    fprintf( fout, "total_dl1_miss_rate= %f\n", (float)total_d1_misses / (float)total_d1_accesses );
>    /*
>    fprintf(fout, "THD_INSN_AC: ");
>    for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) 
>       fprintf(fout, "%d ", m_sc[0]->get_thread_n_insn_ac(i));
>    fprintf(fout, "\n");
>    fprintf(fout, "T_L1_Mss: "); //l1 miss rate per thread
>    for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) 
>       fprintf(fout, "%d ", m_sc[0]->get_thread_n_l1_mis_ac(i));
>    fprintf(fout, "\n");
>    fprintf(fout, "T_L1_Mgs: "); //l1 merged miss rate per thread
>    for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) 
>       fprintf(fout, "%d ", m_sc[0]->get_thread_n_l1_mis_ac(i) - m_sc[0]->get_thread_n_l1_mrghit_ac(i));
>    fprintf(fout, "\n");
>    fprintf(fout, "T_L1_Acc: "); //l1 access per thread
>    for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) 
>       fprintf(fout, "%d ", m_sc[0]->get_thread_n_l1_access_ac(i));
>    fprintf(fout, "\n");
> 
>    //per warp
>    int temp =0; 
>    fprintf(fout, "W_L1_Mss: "); //l1 miss rate per warp
>    for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) {
>       temp += m_sc[0]->get_thread_n_l1_mis_ac(i);
>       if (i%m_shader_config->warp_size == (unsigned)(m_shader_config->warp_size-1)) {
>          fprintf(fout, "%d ", temp);
>          temp = 0;
>       }
>    }
>    fprintf(fout, "\n");
>    temp=0;
>    fprintf(fout, "W_L1_Mgs: "); //l1 merged miss rate per warp
>    for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) {
>       temp += (m_sc[0]->get_thread_n_l1_mis_ac(i) - m_sc[0]->get_thread_n_l1_mrghit_ac(i) );
>       if (i%m_shader_config->warp_size == (unsigned)(m_shader_config->warp_size-1)) {
>          fprintf(fout, "%d ", temp);
>          temp = 0;
>       }
>    }
>    fprintf(fout, "\n");
>    temp =0;
>    fprintf(fout, "W_L1_Acc: "); //l1 access per warp
>    for (unsigned i=0; i<m_shader_config->n_thread_per_shader; i++) {
>       temp += m_sc[0]->get_thread_n_l1_access_ac(i);
>       if (i%m_shader_config->warp_size == (unsigned)(m_shader_config->warp_size-1)) {
>          fprintf(fout, "%d ", temp);
>          temp = 0;
>       }
>    }
>    fprintf(fout, "\n");
>    */
2155,2165c2154,2164
<   if (empty() ) {
<     fprintf(fout, "bubble\n" );
<     return;
<   } else
<     fprintf(fout, "0x%04x ", pc );
<   fprintf(fout, "w%02d[", m_warp_id);
<   for (unsigned j = 0; j < m_config->warp_size; j++)
<     fprintf(fout, "%c", (active(j) ? '1' : '0') );
<   fprintf(fout, "]: ");
<   ptx_print_insn( pc, fout );
<   fprintf(fout, "\n");
---
>     if (empty() ) {
>         fprintf(fout,"bubble\n" );
>         return;
>     } else 
>         fprintf(fout,"0x%04x ", pc );
>     fprintf(fout, "w%02d[", m_warp_id);
>     for (unsigned j=0; j<m_config->warp_size; j++)
>         fprintf(fout, "%c", (active(j)?'1':'0') );
>     fprintf(fout, "]: ");
>     ptx_print_insn( pc, fout );
>     fprintf(fout, "\n");
2169,2170c2168,2169
<   if (inst->mem_op == TEX)
<     inctex_stat(inst->active_count(), 1);
---
> 	if(inst->mem_op==TEX)
> 		inctex_stat(inst->active_count(),1);
2172,2215c2171,2214
<   // Latency numbers for next operations are used to scale the power values
<   // for special operations, according observations from microbenchmarking
<   // TODO: put these numbers in the xml configuration
< 
<   switch (inst->sp_op) {
<   case INT__OP:
<     incialu_stat(inst->active_count(), 25);
<     break;
<   case INT_MUL_OP:
<     incimul_stat(inst->active_count(), 7.2);
<     break;
<   case INT_MUL24_OP:
<     incimul24_stat(inst->active_count(), 4.2);
<     break;
<   case INT_MUL32_OP:
<     incimul32_stat(inst->active_count(), 4);
<     break;
<   case INT_DIV_OP:
<     incidiv_stat(inst->active_count(), 40);
<     break;
<   case FP__OP:
<     incfpalu_stat(inst->active_count(), 1);
<     break;
<   case FP_MUL_OP:
<     incfpmul_stat(inst->active_count(), 1.8);
<     break;
<   case FP_DIV_OP:
<     incfpdiv_stat(inst->active_count(), 48);
<     break;
<   case FP_SQRT_OP:
<     inctrans_stat(inst->active_count(), 25);
<     break;
<   case FP_LG_OP:
<     inctrans_stat(inst->active_count(), 35);
<     break;
<   case FP_SIN_OP:
<     inctrans_stat(inst->active_count(), 12);
<     break;
<   case FP_EXP_OP:
<     inctrans_stat(inst->active_count(), 35);
<     break;
<   default:
<     break;
<   }
---
>     // Latency numbers for next operations are used to scale the power values
>     // for special operations, according observations from microbenchmarking
>     // TODO: put these numbers in the xml configuration
> 
> 	switch(inst->sp_op){
> 	case INT__OP:
> 		incialu_stat(inst->active_count(),25);
> 		break;
> 	case INT_MUL_OP:
> 		incimul_stat(inst->active_count(),7.2);
> 		break;
> 	case INT_MUL24_OP:
> 		incimul24_stat(inst->active_count(),4.2);
> 		break;
> 	case INT_MUL32_OP:
> 		incimul32_stat(inst->active_count(),4);
> 		break;
> 	case INT_DIV_OP:
> 		incidiv_stat(inst->active_count(),40);
> 		break;
> 	case FP__OP:
> 		incfpalu_stat(inst->active_count(),1);
> 		break;
> 	case FP_MUL_OP:
> 		incfpmul_stat(inst->active_count(),1.8);
> 		break;
> 	case FP_DIV_OP:
> 		incfpdiv_stat(inst->active_count(),48);
> 		break;
> 	case FP_SQRT_OP:
> 		inctrans_stat(inst->active_count(),25);
> 		break;
> 	case FP_LG_OP:
> 		inctrans_stat(inst->active_count(),35);
> 		break;
> 	case FP_SIN_OP:
> 		inctrans_stat(inst->active_count(),12);
> 		break;
> 	case FP_EXP_OP:
> 		inctrans_stat(inst->active_count(),35);
> 		break;
> 	default:
> 		break;
> 	}
2219,2220c2218,2219
<   m_pipeline_reg[stage].print(fout);
<   //m_pipeline_reg[stage].print(fout);
---
>    m_pipeline_reg[stage].print(fout);
>    //m_pipeline_reg[stage].print(fout);
2225,2237c2224,2238
<   if ( (mask & 4) && m_config->model == POST_DOMINATOR ) {
<     fprintf(fout, "per warp SIMT control-flow state:\n");
<     unsigned n = m_config->n_thread_per_shader / m_config->warp_size;
<     for (unsigned i = 0; i < n; i++) {
<       unsigned nactive = 0;
<       for (unsigned j = 0; j < m_config->warp_size; j++ ) {
<         unsigned tid = i * m_config->warp_size + j;
<         int done = ptx_thread_done(tid);
<         nactive += (ptx_thread_done(tid) ? 0 : 1);
<         if ( done && (mask & 8) ) {
<           unsigned done_cycle = m_thread[tid]->donecycle();
<           if ( done_cycle ) {
<             printf("\n w%02u:t%03u: done @ cycle %u", i, tid, done_cycle );
---
>     if ( (mask & 4) && m_config->model == POST_DOMINATOR ) {
>        fprintf(fout,"per warp SIMT control-flow state:\n");
>        unsigned n = m_config->n_thread_per_shader / m_config->warp_size;
>        for (unsigned i=0; i < n; i++) {
>           unsigned nactive = 0;
>           for (unsigned j=0; j<m_config->warp_size; j++ ) {
>              unsigned tid = i*m_config->warp_size + j;
>              int done = ptx_thread_done(tid);
>              nactive += (ptx_thread_done(tid)?0:1);
>              if ( done && (mask & 8) ) {
>                 unsigned done_cycle = m_thread[tid]->donecycle();
>                 if ( done_cycle ) {
>                    printf("\n w%02u:t%03u: done @ cycle %u", i, tid, done_cycle );
>                 }
>              }
2239,2244c2240,2245
<         }
<       }
<       if ( nactive == 0 ) {
<         continue;
<       }
<       m_simt_stack[i]->print(fout);
---
>           if ( nactive == 0 ) {
>              continue;
>           }
>           m_simt_stack[i]->print(fout);
>        }
>        fprintf(fout,"\n");
2246,2247d2246
<     fprintf(fout, "\n");
<   }
2252,2264c2251,2265
<   fprintf(fout, "LD/ST unit  = ");
<   m_dispatch_reg->print(fout);
<   if ( m_mem_rc != NO_RC_FAIL ) {
<     fprintf(fout, "              LD/ST stall condition: ");
<     switch ( m_mem_rc ) {
<     case BK_CONF:        fprintf(fout, "BK_CONF"); break;
<     case MSHR_RC_FAIL:   fprintf(fout, "MSHR_RC_FAIL"); break;
<     case ICNT_RC_FAIL:   fprintf(fout, "ICNT_RC_FAIL"); break;
<     case COAL_STALL:     fprintf(fout, "COAL_STALL"); break;
<     case WB_ICNT_RC_FAIL: fprintf(fout, "WB_ICNT_RC_FAIL"); break;
<     case WB_CACHE_RSRV_FAIL: fprintf(fout, "WB_CACHE_RSRV_FAIL"); break;
<     case N_MEM_STAGE_STALL_TYPE: fprintf(fout, "N_MEM_STAGE_STALL_TYPE"); break;
<     default: abort();
---
>     fprintf(fout,"LD/ST unit  = ");
>     m_dispatch_reg->print(fout);
>     if ( m_mem_rc != NO_RC_FAIL ) {
>         fprintf(fout,"              LD/ST stall condition: ");
>         switch ( m_mem_rc ) {
>         case BK_CONF:        fprintf(fout,"BK_CONF"); break;
>         case MSHR_RC_FAIL:   fprintf(fout,"MSHR_RC_FAIL"); break;
>         case ICNT_RC_FAIL:   fprintf(fout,"ICNT_RC_FAIL"); break;
>         case COAL_STALL:     fprintf(fout,"COAL_STALL"); break;
>         case WB_ICNT_RC_FAIL: fprintf(fout,"WB_ICNT_RC_FAIL"); break;
>         case WB_CACHE_RSRV_FAIL: fprintf(fout,"WB_CACHE_RSRV_FAIL"); break;
>         case N_MEM_STAGE_STALL_TYPE: fprintf(fout,"N_MEM_STAGE_STALL_TYPE"); break;
>         default: abort();
>         }
>         fprintf(fout,"\n");
2266,2282c2267,2292
<     fprintf(fout, "\n");
<   }
<   fprintf(fout, "LD/ST wb    = ");
<   m_next_wb.print(fout);
<   fprintf(fout, "Last LD/ST writeback @ %llu + %llu (gpu_sim_cycle+gpu_tot_sim_cycle)\n",
<           m_last_inst_gpu_sim_cycle, m_last_inst_gpu_tot_sim_cycle );
<   fprintf(fout, "Pending register writes:\n");
<   std::map<unsigned/*warp_id*/, std::map<unsigned/*regnum*/, unsigned/*count*/> >::const_iterator w;
<   for ( w = m_pending_writes.begin(); w != m_pending_writes.end(); w++ ) {
<     unsigned warp_id = w->first;
<     const std::map<unsigned/*regnum*/, unsigned/*count*/> &warp_info = w->second;
<     if ( warp_info.empty() )
<       continue;
<     fprintf(fout, "  w%2u : ", warp_id );
<     std::map<unsigned/*regnum*/, unsigned/*count*/>::const_iterator r;
<     for ( r = warp_info.begin(); r != warp_info.end(); ++r ) {
<       fprintf(fout, "  %u(%u)", r->first, r->second );
---
>     fprintf(fout,"LD/ST wb    = ");
>     m_next_wb.print(fout);
>     fprintf(fout, "Last LD/ST writeback @ %llu + %llu (gpu_sim_cycle+gpu_tot_sim_cycle)\n",
>                   m_last_inst_gpu_sim_cycle, m_last_inst_gpu_tot_sim_cycle );
>     fprintf(fout,"Pending register writes:\n");
>     std::map<unsigned/*warp_id*/, std::map<unsigned/*regnum*/,unsigned/*count*/> >::const_iterator w;
>     for( w=m_pending_writes.begin(); w!=m_pending_writes.end(); w++ ) {
>         unsigned warp_id = w->first;
>         const std::map<unsigned/*regnum*/,unsigned/*count*/> &warp_info = w->second;
>         if( warp_info.empty() ) 
>             continue;
>         fprintf(fout,"  w%2u : ", warp_id );
>         std::map<unsigned/*regnum*/,unsigned/*count*/>::const_iterator r;
>         for( r=warp_info.begin(); r!=warp_info.end(); ++r ) {
>             fprintf(fout,"  %u(%u)", r->first, r->second );
>         }
>         fprintf(fout,"\n");
>     }
>     m_L1C->display_state(fout);
>     m_L1T->display_state(fout);
>     if( !m_config->m_L1D_config.disabled() )
>     	m_L1D->display_state(fout);
>     fprintf(fout,"LD/ST response FIFO (occupancy = %zu):\n", m_response_fifo.size() );
>     for( std::list<mem_fetch*>::const_iterator i=m_response_fifo.begin(); i != m_response_fifo.end(); i++ ) {
>         const mem_fetch *mf = *i;
>         mf->print(fout);
2284,2294d2293
<     fprintf(fout, "\n");
<   }
<   m_L1C->display_state(fout);
<   m_L1T->display_state(fout);
<   if ( !m_config->m_L1D_config.disabled() )
<     m_L1D->display_state(fout);
<   fprintf(fout, "LD/ST response FIFO (occupancy = %zu):\n", m_response_fifo.size() );
<   for ( std::list<mem_fetch*>::const_iterator i = m_response_fifo.begin(); i != m_response_fifo.end(); i++ ) {
<     const mem_fetch *mf = *i;
<     mf->print(fout);
<   }
2299,2349c2298,2343
<   fprintf(fout, "=================================================\n");
<   fprintf(fout, "shader %u at cycle %Lu+%Lu (%u threads running)\n", m_sid,
<           gpu_tot_sim_cycle, gpu_sim_cycle, m_not_completed);
<   fprintf(fout, "=================================================\n");
< 
<   dump_warp_state(fout);
<   fprintf(fout, "\n");
< 
<   m_L1I->display_state(fout);
< 
<   fprintf(fout, "IF/ID       = ");
<   if ( !m_inst_fetch_buffer.m_valid )
<     fprintf(fout, "bubble\n");
<   else {
<     fprintf(fout, "w%2u : pc = 0x%x, nbytes = %u\n",
<             m_inst_fetch_buffer.m_warp_id,
<             m_inst_fetch_buffer.m_pc,
<             m_inst_fetch_buffer.m_nbytes );
<   }
<   fprintf(fout, "\nibuffer status:\n");
<   for ( unsigned i = 0; i < m_config->max_warps_per_shader; i++) {
<     if ( !m_warp[i].ibuffer_empty() )
<       m_warp[i].print_ibuffer(fout);
<   }
<   fprintf(fout, "\n");
<   display_simt_state(fout, mask);
<   fprintf(fout, "-------------------------- Scoreboard\n");
<   m_scoreboard->printContents();
<   /*
<      fprintf(fout,"ID/OC (SP)  = ");
<      print_stage(ID_OC_SP, fout);
<      fprintf(fout,"ID/OC (SFU) = ");
<      print_stage(ID_OC_SFU, fout);
<      fprintf(fout,"ID/OC (MEM) = ");
<      print_stage(ID_OC_MEM, fout);
<   */
<   fprintf(fout, "-------------------------- OP COL\n");
<   m_operand_collector.dump(fout);
<   /* fprintf(fout, "OC/EX (SP)  = ");
<      print_stage(OC_EX_SP, fout);
<      fprintf(fout, "OC/EX (SFU) = ");
<      print_stage(OC_EX_SFU, fout);
<      fprintf(fout, "OC/EX (MEM) = ");
<      print_stage(OC_EX_MEM, fout);
<   */
<   fprintf(fout, "-------------------------- Pipe Regs\n");
< 
<   for (unsigned i = 0; i < N_PIPELINE_STAGES; i++) {
<     fprintf(fout, "--- %s ---\n", pipeline_stage_name_decode[i]);
<     print_stage(i, fout); fprintf(fout, "\n");
<   }
---
>    fprintf(fout, "=================================================\n");
>    fprintf(fout, "shader %u at cycle %Lu+%Lu (%u threads running)\n", m_sid, 
>            gpu_tot_sim_cycle, gpu_sim_cycle, m_not_completed);
>    fprintf(fout, "=================================================\n");
> 
>    dump_warp_state(fout);
>    fprintf(fout,"\n");
> 
>    m_L1I->display_state(fout);
> 
>    fprintf(fout, "IF/ID       = ");
>    if( !m_inst_fetch_buffer.m_valid )
>        fprintf(fout,"bubble\n");
>    else {
>        fprintf(fout,"w%2u : pc = 0x%x, nbytes = %u\n", 
>                m_inst_fetch_buffer.m_warp_id,
>                m_inst_fetch_buffer.m_pc, 
>                m_inst_fetch_buffer.m_nbytes );
>    }
>    fprintf(fout,"\nibuffer status:\n");
>    for( unsigned i=0; i<m_config->max_warps_per_shader; i++) {
>        if( !m_warp[i].ibuffer_empty() ) 
>            m_warp[i].print_ibuffer(fout);
>    }
>    fprintf(fout,"\n");
>    display_simt_state(fout,mask);
>    fprintf(fout, "-------------------------- Scoreboard\n");
>    m_scoreboard->printContents();
> /*
>    fprintf(fout,"ID/OC (SP)  = ");
>    print_stage(ID_OC_SP, fout);
>    fprintf(fout,"ID/OC (SFU) = ");
>    print_stage(ID_OC_SFU, fout);
>    fprintf(fout,"ID/OC (MEM) = ");
>    print_stage(ID_OC_MEM, fout);
> */
>    fprintf(fout, "-------------------------- OP COL\n");
>    m_operand_collector.dump(fout);
> /* fprintf(fout, "OC/EX (SP)  = ");
>    print_stage(OC_EX_SP, fout);
>    fprintf(fout, "OC/EX (SFU) = ");
>    print_stage(OC_EX_SFU, fout);
>    fprintf(fout, "OC/EX (MEM) = ");
>    print_stage(OC_EX_MEM, fout);
> */
>    fprintf(fout, "-------------------------- Pipe Regs\n");
2351,2356c2345,2348
<   fprintf(fout, "-------------------------- Fu\n");
<   for ( unsigned n = 0; n < m_num_function_units; n++ ) {
<     m_fu[n]->print(fout);
<     fprintf(fout, "---------------\n");
<   }
<   fprintf(fout, "-------------------------- other:\n");
---
>    for (unsigned i = 0; i < N_PIPELINE_STAGES; i++) {
>        fprintf(fout,"--- %s ---\n",pipeline_stage_name_decode[i]);
>        print_stage(i,fout);fprintf(fout,"\n");
>    }
2358,2381c2350,2380
<   for (unsigned i = 0; i < num_result_bus; i++) {
<     std::string bits = m_result_bus[i]->to_string();
<     fprintf(fout, "EX/WB sched[%d]= %s\n", i, bits.c_str() );
<   }
<   fprintf(fout, "EX/WB      = ");
<   print_stage(EX_WB, fout);
<   fprintf(fout, "\n");
<   fprintf(fout, "Last EX/WB writeback @ %llu + %llu (gpu_sim_cycle+gpu_tot_sim_cycle)\n",
<           m_last_inst_gpu_sim_cycle, m_last_inst_gpu_tot_sim_cycle );
< 
<   if ( m_active_threads.count() <= 2 * m_config->warp_size ) {
<     fprintf(fout, "Active Threads : ");
<     unsigned last_warp_id = -1;
<     for (unsigned tid = 0; tid < m_active_threads.size(); tid++ ) {
<       unsigned warp_id = tid / m_config->warp_size;
<       if ( m_active_threads.test(tid) ) {
<         if ( warp_id != last_warp_id ) {
<           fprintf(fout, "\n  warp %u : ", warp_id );
<           last_warp_id = warp_id;
<         }
<         fprintf(fout, "%u ", tid );
<       }
<     }
<   }
---
>    fprintf(fout, "-------------------------- Fu\n");
>    for( unsigned n=0; n < m_num_function_units; n++ ){
>        m_fu[n]->print(fout);
>        fprintf(fout, "---------------\n");
>    }
>    fprintf(fout, "-------------------------- other:\n");
> 
>    for(unsigned i=0; i<num_result_bus; i++){
> 	   std::string bits = m_result_bus[i]->to_string();
> 	   fprintf(fout, "EX/WB sched[%d]= %s\n", i, bits.c_str() );
>    }
>    fprintf(fout, "EX/WB      = ");
>    print_stage(EX_WB, fout);
>    fprintf(fout, "\n");
>    fprintf(fout, "Last EX/WB writeback @ %llu + %llu (gpu_sim_cycle+gpu_tot_sim_cycle)\n",
>                  m_last_inst_gpu_sim_cycle, m_last_inst_gpu_tot_sim_cycle );
> 
>    if( m_active_threads.count() <= 2*m_config->warp_size ) {
>        fprintf(fout,"Active Threads : ");
>        unsigned last_warp_id = -1;
>        for(unsigned tid=0; tid < m_active_threads.size(); tid++ ) {
>            unsigned warp_id = tid/m_config->warp_size;
>            if( m_active_threads.test(tid) ) {
>                if( warp_id != last_warp_id ) {
>                    fprintf(fout,"\n  warp %u : ", warp_id );
>                    last_warp_id=warp_id;
>                }
>                fprintf(fout,"%u ", tid );
>            }
>        }
>    }
2387,2425c2386,2424
<   unsigned threads_per_cta  = k.threads_per_cta();
<   const class function_info *kernel = k.entry();
<   unsigned int padded_cta_size = threads_per_cta;
<   if (padded_cta_size % warp_size)
<     padded_cta_size = ((padded_cta_size / warp_size) + 1) * (warp_size);
< 
<   //Limit by n_threads/shader
<   unsigned int result_thread = n_thread_per_shader / padded_cta_size;
< 
<   const struct gpgpu_ptx_sim_kernel_info *kernel_info = ptx_sim_kernel_info(kernel);
< 
<   //Limit by shmem/shader
<   unsigned int result_shmem = (unsigned) - 1;
<   if (kernel_info->smem > 0)
<     result_shmem = gpgpu_shmem_size / kernel_info->smem;
< 
<   //Limit by register count, rounded up to multiple of 4.
<   unsigned int result_regs = (unsigned) - 1;
<   if (kernel_info->regs > 0)
<     result_regs = gpgpu_shader_registers / (padded_cta_size * ((kernel_info->regs + 3) & ~3));
< 
<   //Limit by CTA
<   unsigned int result_cta = max_cta_per_core;
< 
<   unsigned result = result_thread;
<   result = gs_min2(result, result_shmem);
<   result = gs_min2(result, result_regs);
<   result = gs_min2(result, result_cta);
< 
<   static const struct gpgpu_ptx_sim_kernel_info* last_kinfo = NULL;
<   if (last_kinfo != kernel_info) {   //Only print out stats if kernel_info struct changes
<     last_kinfo = kernel_info;
<     printf ("GPGPU-Sim uArch: CTA/core = %u, limited by:", result);
<     if (result == result_thread) printf (" threads");
<     if (result == result_shmem) printf (" shmem");
<     if (result == result_regs) printf (" regs");
<     if (result == result_cta) printf (" cta_limit");
<     printf ("\n");
<   }
---
>    unsigned threads_per_cta  = k.threads_per_cta();
>    const class function_info *kernel = k.entry();
>    unsigned int padded_cta_size = threads_per_cta;
>    if (padded_cta_size%warp_size) 
>       padded_cta_size = ((padded_cta_size/warp_size)+1)*(warp_size);
> 
>    //Limit by n_threads/shader
>    unsigned int result_thread = n_thread_per_shader / padded_cta_size;
> 
>    const struct gpgpu_ptx_sim_kernel_info *kernel_info = ptx_sim_kernel_info(kernel);
> 
>    //Limit by shmem/shader
>    unsigned int result_shmem = (unsigned)-1;
>    if (kernel_info->smem > 0)
>       result_shmem = gpgpu_shmem_size / kernel_info->smem;
> 
>    //Limit by register count, rounded up to multiple of 4.
>    unsigned int result_regs = (unsigned)-1;
>    if (kernel_info->regs > 0)
>       result_regs = gpgpu_shader_registers / (padded_cta_size * ((kernel_info->regs+3)&~3));
> 
>    //Limit by CTA
>    unsigned int result_cta = max_cta_per_core;
> 
>    unsigned result = result_thread;
>    result = gs_min2(result, result_shmem);
>    result = gs_min2(result, result_regs);
>    result = gs_min2(result, result_cta);
> 
>    static const struct gpgpu_ptx_sim_kernel_info* last_kinfo = NULL;
>    if (last_kinfo != kernel_info) {   //Only print out stats if kernel_info struct changes
>       last_kinfo = kernel_info;
>       printf ("GPGPU-Sim uArch: CTA/core = %u, limited by:", result);
>       if (result == result_thread) printf (" threads");
>       if (result == result_shmem) printf (" shmem");
>       if (result == result_regs) printf (" regs");
>       if (result == result_cta) printf (" cta_limit");
>       printf ("\n");
>    }
2427,2432c2426,2431
<   //gpu_max_cta_per_shader is limited by number of CTAs if not enough to keep all cores busy
<   if ( k.num_blocks() < result * num_shader() ) {
<     result = k.num_blocks() / num_shader();
<     if (k.num_blocks() % num_shader())
<       result++;
<   }
---
>     //gpu_max_cta_per_shader is limited by number of CTAs if not enough to keep all cores busy    
>     if( k.num_blocks() < result*num_shader() ) { 
>        result = k.num_blocks() / num_shader();
>        if (k.num_blocks() % num_shader())
>           result++;
>     }
2434,2438c2433,2437
<   assert( result <= MAX_CTA_PER_SHADER );
<   if (result < 1) {
<     printf ("GPGPU-Sim uArch: ERROR ** Kernel requires more resources than shader has.\n");
<     abort();
<   }
---
>     assert( result <= MAX_CTA_PER_SHADER );
>     if (result < 1) {
>        printf ("GPGPU-Sim uArch: ERROR ** Kernel requires more resources than shader has.\n");
>        abort();
>     }
2440c2439
<   return result;
---
>     return result;
2445,2451c2444,2450
<   m_stats->shader_cycles[m_sid]++;
<   writeback();
<   execute();
<   read_operands();
<   issue();
<   decode();
<   fetch();
---
> 	m_stats->shader_cycles[m_sid]++;
>     writeback();
>     execute();
>     read_operands();
>     issue();
>     decode();
>     fetch();
2458c2457
<   m_ldst_unit->flush();
---
>    m_ldst_unit->flush();
2462c2461
< std::list<opndcoll_rfu_t::op_t> opndcoll_rfu_t::arbiter_t::allocate_reads()
---
> std::list<opndcoll_rfu_t::op_t> opndcoll_rfu_t::arbiter_t::allocate_reads() 
2464,2497c2463
<   std::list<op_t> result;  // a list of registers that (a) are in different register banks, (b) do not go to the same operand collector
< 
<   int input;
<   int output;
<   int _inputs = m_num_banks;
<   int _outputs = m_num_collectors;
<   int _square = ( _inputs > _outputs ) ? _inputs : _outputs;
<   assert(_square > 0);
<   int _pri = (int)m_last_cu;
< 
<   // Clear matching
<   for ( int i = 0; i < _inputs; ++i )
<     _inmatch[i] = -1;
<   for ( int j = 0; j < _outputs; ++j )
<     _outmatch[j] = -1;
< 
<   for ( unsigned i = 0; i < m_num_banks; i++) {
<     for ( unsigned j = 0; j < m_num_collectors; j++) {
<       assert( i < (unsigned)_inputs );
<       assert( j < (unsigned)_outputs );
<       _request[i][j] = 0;
<     }
<     if ( !m_queue[i].empty() ) {
<       const op_t &op = m_queue[i].front();
<       int oc_id = op.get_oc_id();
<       assert( i < (unsigned)_inputs );
<       assert( oc_id < _outputs );
<       _request[i][oc_id] = 1;
<     }
<     if ( m_allocated_bank[i].is_write() ) {
<       assert( i < (unsigned)_inputs );
<       _inmatch[i] = 0; // write gets priority
<     }
<   }
---
>    std::list<op_t> result;  // a list of registers that (a) are in different register banks, (b) do not go to the same operand collector
2499,2501c2465,2496
<   ///// wavefront allocator from booksim... --->
< 
<   // Loop through diagonals of request matrix
---
>    int input;
>    int output;
>    int _inputs = m_num_banks;
>    int _outputs = m_num_collectors;
>    int _square = ( _inputs > _outputs ) ? _inputs : _outputs;
>    assert(_square > 0);
>    int _pri = (int)m_last_cu;
> 
>    // Clear matching
>    for ( int i = 0; i < _inputs; ++i ) 
>       _inmatch[i] = -1;
>    for ( int j = 0; j < _outputs; ++j ) 
>       _outmatch[j] = -1;
> 
>    for( unsigned i=0; i<m_num_banks; i++) {
>       for( unsigned j=0; j<m_num_collectors; j++) {
>          assert( i < (unsigned)_inputs );
>          assert( j < (unsigned)_outputs );
>          _request[i][j] = 0;
>       }
>       if( !m_queue[i].empty() ) {
>          const op_t &op = m_queue[i].front();
>          int oc_id = op.get_oc_id();
>          assert( i < (unsigned)_inputs );
>          assert( oc_id < _outputs );
>          _request[i][oc_id] = 1;
>       }
>       if( m_allocated_bank[i].is_write() ) {
>          assert( i < (unsigned)_inputs );
>          _inmatch[i] = 0; // write gets priority
>       }
>    }
2503,2504c2498,2516
<   for ( int p = 0; p < _square; ++p ) {
<     output = ( _pri + p ) % _square;
---
>    ///// wavefront allocator from booksim... --->
>    
>    // Loop through diagonals of request matrix
> 
>    for ( int p = 0; p < _square; ++p ) {
>       output = ( _pri + p ) % _square;
> 
>       // Step through the current diagonal
>       for ( input = 0; input < _inputs; ++input ) {
>           assert( input < _inputs );
>           assert( output < _outputs );
>          if ( ( output < _outputs ) && 
>               ( _inmatch[input] == -1 ) && 
>               ( _outmatch[output] == -1 ) &&
>               ( _request[input][output]/*.label != -1*/ ) ) {
>             // Grant!
>             _inmatch[input] = output;
>             _outmatch[output] = input;
>          }
2506,2516c2518
<     // Step through the current diagonal
<     for ( input = 0; input < _inputs; ++input ) {
<       assert( input < _inputs );
<       assert( output < _outputs );
<       if ( ( output < _outputs ) &&
<            ( _inmatch[input] == -1 ) &&
<            ( _outmatch[output] == -1 ) &&
<            ( _request[input][output]/*.label != -1*/ ) ) {
<         // Grant!
<         _inmatch[input] = output;
<         _outmatch[output] = input;
---
>          output = ( output + 1 ) % _square;
2517a2520
>    }
2519,2524c2522,2523
<       output = ( output + 1 ) % _square;
<     }
<   }
< 
<   // Round-robin the priority diagonal
<   _pri = ( _pri + 1 ) % _square;
---
>    // Round-robin the priority diagonal
>    _pri = ( _pri + 1 ) % _square;
2526c2525
<   /// <--- end code from booksim
---
>    /// <--- end code from booksim
2528,2535c2527,2535
<   m_last_cu = _pri;
<   for ( unsigned i = 0; i < m_num_banks; i++ ) {
<     if ( _inmatch[i] != -1 ) {
<       if ( !m_allocated_bank[i].is_write() ) {
<         unsigned bank = (unsigned)i;
<         op_t &op = m_queue[bank].front();
<         result.push_back(op);
<         m_queue[bank].pop_front();
---
>    m_last_cu = _pri;
>    for( unsigned i=0; i < m_num_banks; i++ ) {
>       if( _inmatch[i] != -1 ) {
>          if( !m_allocated_bank[i].is_write() ) {
>             unsigned bank = (unsigned)i;
>             op_t &op = m_queue[bank].front();
>             result.push_back(op);
>             m_queue[bank].pop_front();
>          }
2537,2538c2537
<     }
<   }
---
>    }
2540c2539
<   return result;
---
>    return result;
2543c2542
< barrier_set_t::barrier_set_t(shader_core_ctx *shader, unsigned max_warps_per_core, unsigned max_cta_per_core, unsigned max_barriers_per_cta, unsigned warp_size)
---
> barrier_set_t::barrier_set_t(shader_core_ctx *shader,unsigned max_warps_per_core, unsigned max_cta_per_core, unsigned max_barriers_per_cta, unsigned warp_size)
2545,2564c2544,2563
<   m_max_warps_per_core = max_warps_per_core;
<   m_max_cta_per_core = max_cta_per_core;
<   m_max_barriers_per_cta = max_barriers_per_cta;
<   m_warp_size = warp_size;
<   m_shader = shader;
<   if ( max_warps_per_core > WARP_PER_CTA_MAX ) {
<     printf("ERROR ** increase WARP_PER_CTA_MAX in shader.h from %u to >= %u or warps per cta in gpgpusim.config\n",
<            WARP_PER_CTA_MAX, max_warps_per_core );
<     exit(1);
<   }
<   if (max_barriers_per_cta > MAX_BARRIERS_PER_CTA) {
<     printf("ERROR ** increase MAX_BARRIERS_PER_CTA in abstract_hardware_model.h from %u to >= %u or barriers per cta in gpgpusim.config\n",
<            MAX_BARRIERS_PER_CTA, max_barriers_per_cta );
<     exit(1);
<   }
<   m_warp_active.reset();
<   m_warp_at_barrier.reset();
<   for (unsigned i = 0; i < max_barriers_per_cta; i++) {
<     m_bar_id_to_warps[i].reset();
<   }
---
>    m_max_warps_per_core = max_warps_per_core;
>    m_max_cta_per_core = max_cta_per_core;
>    m_max_barriers_per_cta = max_barriers_per_cta;
>    m_warp_size = warp_size;
>    m_shader = shader;
>    if( max_warps_per_core > WARP_PER_CTA_MAX ) {
>       printf("ERROR ** increase WARP_PER_CTA_MAX in shader.h from %u to >= %u or warps per cta in gpgpusim.config\n",
>              WARP_PER_CTA_MAX, max_warps_per_core );
>       exit(1);
>    }
>    if(max_barriers_per_cta > MAX_BARRIERS_PER_CTA){
> 	   printf("ERROR ** increase MAX_BARRIERS_PER_CTA in abstract_hardware_model.h from %u to >= %u or barriers per cta in gpgpusim.config\n",
> 			   MAX_BARRIERS_PER_CTA, max_barriers_per_cta );
> 	   exit(1);
>    }
>    m_warp_active.reset();
>    m_warp_at_barrier.reset();
>    for(unsigned i=0; i<max_barriers_per_cta; i++){
> 	   m_bar_id_to_warps[i].reset();
>    }
2570,2580c2569,2579
<   assert( cta_id < m_max_cta_per_core );
<   cta_to_warp_t::iterator w = m_cta_to_warps.find(cta_id);
<   assert( w == m_cta_to_warps.end() ); // cta should not already be active or allocated barrier resources
<   m_cta_to_warps[cta_id] = warps;
<   assert( m_cta_to_warps.size() <= m_max_cta_per_core ); // catch cta's that were not properly deallocated
< 
<   m_warp_active |= warps;
<   m_warp_at_barrier &= ~warps;
<   for (unsigned i = 0; i < m_max_barriers_per_cta; i++) {
<     m_bar_id_to_warps[i] &= ~warps;
<   }
---
>    assert( cta_id < m_max_cta_per_core );
>    cta_to_warp_t::iterator w=m_cta_to_warps.find(cta_id);
>    assert( w == m_cta_to_warps.end() ); // cta should not already be active or allocated barrier resources
>    m_cta_to_warps[cta_id] = warps;
>    assert( m_cta_to_warps.size() <= m_max_cta_per_core ); // catch cta's that were not properly deallocated
>   
>    m_warp_active |= warps;
>    m_warp_at_barrier &= ~warps;
>    for(unsigned i=0; i<m_max_barriers_per_cta; i++){
> 	   m_bar_id_to_warps[i] &=~warps;
>    }
2587,2603c2586,2602
<   cta_to_warp_t::iterator w = m_cta_to_warps.find(cta_id);
<   if ( w == m_cta_to_warps.end() )
<     return;
<   warp_set_t warps = w->second;
<   warp_set_t at_barrier = warps & m_warp_at_barrier;
<   assert( at_barrier.any() == false ); // no warps stuck at barrier
<   warp_set_t active = warps & m_warp_active;
<   assert( active.any() == false ); // no warps in CTA still running
<   m_warp_active &= ~warps;
<   m_warp_at_barrier &= ~warps;
< 
<   for (unsigned i = 0; i < m_max_barriers_per_cta; i++) {
<     warp_set_t at_a_specific_barrier = warps & m_bar_id_to_warps[i];
<     assert( at_a_specific_barrier.any() == false ); // no warps stuck at barrier
<     m_bar_id_to_warps[i] &= ~warps;
<   }
<   m_cta_to_warps.erase(w);
---
>    cta_to_warp_t::iterator w=m_cta_to_warps.find(cta_id);
>    if( w == m_cta_to_warps.end() )
>       return;
>    warp_set_t warps = w->second;
>    warp_set_t at_barrier = warps & m_warp_at_barrier;
>    assert( at_barrier.any() == false ); // no warps stuck at barrier
>    warp_set_t active = warps & m_warp_active;
>    assert( active.any() == false ); // no warps in CTA still running
>    m_warp_active &= ~warps;
>    m_warp_at_barrier &= ~warps;
> 
>    for(unsigned i=0; i<m_max_barriers_per_cta; i++){
> 	   warp_set_t at_a_specific_barrier = warps & m_bar_id_to_warps[i];
> 	   assert( at_a_specific_barrier.any() == false ); // no warps stuck at barrier
> 	   m_bar_id_to_warps[i] &=~warps;
>    }
>    m_cta_to_warps.erase(w);
2607c2606
< void barrier_set_t::warp_reaches_barrier(unsigned cta_id, unsigned warp_id, warp_inst_t* inst)
---
> void barrier_set_t::warp_reaches_barrier(unsigned cta_id,unsigned warp_id,warp_inst_t* inst)
2609,2620c2608,2619
<   barrier_type bar_type = inst->bar_type;
<   unsigned bar_id = inst->bar_id;
<   unsigned bar_count = inst->bar_count;
<   assert(bar_id != (unsigned) - 1);
<   cta_to_warp_t::iterator w = m_cta_to_warps.find(cta_id);
< 
<   if ( w == m_cta_to_warps.end() ) { // cta is active
<     printf("ERROR ** cta_id %u not found in barrier set on cycle %llu+%llu...\n", cta_id, gpu_tot_sim_cycle, gpu_sim_cycle );
<     dump();
<     abort();
<   }
<   assert( w->second.test(warp_id) == true ); // warp is in cta
---
> 	barrier_type bar_type = inst->bar_type;
> 	unsigned bar_id = inst->bar_id;
> 	unsigned bar_count = inst->bar_count;
> 	assert(bar_id!=(unsigned)-1);
>    cta_to_warp_t::iterator w=m_cta_to_warps.find(cta_id);
> 
>    if( w == m_cta_to_warps.end() ) { // cta is active
>       printf("ERROR ** cta_id %u not found in barrier set on cycle %llu+%llu...\n", cta_id, gpu_tot_sim_cycle, gpu_sim_cycle );
>       dump();
>       abort();
>    }
>    assert( w->second.test(warp_id) == true ); // warp is in cta
2622,2647c2621,2646
<   m_bar_id_to_warps[bar_id].set(warp_id);
<   if (bar_type == SYNC || bar_type == RED) {
<     m_warp_at_barrier.set(warp_id);
<   }
<   warp_set_t warps_in_cta = w->second;
<   warp_set_t at_barrier = warps_in_cta & m_bar_id_to_warps[bar_id];
<   warp_set_t active = warps_in_cta & m_warp_active;
<   if (bar_count == (unsigned) - 1) {
<     if ( at_barrier == active ) {
<       // all warps have reached barrier, so release waiting warps...
<       m_bar_id_to_warps[bar_id] &= ~at_barrier;
<       m_warp_at_barrier &= ~at_barrier;
<       if (bar_type == RED) {
<         m_shader->broadcast_barrier_reduction(cta_id, bar_id, at_barrier);
<       }
<     }
<   } else {
<     // TODO: check on the hardware if the count should include warp that exited
<     if ((at_barrier.count() * m_warp_size) == bar_count) {
<       // required number of warps have reached barrier, so release waiting warps...
<       m_bar_id_to_warps[bar_id] &= ~at_barrier;
<       m_warp_at_barrier &= ~at_barrier;
<       if (bar_type == RED) {
<         m_shader->broadcast_barrier_reduction(cta_id, bar_id, at_barrier);
<       }
<     }
---
>    m_bar_id_to_warps[bar_id].set(warp_id);
>    if(bar_type==SYNC || bar_type==RED){
> 	   m_warp_at_barrier.set(warp_id);
>    }
>    warp_set_t warps_in_cta = w->second;
>    warp_set_t at_barrier = warps_in_cta & m_bar_id_to_warps[bar_id];
>    warp_set_t active = warps_in_cta & m_warp_active;
>    if(bar_count==(unsigned)-1){
> 	   if( at_barrier == active ) {
> 		   // all warps have reached barrier, so release waiting warps...
> 		   m_bar_id_to_warps[bar_id] &= ~at_barrier;
> 		   m_warp_at_barrier &= ~at_barrier;
> 		   if(bar_type==RED){
> 			   m_shader->broadcast_barrier_reduction(cta_id, bar_id,at_barrier);
> 		   }
> 	   }
>   }else{
> 	  // TODO: check on the hardware if the count should include warp that exited
> 	  if ((at_barrier.count() * m_warp_size) == bar_count){
> 		   // required number of warps have reached barrier, so release waiting warps...
> 		   m_bar_id_to_warps[bar_id] &= ~at_barrier;
> 		   m_warp_at_barrier &= ~at_barrier;
> 		   if(bar_type==RED){
> 			   m_shader->broadcast_barrier_reduction(cta_id, bar_id,at_barrier);
> 		   }
> 	  }
2654c2653
< // warp reaches exit
---
> // warp reaches exit 
2657,2667c2656,2666
<   // caller needs to verify all threads in warp are done, e.g., by checking PDOM stack to
<   // see it has only one entry during exit_impl()
<   m_warp_active.reset(warp_id);
< 
<   // test for barrier release
<   cta_to_warp_t::iterator w = m_cta_to_warps.begin();
<   for (; w != m_cta_to_warps.end(); ++w) {
<     if (w->second.test(warp_id) == true) break;
<   }
<   warp_set_t warps_in_cta = w->second;
<   warp_set_t active = warps_in_cta & m_warp_active;
---
>    // caller needs to verify all threads in warp are done, e.g., by checking PDOM stack to 
>    // see it has only one entry during exit_impl()
>    m_warp_active.reset(warp_id);
> 
>    // test for barrier release 
>    cta_to_warp_t::iterator w=m_cta_to_warps.begin(); 
>    for (; w != m_cta_to_warps.end(); ++w) {
>       if (w->second.test(warp_id) == true) break; 
>    }
>    warp_set_t warps_in_cta = w->second;
>    warp_set_t active = warps_in_cta & m_warp_active;
2669,2676c2668,2675
<   for (unsigned i = 0; i < m_max_barriers_per_cta; i++) {
<     warp_set_t at_a_specific_barrier = warps_in_cta & m_bar_id_to_warps[i];
<     if ( at_a_specific_barrier == active ) {
<       // all warps have reached barrier, so release waiting warps...
<       m_bar_id_to_warps[i] &= ~at_a_specific_barrier;
<       m_warp_at_barrier &= ~at_a_specific_barrier;
<     }
<   }
---
>    for(unsigned i=0; i<m_max_barriers_per_cta; i++){
> 	   warp_set_t at_a_specific_barrier = warps_in_cta & m_bar_id_to_warps[i];
> 	   if( at_a_specific_barrier == active ) {
> 	      // all warps have reached barrier, so release waiting warps...
> 		   m_bar_id_to_warps[i] &= ~at_a_specific_barrier;
> 		   m_warp_at_barrier &= ~at_a_specific_barrier;
> 	   }
>    }
2681,2682c2680,2681
< {
<   return m_warp_at_barrier.test(warp_id);
---
> { 
>    return m_warp_at_barrier.test(warp_id);
2687,2705c2686,2704
<   printf( "barrier set information\n");
<   printf( "  m_max_cta_per_core = %u\n",  m_max_cta_per_core );
<   printf( "  m_max_warps_per_core = %u\n", m_max_warps_per_core );
<   printf( " m_max_barriers_per_cta =%u\n", m_max_barriers_per_cta);
<   printf( "  cta_to_warps:\n");
< 
<   cta_to_warp_t::const_iterator i;
<   for ( i = m_cta_to_warps.begin(); i != m_cta_to_warps.end(); i++ ) {
<     unsigned cta_id = i->first;
<     warp_set_t warps = i->second;
<     printf("    cta_id %u : %s\n", cta_id, warps.to_string().c_str() );
<   }
<   printf("  warp_active: %s\n", m_warp_active.to_string().c_str() );
<   printf("  warp_at_barrier: %s\n", m_warp_at_barrier.to_string().c_str() );
<   for ( unsigned i = 0; i < m_max_barriers_per_cta; i++) {
<     warp_set_t warps_reached_barrier = m_bar_id_to_warps[i];
<     printf("  warp_at_barrier %u: %s\n", i, warps_reached_barrier.to_string().c_str() );
<   }
<   fflush(stdout);
---
>    printf( "barrier set information\n");
>    printf( "  m_max_cta_per_core = %u\n",  m_max_cta_per_core );
>    printf( "  m_max_warps_per_core = %u\n", m_max_warps_per_core );
>    printf( " m_max_barriers_per_cta =%u\n", m_max_barriers_per_cta);
>    printf( "  cta_to_warps:\n");
>    
>    cta_to_warp_t::const_iterator i;
>    for( i=m_cta_to_warps.begin(); i!=m_cta_to_warps.end(); i++ ) {
>       unsigned cta_id = i->first;
>       warp_set_t warps = i->second;
>       printf("    cta_id %u : %s\n", cta_id, warps.to_string().c_str() );
>    }
>    printf("  warp_active: %s\n", m_warp_active.to_string().c_str() );
>    printf("  warp_at_barrier: %s\n", m_warp_at_barrier.to_string().c_str() );
>    for( unsigned i=0; i<m_max_barriers_per_cta; i++){
> 	   warp_set_t warps_reached_barrier = m_bar_id_to_warps[i];
> 	   printf("  warp_at_barrier %u: %s\n", i, warps_reached_barrier.to_string().c_str() );
>    }
>    fflush(stdout); 
2710,2725c2709,2724
<   bool done = true;
<   for ( unsigned i = warp_id * get_config()->warp_size;
<         i < (warp_id + 1)*get_config()->warp_size;
<         i++ ) {
< 
< //    if(this->m_thread[i]->m_functional_model_thread_state && this->m_thread[i].m_functional_model_thread_state->donecycle()==0) {
< //      done = false;
< //    }
< 
< 
<     if (m_thread[i] && !m_thread[i]->is_done()) done = false;
<   }
<   //if (m_warp[warp_id].get_n_completed() == get_config()->warp_size)
<   //if (this->m_simt_stack[warp_id]->get_num_entries() == 0)
<   if (done)
<     m_barriers.warp_exit( warp_id );
---
> 	bool done = true;
> 	for (	unsigned i = warp_id*get_config()->warp_size;
> 			i < (warp_id+1)*get_config()->warp_size;
> 			i++ ) {
> 
> //		if(this->m_thread[i]->m_functional_model_thread_state && this->m_thread[i].m_functional_model_thread_state->donecycle()==0) {
> //			done = false;
> //		}
> 
> 
> 		if (m_thread[i] && !m_thread[i]->is_done()) done = false;
> 	}
> 	//if (m_warp[warp_id].get_n_completed() == get_config()->warp_size)
> 	//if (this->m_simt_stack[warp_id]->get_num_entries() == 0)
> 	if (done)
> 		m_barriers.warp_exit( warp_id );
2730,2737c2729,2736
<   unsigned warp_id = inst.warp_id();
<   bool bar_red_op = (inst.op == BARRIER_OP) && (inst.bar_type == RED);
<   bool non_released_barrier_reduction = false;
<   bool warp_stucked_at_barrier = warp_waiting_at_barrier(warp_id);
<   bool single_inst_in_pipeline = (m_warp[warp_id].num_issued_inst_in_pipeline() == 1);
<   non_released_barrier_reduction = single_inst_in_pipeline and warp_stucked_at_barrier and bar_red_op;
<   printf("non_released_barrier_reduction=%u\n", non_released_barrier_reduction);
<   return non_released_barrier_reduction;
---
> 	unsigned warp_id = inst.warp_id();
> 	bool bar_red_op = (inst.op == BARRIER_OP) && (inst.bar_type == RED);
>     bool non_released_barrier_reduction = false;
>     bool warp_stucked_at_barrier = warp_waiting_at_barrier(warp_id);
>     bool single_inst_in_pipeline = (m_warp[warp_id].num_issued_inst_in_pipeline()==1);
>     non_released_barrier_reduction = single_inst_in_pipeline and warp_stucked_at_barrier and bar_red_op;
>     printf("non_released_barrier_reduction=%u\n",non_released_barrier_reduction);
>     return non_released_barrier_reduction;
2742c2741
<   return m_barriers.warp_waiting_at_barrier(warp_id);
---
>    return m_barriers.warp_waiting_at_barrier(warp_id);
2745c2744
< bool shader_core_ctx::warp_waiting_at_mem_barrier( unsigned warp_id )
---
> bool shader_core_ctx::warp_waiting_at_mem_barrier( unsigned warp_id ) 
2747,2753c2746,2752
<   if ( !m_warp[warp_id].get_membar() )
<     return false;
<   if ( !m_scoreboard->pendingWrites(warp_id) ) {
<     m_warp[warp_id].clear_membar();
<     return false;
<   }
<   return true;
---
>    if( !m_warp[warp_id].get_membar() ) 
>       return false;
>    if( !m_scoreboard->pendingWrites(warp_id) ) {
>       m_warp[warp_id].clear_membar();
>       return false;
>    }
>    return true;
2756c2755
< void shader_core_ctx::set_max_cta( const kernel_info_t &kernel )
---
> void shader_core_ctx::set_max_cta( const kernel_info_t &kernel ) 
2758,2763c2757,2762
<   // calculate the max cta count and cta size for local memory address mapping
<   kernel_max_cta_per_shader = m_config->max_cta(kernel);
<   unsigned int gpu_cta_size = kernel.threads_per_cta();
<   kernel_padded_threads_per_cta = (gpu_cta_size % m_config->warp_size) ?
<                                   m_config->warp_size * ((gpu_cta_size / m_config->warp_size) + 1) :
<                                   gpu_cta_size;
---
>     // calculate the max cta count and cta size for local memory address mapping
>     kernel_max_cta_per_shader = m_config->max_cta(kernel);
>     unsigned int gpu_cta_size = kernel.threads_per_cta();
>     kernel_padded_threads_per_cta = (gpu_cta_size%m_config->warp_size) ? 
>         m_config->warp_size*((gpu_cta_size/m_config->warp_size)+1) : 
>         gpu_cta_size;
2768,2769c2767,2768
<   assert( m_warp[wid].get_n_atomic() >= n );
<   m_warp[wid].dec_n_atomic(n);
---
>    assert( m_warp[wid].get_n_atomic() >= n );
>    m_warp[wid].dec_n_atomic(n);
2772c2771
< void shader_core_ctx::broadcast_barrier_reduction(unsigned cta_id, unsigned bar_id, warp_set_t warps)
---
> void shader_core_ctx::broadcast_barrier_reduction(unsigned cta_id,unsigned bar_id,warp_set_t warps)
2774,2779c2773,2778
<   for (unsigned i = 0; i < m_config->max_warps_per_shader; i++) {
<     if (warps.test(i)) {
<       const warp_inst_t * inst = m_warp[i].restore_info_of_last_inst_at_barrier();
<       const_cast<warp_inst_t *> (inst)->broadcast_barrier_reduction(inst->get_active_mask());
<     }
<   }
---
> 	for(unsigned i=0; i<m_config->max_warps_per_shader;i++){
> 		if(warps.test(i)){
> 			const warp_inst_t * inst = m_warp[i].restore_info_of_last_inst_at_barrier();
> 			const_cast<warp_inst_t *> (inst)->broadcast_barrier_reduction(inst->get_active_mask());
> 		}
> 	}
2784c2783
<   return false;
---
>     return false;
2789,2790c2788,2789
<   mf->set_status(IN_SHADER_FETCHED, gpu_sim_cycle + gpu_tot_sim_cycle);
<   m_L1I->fill(mf, gpu_sim_cycle + gpu_tot_sim_cycle);
---
>     mf->set_status(IN_SHADER_FETCHED,gpu_sim_cycle+gpu_tot_sim_cycle);
>     m_L1I->fill(mf,gpu_sim_cycle+gpu_tot_sim_cycle);
2795c2794
<   return m_ldst_unit->response_buffer_full();
---
>     return m_ldst_unit->response_buffer_full();
2798c2797
< void shader_core_ctx::accept_ldst_unit_response(mem_fetch * mf)
---
> void shader_core_ctx::accept_ldst_unit_response(mem_fetch * mf) 
2800c2799
<   m_ldst_unit->fill(mf);
---
>    m_ldst_unit->fill(mf);
2805,2807c2804,2806
<   assert( mf->get_type() == WRITE_ACK  || ( m_config->gpgpu_perfect_mem && mf->get_is_write() ) );
<   unsigned warp_id = mf->get_wid();
<   m_warp[warp_id].dec_store_req();
---
> 	assert( mf->get_type() == WRITE_ACK  || ( m_config->gpgpu_perfect_mem && mf->get_is_write() ) );
>     unsigned warp_id = mf->get_wid();
>     m_warp[warp_id].dec_store_req();
2811c2810
<   m_ldst_unit->print_cache_stats( fp, dl1_accesses, dl1_misses );
---
>    m_ldst_unit->print_cache_stats( fp, dl1_accesses, dl1_misses );
2814,2817c2813,2816
< void shader_core_ctx::get_cache_stats(cache_stats &cs) {
<   // Adds stats from each cache to 'cs'
<   cs += m_L1I->get_stats(); // Get L1I stats
<   m_ldst_unit->get_cache_stats(cs); // Get L1D, L1C, L1T stats
---
> void shader_core_ctx::get_cache_stats(cache_stats &cs){
>     // Adds stats from each cache to 'cs'
>     cs += m_L1I->get_stats(); // Get L1I stats
>     m_ldst_unit->get_cache_stats(cs); // Get L1D, L1C, L1T stats
2820,2822c2819,2821
< void shader_core_ctx::get_L1I_sub_stats(struct cache_sub_stats &css) const {
<   if (m_L1I)
<     m_L1I->get_sub_stats(css);
---
> void shader_core_ctx::get_L1I_sub_stats(struct cache_sub_stats &css) const{
>     if(m_L1I)
>         m_L1I->get_sub_stats(css);
2824,2825c2823,2824
< void shader_core_ctx::get_L1D_sub_stats(struct cache_sub_stats &css) const {
<   m_ldst_unit->get_L1D_sub_stats(css);
---
> void shader_core_ctx::get_L1D_sub_stats(struct cache_sub_stats &css) const{
>     m_ldst_unit->get_L1D_sub_stats(css);
2827,2828c2826,2827
< void shader_core_ctx::get_L1C_sub_stats(struct cache_sub_stats &css) const {
<   m_ldst_unit->get_L1C_sub_stats(css);
---
> void shader_core_ctx::get_L1C_sub_stats(struct cache_sub_stats &css) const{
>     m_ldst_unit->get_L1C_sub_stats(css);
2830,2831c2829,2830
< void shader_core_ctx::get_L1T_sub_stats(struct cache_sub_stats &css) const {
<   m_ldst_unit->get_L1T_sub_stats(css);
---
> void shader_core_ctx::get_L1T_sub_stats(struct cache_sub_stats &css) const{
>     m_ldst_unit->get_L1T_sub_stats(css);
2834,2836c2833,2835
< void shader_core_ctx::get_icnt_power_stats(long &n_simt_to_mem, long &n_mem_to_simt) const {
<   n_simt_to_mem += m_stats->n_simt_to_mem[m_sid];
<   n_mem_to_simt += m_stats->n_mem_to_simt[m_sid];
---
> void shader_core_ctx::get_icnt_power_stats(long &n_simt_to_mem, long &n_mem_to_simt) const{
> 	n_simt_to_mem += m_stats->n_simt_to_mem[m_sid];
> 	n_mem_to_simt += m_stats->n_mem_to_simt[m_sid];
2841c2840
<   return get_n_completed() == m_warp_size;
---
>     return get_n_completed() == m_warp_size;
2846c2845
<   return functional_done() && stores_done() && !inst_in_pipeline();
---
>     return functional_done() && stores_done() && !inst_in_pipeline(); 
2849c2848
< bool shd_warp_t::waiting()
---
> bool shd_warp_t::waiting() 
2851,2868c2850,2867
<   if ( functional_done() ) {
<     // waiting to be initialized with a kernel
<     return true;
<   } else if ( m_shader->warp_waiting_at_barrier(m_warp_id) ) {
<     // waiting for other warps in CTA to reach barrier
<     return true;
<   } else if ( m_shader->warp_waiting_at_mem_barrier(m_warp_id) ) {
<     // waiting for memory barrier
<     return true;
<   } else if ( m_n_atomic > 0 ) {
<     // waiting for atomic operation to complete at memory:
<     // this stall is not required for accurate timing model, but rather we
<     // stall here since if a call/return instruction occurs in the meantime
<     // the functional execution of the atomic when it hits DRAM can cause
<     // the wrong register to be read.
<     return true;
<   }
<   return false;
---
>     if ( functional_done() ) {
>         // waiting to be initialized with a kernel
>         return true;
>     } else if ( m_shader->warp_waiting_at_barrier(m_warp_id) ) {
>         // waiting for other warps in CTA to reach barrier
>         return true;
>     } else if ( m_shader->warp_waiting_at_mem_barrier(m_warp_id) ) {
>         // waiting for memory barrier
>         return true;
>     } else if ( m_n_atomic >0 ) {
>         // waiting for atomic operation to complete at memory:
>         // this stall is not required for accurate timing model, but rather we
>         // stall here since if a call/return instruction occurs in the meantime
>         // the functional execution of the atomic when it hits DRAM can cause
>         // the wrong register to be read.
>         return true;
>     }
>     return false;
2873,2897c2872,2896
<   if ( !done_exit() ) {
<     fprintf( fout, "w%02u npc: 0x%04x, done:%c%c%c%c:%2u i:%u s:%u a:%u (done: ",
<              m_warp_id,
<              m_next_pc,
<              (functional_done() ? 'f' : ' '),
<              (stores_done() ? 's' : ' '),
<              (inst_in_pipeline() ? ' ' : 'i'),
<              (done_exit() ? 'e' : ' '),
<              n_completed,
<              m_inst_in_pipeline,
<              m_stores_outstanding,
<              m_n_atomic );
<     for (unsigned i = m_warp_id * m_warp_size; i < (m_warp_id + 1)*m_warp_size; i++ ) {
<       if ( m_shader->ptx_thread_done(i) ) fprintf(fout, "1");
<       else fprintf(fout, "0");
<       if ( (((i + 1) % 4) == 0) && (i + 1) < (m_warp_id + 1)*m_warp_size )
<         fprintf(fout, ",");
<     }
<     fprintf(fout, ") ");
<     fprintf(fout, " active=%s", m_active_threads.to_string().c_str() );
<     fprintf(fout, " last fetched @ %5llu", m_last_fetch);
<     if ( m_imiss_pending )
<       fprintf(fout, " i-miss pending");
<     fprintf(fout, "\n");
<   }
---
>     if( !done_exit() ) {
>         fprintf( fout, "w%02u npc: 0x%04x, done:%c%c%c%c:%2u i:%u s:%u a:%u (done: ", 
>                 m_warp_id,
>                 m_next_pc,
>                 (functional_done()?'f':' '),
>                 (stores_done()?'s':' '),
>                 (inst_in_pipeline()?' ':'i'),
>                 (done_exit()?'e':' '),
>                 n_completed,
>                 m_inst_in_pipeline, 
>                 m_stores_outstanding,
>                 m_n_atomic );
>         for (unsigned i = m_warp_id*m_warp_size; i < (m_warp_id+1)*m_warp_size; i++ ) {
>           if ( m_shader->ptx_thread_done(i) ) fprintf(fout,"1");
>           else fprintf(fout,"0");
>           if ( (((i+1)%4) == 0) && (i+1) < (m_warp_id+1)*m_warp_size ) 
>              fprintf(fout,",");
>         }
>         fprintf(fout,") ");
>         fprintf(fout," active=%s", m_active_threads.to_string().c_str() );
>         fprintf(fout," last fetched @ %5llu", m_last_fetch);
>         if( m_imiss_pending ) 
>             fprintf(fout," i-miss pending");
>         fprintf(fout,"\n");
>     }
2902,2922c2901,2921
<   fprintf(fout, "  ibuffer[%2u] : ", m_warp_id );
<   for ( unsigned i = 0; i < IBUFFER_SIZE; i++) {
<     const inst_t *inst = m_ibuffer[i].m_inst;
<     if ( inst ) inst->print_insn(fout);
<     else if ( m_ibuffer[i].m_valid )
<       fprintf(fout, " <invalid instruction> ");
<     else fprintf(fout, " <empty> ");
<   }
<   fprintf(fout, "\n");
< }
< 
< void opndcoll_rfu_t::add_cu_set(unsigned set_id, unsigned num_cu, unsigned num_dispatch) {
<   m_cus[set_id].reserve(num_cu); //this is necessary to stop pointers in m_cu from being invalid do to a resize;
<   for (unsigned i = 0; i < num_cu; i++) {
<     m_cus[set_id].push_back(collector_unit_t());
<     m_cu.push_back(&m_cus[set_id].back());
<   }
<   // for now each collector set gets dedicated dispatch units.
<   for (unsigned i = 0; i < num_dispatch; i++) {
<     m_dispatch_units.push_back(dispatch_unit_t(&m_cus[set_id]));
<   }
---
>     fprintf(fout,"  ibuffer[%2u] : ", m_warp_id );
>     for( unsigned i=0; i < IBUFFER_SIZE; i++) {
>         const inst_t *inst = m_ibuffer[i].m_inst;
>         if( inst ) inst->print_insn(fout);
>         else if( m_ibuffer[i].m_valid ) 
>            fprintf(fout," <invalid instruction> ");
>         else fprintf(fout," <empty> ");
>     }
>     fprintf(fout,"\n");
> }
> 
> void opndcoll_rfu_t::add_cu_set(unsigned set_id, unsigned num_cu, unsigned num_dispatch){
>     m_cus[set_id].reserve(num_cu); //this is necessary to stop pointers in m_cu from being invalid do to a resize;
>     for (unsigned i = 0; i < num_cu; i++) {
>         m_cus[set_id].push_back(collector_unit_t());
>         m_cu.push_back(&m_cus[set_id].back());
>     }
>     // for now each collector set gets dedicated dispatch units.
>     for (unsigned i = 0; i < num_dispatch; i++) {
>         m_dispatch_units.push_back(dispatch_unit_t(&m_cus[set_id]));
>     }
2928,2936c2927,2935
<   //m_num_ports++;
<   //m_num_collectors += num_collector_units;
<   //m_input.resize(m_num_ports);
<   //m_output.resize(m_num_ports);
<   //m_num_collector_units.resize(m_num_ports);
<   //m_input[m_num_ports-1]=input_port;
<   //m_output[m_num_ports-1]=output_port;
<   //m_num_collector_units[m_num_ports-1]=num_collector_units;
<   m_in_ports.push_back(input_port_t(input, output, cu_sets));
---
>     //m_num_ports++;
>     //m_num_collectors += num_collector_units;
>     //m_input.resize(m_num_ports);
>     //m_output.resize(m_num_ports);
>     //m_num_collector_units.resize(m_num_ports);
>     //m_input[m_num_ports-1]=input_port;
>     //m_output[m_num_ports-1]=output_port;
>     //m_num_collector_units[m_num_ports-1]=num_collector_units;
>     m_in_ports.push_back(input_port_t(input,output,cu_sets));
2941,2949c2940,2948
<   m_shader = shader;
<   m_arbiter.init(m_cu.size(), num_banks);
<   //for( unsigned n=0; n<m_num_ports;n++ )
<   //    m_dispatch_units[m_output[n]].init( m_num_collector_units[n] );
<   m_num_banks = num_banks;
<   m_bank_warp_shift = 0;
<   m_warp_size = shader->get_config()->warp_size;
<   m_bank_warp_shift = (unsigned)(int) (log(m_warp_size + 0.5) / log(2.0));
<   assert( (m_bank_warp_shift == 5) || (m_warp_size != 32) );
---
>    m_shader=shader;
>    m_arbiter.init(m_cu.size(),num_banks);
>    //for( unsigned n=0; n<m_num_ports;n++ ) 
>    //    m_dispatch_units[m_output[n]].init( m_num_collector_units[n] );
>    m_num_banks = num_banks;
>    m_bank_warp_shift = 0; 
>    m_warp_size = shader->get_config()->warp_size;
>    m_bank_warp_shift = (unsigned)(int) (log(m_warp_size+0.5) / log(2.0));
>    assert( (m_bank_warp_shift == 5) || (m_warp_size != 32) );
2951,2954c2950,2953
<   for ( unsigned j = 0; j < m_cu.size(); j++) {
<     m_cu[j]->init(j, num_banks, m_bank_warp_shift, shader->get_config(), this);
<   }
<   m_initialized = true;
---
>    for( unsigned j=0; j<m_cu.size(); j++) {
>        m_cu[j]->init(j,num_banks,m_bank_warp_shift,shader->get_config(),this);
>    }
>    m_initialized=true;
2959,2962c2958,2961
<   int bank = regnum;
<   if (bank_warp_shift)
<     bank += wid;
<   return bank % num_banks;
---
>    int bank = regnum;
>    if (bank_warp_shift)
>       bank += wid;
>    return bank % num_banks;
2967,2989c2966,2976
<   assert( !inst.empty() );
<   std::list<unsigned> regs = m_shader->get_regs_written(inst);
<   std::list<unsigned>::iterator r;
<   unsigned n = 0;
<   for ( r = regs.begin(); r != regs.end(); r++, n++ ) {
<     unsigned reg = *r;
<     unsigned bank = register_bank(reg, inst.warp_id(), m_num_banks, m_bank_warp_shift);
<     if ( m_arbiter.bank_idle(bank) ) {
<       m_arbiter.allocate_bank_for_write(bank, op_t(&inst, reg, m_num_banks, m_bank_warp_shift));
<     } else {
<       return false;
<     }
<   }
<   for (unsigned i = 0; i < (unsigned)regs.size(); i++) {
<     if (m_shader->get_config()->gpgpu_clock_gated_reg_file) {
<       unsigned active_count = 0;
<       for (unsigned i = 0; i < m_shader->get_config()->warp_size; i = i + m_shader->get_config()->n_regfile_gating_group) {
<         for (unsigned j = 0; j < m_shader->get_config()->n_regfile_gating_group; j++) {
<           if (inst.get_active_mask().test(i + j)) {
<             active_count += m_shader->get_config()->n_regfile_gating_group;
<             break;
<           }
<         }
---
>    assert( !inst.empty() );
>    std::list<unsigned> regs = m_shader->get_regs_written(inst);
>    std::list<unsigned>::iterator r;
>    unsigned n=0;
>    for( r=regs.begin(); r!=regs.end();r++,n++ ) {
>       unsigned reg = *r;
>       unsigned bank = register_bank(reg,inst.warp_id(),m_num_banks,m_bank_warp_shift);
>       if( m_arbiter.bank_idle(bank) ) {
>           m_arbiter.allocate_bank_for_write(bank,op_t(&inst,reg,m_num_banks,m_bank_warp_shift));
>       } else {
>           return false;
2991,2996c2978,2995
<       m_shader->incregfile_writes(active_count);
<     } else {
<       m_shader->incregfile_writes(m_shader->get_config()->warp_size);//inst.active_count());
<     }
<   }
<   return true;
---
>    }
>    for(unsigned i=0;i<(unsigned)regs.size();i++){
> 	      if(m_shader->get_config()->gpgpu_clock_gated_reg_file){
> 	    	  unsigned active_count=0;
> 	    	  for(unsigned i=0;i<m_shader->get_config()->warp_size;i=i+m_shader->get_config()->n_regfile_gating_group){
> 	    		  for(unsigned j=0;j<m_shader->get_config()->n_regfile_gating_group;j++){
> 	    			  if(inst.get_active_mask().test(i+j)){
> 	    				  active_count+=m_shader->get_config()->n_regfile_gating_group;
> 	    				  break;
> 	    			  }
> 	    		  }
> 	    	  }
> 	    	  m_shader->incregfile_writes(active_count);
> 	      }else{
> 	    	  m_shader->incregfile_writes(m_shader->get_config()->warp_size);//inst.active_count());
> 	      }
>    }
>    return true;
3001,3019c3000,3020
<   for ( unsigned p = 0; p < m_dispatch_units.size(); ++p ) {
<     dispatch_unit_t &du = m_dispatch_units[p];
<     collector_unit_t *cu = du.find_ready();
<     if ( cu ) {
<       for (unsigned i = 0; i < (cu->get_num_operands() - cu->get_num_regs()); i++) {
<         if (m_shader->get_config()->gpgpu_clock_gated_reg_file) {
<           unsigned active_count = 0;
<           for (unsigned i = 0; i < m_shader->get_config()->warp_size; i = i + m_shader->get_config()->n_regfile_gating_group) {
<             for (unsigned j = 0; j < m_shader->get_config()->n_regfile_gating_group; j++) {
<               if (cu->get_active_mask().test(i + j)) {
<                 active_count += m_shader->get_config()->n_regfile_gating_group;
<                 break;
<               }
<             }
<           }
<           m_shader->incnon_rf_operands(active_count);
<         } else {
<           m_shader->incnon_rf_operands(m_shader->get_config()->warp_size);//cu->get_active_count());
<         }
---
>    for( unsigned p=0; p < m_dispatch_units.size(); ++p ) {
>       dispatch_unit_t &du = m_dispatch_units[p];
>       collector_unit_t *cu = du.find_ready();
>       if( cu ) {
>     	 for(unsigned i=0;i<(cu->get_num_operands()-cu->get_num_regs());i++){
>    	      if(m_shader->get_config()->gpgpu_clock_gated_reg_file){
>    	    	  unsigned active_count=0;
>    	    	  for(unsigned i=0;i<m_shader->get_config()->warp_size;i=i+m_shader->get_config()->n_regfile_gating_group){
>    	    		  for(unsigned j=0;j<m_shader->get_config()->n_regfile_gating_group;j++){
>    	    			  if(cu->get_active_mask().test(i+j)){
>    	    				  active_count+=m_shader->get_config()->n_regfile_gating_group;
>    	    				  break;
>    	    			  }
>    	    		  }
>    	    	  }
>    	    	  m_shader->incnon_rf_operands(active_count);
>    	      }else{
>     		 m_shader->incnon_rf_operands(m_shader->get_config()->warp_size);//cu->get_active_count());
>    	      }
>     	}
>          cu->dispatch();
3021,3023c3022
<       cu->dispatch();
<     }
<   }
---
>    }
3028,3040c3027,3042
<   input_port_t& inp = m_in_ports[port_num];
<   for (unsigned i = 0; i < inp.m_in.size(); i++) {
<     if ( (*inp.m_in[i]).has_ready() ) {
<       //find a free cu
<       for (unsigned j = 0; j < inp.m_cu_sets.size(); j++) {
<         std::vector<collector_unit_t> & cu_set = m_cus[inp.m_cu_sets[j]];
<         bool allocated = false;
<         for (unsigned k = 0; k < cu_set.size(); k++) {
<           if (cu_set[k].is_free()) {
<             collector_unit_t *cu = &cu_set[k];
<             allocated = cu->allocate(inp.m_in[i], inp.m_out[i]);
<             m_arbiter.add_read_requests(cu);
<             break;
---
>    input_port_t& inp = m_in_ports[port_num];
>    for (unsigned i = 0; i < inp.m_in.size(); i++) {
>        if( (*inp.m_in[i]).has_ready() ) {
>           //find a free cu 
>           for (unsigned j = 0; j < inp.m_cu_sets.size(); j++) {
>               std::vector<collector_unit_t> & cu_set = m_cus[inp.m_cu_sets[j]];
> 	      bool allocated = false;
>               for (unsigned k = 0; k < cu_set.size(); k++) {
>                   if(cu_set[k].is_free()) {
>                      collector_unit_t *cu = &cu_set[k];
>                      allocated = cu->allocate(inp.m_in[i],inp.m_out[i]);
>                      m_arbiter.add_read_requests(cu);
>                      break;
>                   }
>               }
>               if (allocated) break; //cu has been allocated, no need to search more.
3042,3047c3044,3046
<         }
<         if (allocated) break; //cu has been allocated, no need to search more.
<       }
<       break; // can only service a single input, if it failed it will fail for others.
<     }
<   }
---
>           break; // can only service a single input, if it failed it will fail for others.
>        }
>    }
3052,3088c3051,3087
<   // process read requests that do not have conflicts
<   std::list<op_t> allocated = m_arbiter.allocate_reads();
<   std::map<unsigned, op_t> read_ops;
<   for ( std::list<op_t>::iterator r = allocated.begin(); r != allocated.end(); r++ ) {
<     const op_t &rr = *r;
<     unsigned reg = rr.get_reg();
<     unsigned wid = rr.get_wid();
<     unsigned bank = register_bank(reg, wid, m_num_banks, m_bank_warp_shift);
<     m_arbiter.allocate_for_read(bank, rr);
<     read_ops[bank] = rr;
<   }
<   std::map<unsigned, op_t>::iterator r;
<   for (r = read_ops.begin(); r != read_ops.end(); ++r ) {
<     op_t &op = r->second;
<     unsigned cu = op.get_oc_id();
<     unsigned operand = op.get_operand();
<     m_cu[cu]->collect_operand(operand);
<     if (m_shader->get_config()->gpgpu_clock_gated_reg_file) {
<       unsigned active_count = 0;
<       for (unsigned i = 0; i < m_shader->get_config()->warp_size; i = i + m_shader->get_config()->n_regfile_gating_group) {
<         for (unsigned j = 0; j < m_shader->get_config()->n_regfile_gating_group; j++) {
<           if (op.get_active_mask().test(i + j)) {
<             active_count += m_shader->get_config()->n_regfile_gating_group;
<             break;
<           }
<         }
<       }
<       m_shader->incregfile_reads(active_count);
<     } else {
<       m_shader->incregfile_reads(m_shader->get_config()->warp_size);//op.get_active_count());
<     }
<   }
< }
< 
< bool opndcoll_rfu_t::collector_unit_t::ready() const
< {
<   return (!m_free) && m_not_ready.none() && (*m_output_register).has_free();
---
>    // process read requests that do not have conflicts
>    std::list<op_t> allocated = m_arbiter.allocate_reads();
>    std::map<unsigned,op_t> read_ops;
>    for( std::list<op_t>::iterator r=allocated.begin(); r!=allocated.end(); r++ ) {
>       const op_t &rr = *r;
>       unsigned reg = rr.get_reg();
>       unsigned wid = rr.get_wid();
>       unsigned bank = register_bank(reg,wid,m_num_banks,m_bank_warp_shift);
>       m_arbiter.allocate_for_read(bank,rr);
>       read_ops[bank] = rr;
>    }
>    std::map<unsigned,op_t>::iterator r;
>    for(r=read_ops.begin();r!=read_ops.end();++r ) {
>       op_t &op = r->second;
>       unsigned cu = op.get_oc_id();
>       unsigned operand = op.get_operand();
>       m_cu[cu]->collect_operand(operand);
>       if(m_shader->get_config()->gpgpu_clock_gated_reg_file){
>     	  unsigned active_count=0;
>     	  for(unsigned i=0;i<m_shader->get_config()->warp_size;i=i+m_shader->get_config()->n_regfile_gating_group){
>     		  for(unsigned j=0;j<m_shader->get_config()->n_regfile_gating_group;j++){
>     			  if(op.get_active_mask().test(i+j)){
>     				  active_count+=m_shader->get_config()->n_regfile_gating_group;
>     				  break;
>     			  }
>     		  }
>     	  }
>     	  m_shader->incregfile_reads(active_count);
>       }else{
>     	  m_shader->incregfile_reads(m_shader->get_config()->warp_size);//op.get_active_count());
>       }
>   }
> } 
> 
> bool opndcoll_rfu_t::collector_unit_t::ready() const 
> { 
>    return (!m_free) && m_not_ready.none() && (*m_output_register).has_free(); 
3093,3100c3092,3100
<   if ( m_free ) {
<     fprintf(fp, "    <free>\n");
<   } else {
<     m_warp->print(fp);
<     for ( unsigned i = 0; i < MAX_REG_OPERANDS * 2; i++ ) {
<       if ( m_not_ready.test(i) ) {
<         std::string r = m_src_op[i].get_reg_string();
<         fprintf(fp, "    '%s' not ready\n", r.c_str() );
---
>    if( m_free ) {
>       fprintf(fp,"    <free>\n");
>    } else {
>       m_warp->print(fp);
>       for( unsigned i=0; i < MAX_REG_OPERANDS*2; i++ ) {
>          if( m_not_ready.test(i) ) {
>             std::string r = m_src_op[i].get_reg_string();
>             fprintf(fp,"    '%s' not ready\n", r.c_str() );
>          }
3102,3103c3102
<     }
<   }
---
>    }
3106,3141c3105,3140
< void opndcoll_rfu_t::collector_unit_t::init( unsigned n,
<     unsigned num_banks,
<     unsigned log2_warp_size,
<     const core_config *config,
<     opndcoll_rfu_t *rfu )
< {
<   m_rfu = rfu;
<   m_cuid = n;
<   m_num_banks = num_banks;
<   assert(m_warp == NULL);
<   m_warp = new warp_inst_t(config);
<   m_bank_warp_shift = log2_warp_size;
< }
< 
< bool opndcoll_rfu_t::collector_unit_t::allocate( register_set* pipeline_reg_set, register_set* output_reg_set )
< {
<   assert(m_free);
<   assert(m_not_ready.none());
<   m_free = false;
<   m_output_register = output_reg_set;
<   warp_inst_t **pipeline_reg = pipeline_reg_set->get_ready();
<   if ( (pipeline_reg) and !((*pipeline_reg)->empty()) ) {
<     m_warp_id = (*pipeline_reg)->warp_id();
<     for ( unsigned op = 0; op < MAX_REG_OPERANDS; op++ ) {
<       int reg_num = (*pipeline_reg)->arch_reg.src[op]; // this math needs to match that used in function_info::ptx_decode_inst
<       if ( reg_num >= 0 ) { // valid register
<         m_src_op[op] = op_t( this, op, reg_num, m_num_banks, m_bank_warp_shift );
<         m_not_ready.set(op);
<       } else
<         m_src_op[op] = op_t();
<     }
<     //move_warp(m_warp,*pipeline_reg);
<     pipeline_reg_set->move_out_to(m_warp);
<     return true;
<   }
<   return false;
---
> void opndcoll_rfu_t::collector_unit_t::init( unsigned n, 
>                                              unsigned num_banks, 
>                                              unsigned log2_warp_size,
>                                              const core_config *config,
>                                              opndcoll_rfu_t *rfu ) 
> { 
>    m_rfu=rfu;
>    m_cuid=n; 
>    m_num_banks=num_banks;
>    assert(m_warp==NULL); 
>    m_warp = new warp_inst_t(config);
>    m_bank_warp_shift=log2_warp_size;
> }
> 
> bool opndcoll_rfu_t::collector_unit_t::allocate( register_set* pipeline_reg_set, register_set* output_reg_set ) 
> {
>    assert(m_free);
>    assert(m_not_ready.none());
>    m_free = false;
>    m_output_register = output_reg_set;
>    warp_inst_t **pipeline_reg = pipeline_reg_set->get_ready();
>    if( (pipeline_reg) and !((*pipeline_reg)->empty()) ) {
>       m_warp_id = (*pipeline_reg)->warp_id();
>       for( unsigned op=0; op < MAX_REG_OPERANDS; op++ ) {
>          int reg_num = (*pipeline_reg)->arch_reg.src[op]; // this math needs to match that used in function_info::ptx_decode_inst
>          if( reg_num >= 0 ) { // valid register
>             m_src_op[op] = op_t( this, op, reg_num, m_num_banks, m_bank_warp_shift );
>             m_not_ready.set(op);
>          } else 
>             m_src_op[op] = op_t();
>       }
>       //move_warp(m_warp,*pipeline_reg);
>       pipeline_reg_set->move_out_to(m_warp);
>       return true;
>    }
>    return false;
3146,3152c3145,3151
<   assert( m_not_ready.none() );
<   //move_warp(*m_output_register,m_warp);
<   m_output_register->move_in(m_warp);
<   m_free = true;
<   m_output_register = NULL;
<   for ( unsigned i = 0; i < MAX_REG_OPERANDS * 2; i++)
<     m_src_op[i].reset();
---
>    assert( m_not_ready.none() );
>    //move_warp(*m_output_register,m_warp);
>    m_output_register->move_in(m_warp);
>    m_free=true;
>    m_output_register = NULL;
>    for( unsigned i=0; i<MAX_REG_OPERANDS*2;i++)
>       m_src_op[i].reset();
3155,3157c3154,3156
< simt_core_cluster::simt_core_cluster( class gpgpu_sim *gpu,
<                                       unsigned cluster_id,
<                                       const struct shader_core_config *config,
---
> simt_core_cluster::simt_core_cluster( class gpgpu_sim *gpu, 
>                                       unsigned cluster_id, 
>                                       const struct shader_core_config *config, 
3159c3158
<                                       shader_core_stats *stats,
---
>                                       shader_core_stats *stats, 
3162,3173c3161,3172
<   m_config = config;
<   m_cta_issue_next_core = m_config->n_simt_cores_per_cluster - 1; // this causes first launch to use hw cta 0
<   m_cluster_id = cluster_id;
<   m_gpu = gpu;
<   m_stats = stats;
<   m_memory_stats = mstats;
<   m_core = new shader_core_ctx*[ config->n_simt_cores_per_cluster ];
<   for ( unsigned i = 0; i < config->n_simt_cores_per_cluster; i++ ) {
<     unsigned sid = m_config->cid_to_sid(i, m_cluster_id);
<     m_core[i] = new shader_core_ctx(gpu, this, sid, m_cluster_id, config, mem_config, stats);
<     m_core_sim_order.push_back(i);
<   }
---
>     m_config = config;
>     m_cta_issue_next_core=m_config->n_simt_cores_per_cluster-1; // this causes first launch to use hw cta 0
>     m_cluster_id=cluster_id;
>     m_gpu = gpu;
>     m_stats = stats;
>     m_memory_stats = mstats;
>     m_core = new shader_core_ctx*[ config->n_simt_cores_per_cluster ];
>     for( unsigned i=0; i < config->n_simt_cores_per_cluster; i++ ) {
>         unsigned sid = m_config->cid_to_sid(i,m_cluster_id);
>         m_core[i] = new shader_core_ctx(gpu,this,sid,m_cluster_id,config,mem_config,stats);
>         m_core_sim_order.push_back(i); 
>     }
3178,3180c3177,3179
<   for ( std::list<unsigned>::iterator it = m_core_sim_order.begin(); it != m_core_sim_order.end(); ++it ) {
<     m_core[*it]->cycle();
<   }
---
>     for( std::list<unsigned>::iterator it = m_core_sim_order.begin(); it != m_core_sim_order.end(); ++it ) {
>         m_core[*it]->cycle();
>     }
3182,3184c3181,3183
<   if (m_config->simt_core_sim_order == 1) {
<     m_core_sim_order.splice(m_core_sim_order.end(), m_core_sim_order, m_core_sim_order.begin());
<   }
---
>     if (m_config->simt_core_sim_order == 1) {
>         m_core_sim_order.splice(m_core_sim_order.end(), m_core_sim_order, m_core_sim_order.begin()); 
>     }
3189,3190c3188,3189
<   for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; i++ )
<     m_core[i]->reinit(0, m_config->n_thread_per_shader, true);
---
>     for( unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++ ) 
>         m_core[i]->reinit(0,m_config->n_thread_per_shader,true);
3195c3194
<   return m_config->n_simt_cores_per_cluster * m_config->max_cta(kernel);
---
>     return m_config->n_simt_cores_per_cluster * m_config->max_cta(kernel);
3200,3203c3199,3202
<   unsigned not_completed = 0;
<   for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; i++ )
<     not_completed += m_core[i]->get_not_completed();
<   return not_completed;
---
>     unsigned not_completed=0;
>     for( unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++ ) 
>         not_completed += m_core[i]->get_not_completed();
>     return not_completed;
3208,3212c3207,3211
<   for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; i++ ) {
<     unsigned not_completed = m_core[i]->get_not_completed();
<     unsigned sid = m_config->cid_to_sid(i, m_cluster_id);
<     fprintf(fp, "%u(%u) ", sid, not_completed );
<   }
---
>     for( unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++ ) {
>         unsigned not_completed=m_core[i]->get_not_completed();
>         unsigned sid=m_config->cid_to_sid(i,m_cluster_id);
>         fprintf(fp,"%u(%u) ", sid, not_completed );
>     }
3217,3220c3216,3219
<   unsigned n = 0;
<   for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; i++ )
<     n += m_core[i]->get_n_active_cta();
<   return n;
---
>     unsigned n=0;
>     for( unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++ ) 
>         n += m_core[i]->get_n_active_cta();
>     return n;
3225,3228c3224,3227
<   unsigned n = 0;
<   for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; i++ )
<     n += m_core[i]->isactive();
<   return n;
---
>     unsigned n=0;
>     for( unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++ )
>         n += m_core[i]->isactive();
>     return n;
3233,3248c3232,3248
<   unsigned num_blocks_issued = 0;
<   for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; i++ ) {
<     unsigned core = (i + m_cta_issue_next_core + 1) % m_config->n_simt_cores_per_cluster;
<     if ( m_core[core]->get_not_completed() == 0 ) {
<       if ( m_core[core]->get_kernel() == NULL ) {
<         kernel_info_t *k = m_gpu->select_kernel();
<         if ( k )
<           m_core[core]->set_kernel(k);
<       }
<     }
<     kernel_info_t *kernel = m_core[core]->get_kernel();
<     if ( kernel && !kernel->no_more_ctas_to_run() && (m_core[core]->get_n_active_cta() < m_config->max_cta(*kernel)) ) {
<       m_core[core]->issue_block2core(*kernel);
<       num_blocks_issued++;
<       m_cta_issue_next_core = core;
<       break;
---
>     unsigned num_blocks_issued=0;
>     for( unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++ ) {
>         unsigned core = (i+m_cta_issue_next_core+1)%m_config->n_simt_cores_per_cluster;
>         if( m_core[core]->get_not_completed() == 0 ) {
>             if( m_core[core]->get_kernel() == NULL ) {
>                 kernel_info_t *k = m_gpu->select_kernel();
>                 if( k ) 
>                     m_core[core]->set_kernel(k);
>             }
>         }
>         kernel_info_t *kernel = m_core[core]->get_kernel();
>         if( kernel && !kernel->no_more_ctas_to_run() && (m_core[core]->get_n_active_cta() < m_config->max_cta(*kernel)) ) {
>             m_core[core]->issue_block2core(*kernel);
>             num_blocks_issued++;
>             m_cta_issue_next_core=core; 
>             break;
>         }
3250,3251c3250
<   }
<   return num_blocks_issued;
---
>     return num_blocks_issued;
3256,3257c3255,3256
<   for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; i++ )
<     m_core[i]->cache_flush();
---
>     for( unsigned i=0; i < m_config->n_simt_cores_per_cluster; i++ ) 
>         m_core[i]->cache_flush();
3262,3265c3261,3264
<   unsigned request_size = size;
<   if (!write)
<     request_size = READ_PACKET_SIZE;
<   return ! ::icnt_has_buffer(m_cluster_id, request_size);
---
>     unsigned request_size = size;
>     if (!write) 
>         request_size = READ_PACKET_SIZE;
>     return ! ::icnt_has_buffer(m_cluster_id, request_size);
3270,3301c3269,3300
<   // stats
<   if (mf->get_is_write()) m_stats->made_write_mfs++;
<   else m_stats->made_read_mfs++;
<   switch (mf->get_access_type()) {
<   case CONST_ACC_R: m_stats->gpgpu_n_mem_const++; break;
<   case TEXTURE_ACC_R: m_stats->gpgpu_n_mem_texture++; break;
<   case GLOBAL_ACC_R: m_stats->gpgpu_n_mem_read_global++; break;
<   case GLOBAL_ACC_W: m_stats->gpgpu_n_mem_write_global++; break;
<   case LOCAL_ACC_R: m_stats->gpgpu_n_mem_read_local++; break;
<   case LOCAL_ACC_W: m_stats->gpgpu_n_mem_write_local++; break;
<   case INST_ACC_R: m_stats->gpgpu_n_mem_read_inst++; break;
<   case L1_WRBK_ACC: m_stats->gpgpu_n_mem_write_global++; break;
<   case L2_WRBK_ACC: m_stats->gpgpu_n_mem_l2_writeback++; break;
<   case L1_WR_ALLOC_R: m_stats->gpgpu_n_mem_l1_write_allocate++; break;
<   case L2_WR_ALLOC_R: m_stats->gpgpu_n_mem_l2_write_allocate++; break;
<   default: assert(0);
<   }
< 
<   // The packet size varies depending on the type of request:
<   // - For write request and atomic request, the packet contains the data
<   // - For read request (i.e. not write nor atomic), the packet only has control metadata
<   unsigned int packet_size = mf->size();
<   if (!mf->get_is_write() && !mf->isatomic()) {
<     packet_size = mf->get_ctrl_size();
<   }
<   m_stats->m_outgoing_traffic_stats->record_traffic(mf, packet_size);
<   unsigned destination = mf->get_sub_partition_id();
<   mf->set_status(IN_ICNT_TO_MEM, gpu_sim_cycle + gpu_tot_sim_cycle);
<   if (!mf->get_is_write() && !mf->isatomic())
<     ::icnt_push(m_cluster_id, m_config->mem2device(destination), (void*)mf, mf->get_ctrl_size() );
<   else
<     ::icnt_push(m_cluster_id, m_config->mem2device(destination), (void*)mf, mf->size());
---
>     // stats
>     if (mf->get_is_write()) m_stats->made_write_mfs++;
>     else m_stats->made_read_mfs++;
>     switch (mf->get_access_type()) {
>     case CONST_ACC_R: m_stats->gpgpu_n_mem_const++; break;
>     case TEXTURE_ACC_R: m_stats->gpgpu_n_mem_texture++; break;
>     case GLOBAL_ACC_R: m_stats->gpgpu_n_mem_read_global++; break;
>     case GLOBAL_ACC_W: m_stats->gpgpu_n_mem_write_global++; break;
>     case LOCAL_ACC_R: m_stats->gpgpu_n_mem_read_local++; break;
>     case LOCAL_ACC_W: m_stats->gpgpu_n_mem_write_local++; break;
>     case INST_ACC_R: m_stats->gpgpu_n_mem_read_inst++; break;
>     case L1_WRBK_ACC: m_stats->gpgpu_n_mem_write_global++; break;
>     case L2_WRBK_ACC: m_stats->gpgpu_n_mem_l2_writeback++; break;
>     case L1_WR_ALLOC_R: m_stats->gpgpu_n_mem_l1_write_allocate++; break;
>     case L2_WR_ALLOC_R: m_stats->gpgpu_n_mem_l2_write_allocate++; break;
>     default: assert(0);
>     }
> 
>    // The packet size varies depending on the type of request: 
>    // - For write request and atomic request, the packet contains the data 
>    // - For read request (i.e. not write nor atomic), the packet only has control metadata
>    unsigned int packet_size = mf->size(); 
>    if (!mf->get_is_write() && !mf->isatomic()) {
>       packet_size = mf->get_ctrl_size(); 
>    }
>    m_stats->m_outgoing_traffic_stats->record_traffic(mf, packet_size); 
>    unsigned destination = mf->get_sub_partition_id();
>    mf->set_status(IN_ICNT_TO_MEM,gpu_sim_cycle+gpu_tot_sim_cycle);
>    if (!mf->get_is_write() && !mf->isatomic())
>       ::icnt_push(m_cluster_id, m_config->mem2device(destination), (void*)mf, mf->get_ctrl_size() );
>    else 
>       ::icnt_push(m_cluster_id, m_config->mem2device(destination), (void*)mf, mf->size());
3306,3321c3305,3338
<   if ( !m_response_fifo.empty() ) {
<     mem_fetch *mf = m_response_fifo.front();
<     unsigned cid = m_config->sid_to_cid(mf->get_sid());
<     if ( mf->get_access_type() == INST_ACC_R ) {
<       // instruction fetch response
<       if ( !m_core[cid]->fetch_unit_response_buffer_full() ) {
<         m_response_fifo.pop_front();
<         m_core[cid]->accept_fetch_response(mf);
<       }
<     } else {
<       // data response
<       if ( !m_core[cid]->ldst_unit_response_buffer_full() ) {
<         m_response_fifo.pop_front();
<         m_memory_stats->memlatstat_read_done(mf);
<         m_core[cid]->accept_ldst_unit_response(mf);
<       }
---
>     if( !m_response_fifo.empty() ) {
>         mem_fetch *mf = m_response_fifo.front();
>         unsigned cid = m_config->sid_to_cid(mf->get_sid());
>         if( mf->get_access_type() == INST_ACC_R ) {
>             // instruction fetch response
>             if( !m_core[cid]->fetch_unit_response_buffer_full() ) {
>                 m_response_fifo.pop_front();
>                 m_core[cid]->accept_fetch_response(mf);
>             }
>         } else {
>             // data response
>             if( !m_core[cid]->ldst_unit_response_buffer_full() ) {
>                 m_response_fifo.pop_front();
>                 m_memory_stats->memlatstat_read_done(mf);
>                 m_core[cid]->accept_ldst_unit_response(mf);
>             }
>         }
>     }
>     if( m_response_fifo.size() < m_config->n_simt_ejection_buffer_size ) {
>         mem_fetch *mf = (mem_fetch*) ::icnt_pop(m_cluster_id);
>         if (!mf) 
>             return;
>         assert(mf->get_tpc() == m_cluster_id);
>         assert(mf->get_type() == READ_REPLY || mf->get_type() == WRITE_ACK );
> 
>         // The packet size varies depending on the type of request: 
>         // - For read request and atomic request, the packet contains the data 
>         // - For write-ack, the packet only has control metadata
>         unsigned int packet_size = (mf->get_is_write())? mf->get_ctrl_size() : mf->size(); 
>         m_stats->m_incoming_traffic_stats->record_traffic(mf, packet_size); 
>         mf->set_status(IN_CLUSTER_TO_SHADER_QUEUE,gpu_sim_cycle+gpu_tot_sim_cycle);
>         //m_memory_stats->memlatstat_read_done(mf,m_shader_config->max_warps_per_shader);
>         m_response_fifo.push_back(mf);
>         m_stats->n_mem_to_simt[m_cluster_id] += mf->get_num_flits(false);
3323,3340d3339
<   }
<   if ( m_response_fifo.size() < m_config->n_simt_ejection_buffer_size ) {
<     mem_fetch *mf = (mem_fetch*) ::icnt_pop(m_cluster_id);
<     if (!mf)
<       return;
<     assert(mf->get_tpc() == m_cluster_id);
<     assert(mf->get_type() == READ_REPLY || mf->get_type() == WRITE_ACK );
< 
<     // The packet size varies depending on the type of request:
<     // - For read request and atomic request, the packet contains the data
<     // - For write-ack, the packet only has control metadata
<     unsigned int packet_size = (mf->get_is_write()) ? mf->get_ctrl_size() : mf->size();
<     m_stats->m_incoming_traffic_stats->record_traffic(mf, packet_size);
<     mf->set_status(IN_CLUSTER_TO_SHADER_QUEUE, gpu_sim_cycle + gpu_tot_sim_cycle);
<     //m_memory_stats->memlatstat_read_done(mf,m_shader_config->max_warps_per_shader);
<     m_response_fifo.push_back(mf);
<     m_stats->n_mem_to_simt[m_cluster_id] += mf->get_num_flits(false);
<   }
3345,3346c3344,3345
<   unsigned cid = m_config->sid_to_cid(sid);
<   m_core[cid]->get_pdom_stack_top_info(tid, pc, rpc);
---
>     unsigned cid = m_config->sid_to_cid(sid);
>     m_core[cid]->get_pdom_stack_top_info(tid,pc,rpc);
3351c3350
<   m_core[m_config->sid_to_cid(sid)]->display_pipeline(fout, print_mem, mask);
---
>     m_core[m_config->sid_to_cid(sid)]->display_pipeline(fout,print_mem,mask);
3353,3359c3352,3358
<   fprintf(fout, "\n");
<   fprintf(fout, "Cluster %u pipeline state\n", m_cluster_id );
<   fprintf(fout, "Response FIFO (occupancy = %zu):\n", m_response_fifo.size() );
<   for ( std::list<mem_fetch*>::const_iterator i = m_response_fifo.begin(); i != m_response_fifo.end(); i++ ) {
<     const mem_fetch *mf = *i;
<     mf->print(fout);
<   }
---
>     fprintf(fout,"\n");
>     fprintf(fout,"Cluster %u pipeline state\n", m_cluster_id );
>     fprintf(fout,"Response FIFO (occupancy = %zu):\n", m_response_fifo.size() );
>     for( std::list<mem_fetch*>::const_iterator i=m_response_fifo.begin(); i != m_response_fifo.end(); i++ ) {
>         const mem_fetch *mf = *i;
>         mf->print(fout);
>     }
3363,3365c3362,3364
<   for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
<     m_core[ i ]->print_cache_stats( fp, dl1_accesses, dl1_misses );
<   }
---
>    for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
>       m_core[ i ]->print_cache_stats( fp, dl1_accesses, dl1_misses );
>    }
3369,3375c3368,3374
<   long simt_to_mem = 0;
<   long mem_to_simt = 0;
<   for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
<     m_core[i]->get_icnt_power_stats(simt_to_mem, mem_to_simt);
<   }
<   n_simt_to_mem = simt_to_mem;
<   n_mem_to_simt = mem_to_simt;
---
> 	long simt_to_mem=0;
> 	long mem_to_simt=0;
> 	for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
> 		m_core[i]->get_icnt_power_stats(simt_to_mem, mem_to_simt);
> 	}
> 	n_simt_to_mem = simt_to_mem;
> 	n_mem_to_simt = mem_to_simt;
3378,3381c3377,3380
< void simt_core_cluster::get_cache_stats(cache_stats &cs) const {
<   for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
<     m_core[i]->get_cache_stats(cs);
<   }
---
> void simt_core_cluster::get_cache_stats(cache_stats &cs) const{
>     for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
>         m_core[i]->get_cache_stats(cs);
>     }
3384,3426c3383,3425
< void simt_core_cluster::get_L1I_sub_stats(struct cache_sub_stats &css) const {
<   struct cache_sub_stats temp_css;
<   struct cache_sub_stats total_css;
<   temp_css.clear();
<   total_css.clear();
<   for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
<     m_core[i]->get_L1I_sub_stats(temp_css);
<     total_css += temp_css;
<   }
<   css = total_css;
< }
< void simt_core_cluster::get_L1D_sub_stats(struct cache_sub_stats &css) const {
<   struct cache_sub_stats temp_css;
<   struct cache_sub_stats total_css;
<   temp_css.clear();
<   total_css.clear();
<   for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
<     m_core[i]->get_L1D_sub_stats(temp_css);
<     total_css += temp_css;
<   }
<   css = total_css;
< }
< void simt_core_cluster::get_L1C_sub_stats(struct cache_sub_stats &css) const {
<   struct cache_sub_stats temp_css;
<   struct cache_sub_stats total_css;
<   temp_css.clear();
<   total_css.clear();
<   for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
<     m_core[i]->get_L1C_sub_stats(temp_css);
<     total_css += temp_css;
<   }
<   css = total_css;
< }
< void simt_core_cluster::get_L1T_sub_stats(struct cache_sub_stats &css) const {
<   struct cache_sub_stats temp_css;
<   struct cache_sub_stats total_css;
<   temp_css.clear();
<   total_css.clear();
<   for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
<     m_core[i]->get_L1T_sub_stats(temp_css);
<     total_css += temp_css;
<   }
<   css = total_css;
---
> void simt_core_cluster::get_L1I_sub_stats(struct cache_sub_stats &css) const{
>     struct cache_sub_stats temp_css;
>     struct cache_sub_stats total_css;
>     temp_css.clear();
>     total_css.clear();
>     for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
>         m_core[i]->get_L1I_sub_stats(temp_css);
>         total_css += temp_css;
>     }
>     css = total_css;
> }
> void simt_core_cluster::get_L1D_sub_stats(struct cache_sub_stats &css) const{
>     struct cache_sub_stats temp_css;
>     struct cache_sub_stats total_css;
>     temp_css.clear();
>     total_css.clear();
>     for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
>         m_core[i]->get_L1D_sub_stats(temp_css);
>         total_css += temp_css;
>     }
>     css = total_css;
> }
> void simt_core_cluster::get_L1C_sub_stats(struct cache_sub_stats &css) const{
>     struct cache_sub_stats temp_css;
>     struct cache_sub_stats total_css;
>     temp_css.clear();
>     total_css.clear();
>     for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
>         m_core[i]->get_L1C_sub_stats(temp_css);
>         total_css += temp_css;
>     }
>     css = total_css;
> }
> void simt_core_cluster::get_L1T_sub_stats(struct cache_sub_stats &css) const{
>     struct cache_sub_stats temp_css;
>     struct cache_sub_stats total_css;
>     temp_css.clear();
>     total_css.clear();
>     for ( unsigned i = 0; i < m_config->n_simt_cores_per_cluster; ++i ) {
>         m_core[i]->get_L1T_sub_stats(temp_css);
>         total_css += temp_css;
>     }
>     css = total_css;
3431,3443c3430,3442
<   if (inst.isatomic())
<     m_warp[inst.warp_id()].inc_n_atomic();
<   if (inst.space.is_local() && (inst.is_load() || inst.is_store())) {
<     new_addr_type localaddrs[MAX_ACCESSES_PER_INSN_PER_THREAD];
<     unsigned num_addrs;
<     num_addrs = translate_local_memaddr(inst.get_addr(t), tid, m_config->n_simt_clusters * m_config->n_simt_cores_per_cluster,
<                                         inst.data_size, (new_addr_type*) localaddrs );
<     inst.set_addr(t, (new_addr_type*) localaddrs, num_addrs);
<   }
<   if ( ptx_thread_done(tid) ) {
<     m_warp[inst.warp_id()].set_completed(t);
<     m_warp[inst.warp_id()].ibuffer_flush();
<   }
---
>     if(inst.isatomic())
>            m_warp[inst.warp_id()].inc_n_atomic();
>         if (inst.space.is_local() && (inst.is_load() || inst.is_store())) {
>             new_addr_type localaddrs[MAX_ACCESSES_PER_INSN_PER_THREAD];
>             unsigned num_addrs;
>             num_addrs = translate_local_memaddr(inst.get_addr(t), tid, m_config->n_simt_clusters*m_config->n_simt_cores_per_cluster,
>                    inst.data_size, (new_addr_type*) localaddrs );
>             inst.set_addr(t, (new_addr_type*) localaddrs, num_addrs);
>         }
>         if ( ptx_thread_done(tid) ) {
>             m_warp[inst.warp_id()].set_completed(t);
>             m_warp[inst.warp_id()].ibuffer_flush();
>         }
3445,3451c3444,3451
<   // PC-Histogram Update
<   unsigned warp_id = inst.warp_id();
<   unsigned pc = inst.pc;
<   for (unsigned t = 0; t < m_config->warp_size; t++) {
<     if (inst.active(t)) {
<       int tid = warp_id * m_config->warp_size + t;
<       cflog_update_thread_pc(m_sid, tid, pc);
---
>     // PC-Histogram Update 
>     unsigned warp_id = inst.warp_id(); 
>     unsigned pc = inst.pc; 
>     for (unsigned t = 0; t < m_config->warp_size; t++) {
>         if (inst.active(t)) {
>             int tid = warp_id * m_config->warp_size + t; 
>             cflog_update_thread_pc(m_sid, tid, pc);  
>         }
3453d3452
<   }
diff -r gpgpu-sim/shader.h ../../../gpgpu-sim_distribution/src/gpgpu-sim/shader.h
2c2
< // Ali Bakhoda
---
> // Ali Bakhoda 
49,50c49
< //#include "dram.h"
< #include  "mem_latency_stat.h"
---
> #include "dram.h"
64c63
<           2 bytes   [shaderid + mshrid](14 bits) + req_size(0-2 bits if req_size variable) - so up to 2^14 = 16384 mshr total
---
>           2 bytes   [shaderid + mshrid](14 bits) + req_size(0-2 bits if req_size variable) - so up to 2^14 = 16384 mshr total 
69c68
< //WRITE_PACKET_SIZE: bytes: 6 address, 2 miscelaneous.
---
> //WRITE_PACKET_SIZE: bytes: 6 address, 2 miscelaneous. 
77c76
<     unsigned m_cta_id; // hardware CTA this thread belongs
---
>    unsigned m_cta_id; // hardware CTA this thread belongs
79,84c78,83
<     // per thread stats (ac stands for accumulative).
<     unsigned n_insn;
<     unsigned n_insn_ac;
<     unsigned n_l1_mis_ac;
<     unsigned n_l1_mrghit_ac;
<     unsigned n_l1_access_ac;
---
>    // per thread stats (ac stands for accumulative).
>    unsigned n_insn;
>    unsigned n_insn_ac;
>    unsigned n_l1_mis_ac;
>    unsigned n_l1_mrghit_ac;
>    unsigned n_l1_access_ac; 
86c85
<     bool m_active;
---
>    bool m_active; 
91c90
<     shd_warp_t( class shader_core_ctx *shader, unsigned warp_size)
---
>     shd_warp_t( class shader_core_ctx *shader, unsigned warp_size) 
94,96c93,95
<         m_stores_outstanding = 0;
<         m_inst_in_pipeline = 0;
<         reset();
---
>         m_stores_outstanding=0;
>         m_inst_in_pipeline=0;
>         reset(); 
100,111c99,110
<         assert( m_stores_outstanding == 0);
<         assert( m_inst_in_pipeline == 0);
<         m_imiss_pending = false;
<         m_warp_id = (unsigned) - 1;
<         m_dynamic_warp_id = (unsigned) - 1;
<         n_completed = m_warp_size;
<         m_n_atomic = 0;
<         m_membar = false;
<         m_done_exit = true;
<         m_last_fetch = 0;
<         m_next = 0;
<         m_inst_at_barrier = NULL;
---
>         assert( m_stores_outstanding==0);
>         assert( m_inst_in_pipeline==0);
>         m_imiss_pending=false;
>         m_warp_id=(unsigned)-1;
>         m_dynamic_warp_id = (unsigned)-1;
>         n_completed = m_warp_size; 
>         m_n_atomic=0;
>         m_membar=false;
>         m_done_exit=true;
>         m_last_fetch=0;
>         m_next=0;
>         m_inst_at_barrier=NULL;
119,122c118,121
<         m_cta_id = cta_id;
<         m_warp_id = wid;
<         m_dynamic_warp_id = dynamic_warp_id;
<         m_next_pc = start_pc;
---
>         m_cta_id=cta_id;
>         m_warp_id=wid;
>         m_dynamic_warp_id=dynamic_warp_id;
>         m_next_pc=start_pc;
127c126
<         m_done_exit = false;
---
>         m_done_exit=false;
135c134
<     void set_done_exit() { m_done_exit = true; }
---
>     void set_done_exit() { m_done_exit=true; }
141,142c140,141
<     void set_completed( unsigned lane )
<     {
---
>     void set_completed( unsigned lane ) 
>     { 
145c144
<         n_completed++;
---
>         n_completed++; 
148c147
<     void set_last_fetch( unsigned long long sim_cycle ) { m_last_fetch = sim_cycle; }
---
>     void set_last_fetch( unsigned long long sim_cycle ) { m_last_fetch=sim_cycle; }
152c151
<     void dec_n_atomic(unsigned n) { m_n_atomic -= n; }
---
>     void dec_n_atomic(unsigned n) { m_n_atomic-=n; }
154,155c153,154
<     void set_membar() { m_membar = true; }
<     void clear_membar() { m_membar = false; }
---
>     void set_membar() { m_membar=true; }
>     void clear_membar() { m_membar=false; }
160,161c159,160
<     void store_info_of_last_inst_at_barrier(const warp_inst_t *pI) { m_inst_at_barrier = pI;}
<     const warp_inst_t * restore_info_of_last_inst_at_barrier() { return m_inst_at_barrier;}
---
>     void store_info_of_last_inst_at_barrier(const warp_inst_t *pI){ m_inst_at_barrier = pI;}
>     const warp_inst_t * restore_info_of_last_inst_at_barrier(){ return m_inst_at_barrier;}
165,168c164,167
<         assert(slot < IBUFFER_SIZE );
<         m_ibuffer[slot].m_inst = pI;
<         m_ibuffer[slot].m_valid = true;
<         m_next = 0;
---
>        assert(slot < IBUFFER_SIZE );
>        m_ibuffer[slot].m_inst=pI;
>        m_ibuffer[slot].m_valid=true;
>        m_next=0; 
172,173c171,172
<         for ( unsigned i = 0; i < IBUFFER_SIZE; i++)
<             if (m_ibuffer[i].m_valid)
---
>         for( unsigned i=0; i < IBUFFER_SIZE; i++) 
>             if(m_ibuffer[i].m_valid) 
179,180c178,179
<         for (unsigned i = 0; i < IBUFFER_SIZE; i++) {
<             if ( m_ibuffer[i].m_valid )
---
>         for(unsigned i=0;i<IBUFFER_SIZE;i++) {
>             if( m_ibuffer[i].m_valid )
182,183c181,182
<             m_ibuffer[i].m_inst = NULL;
<             m_ibuffer[i].m_valid = false;
---
>             m_ibuffer[i].m_inst=NULL; 
>             m_ibuffer[i].m_valid=false; 
193c192
<     void ibuffer_step() { m_next = (m_next + 1) % IBUFFER_SIZE; }
---
>     void ibuffer_step() { m_next = (m_next+1)%IBUFFER_SIZE; }
196,197c195,196
<     void set_imiss_pending() { m_imiss_pending = true; }
<     void clear_imiss_pending() { m_imiss_pending = false; }
---
>     void set_imiss_pending() { m_imiss_pending=true; }
>     void clear_imiss_pending() { m_imiss_pending=false; }
201c200
<     void dec_store_req()
---
>     void dec_store_req() 
209,212c208,211
<         unsigned count = 0;
<         for (unsigned i = 0; i < IBUFFER_SIZE; i++) {
<             if ( m_ibuffer[i].m_valid )
<                 count++;
---
>     	unsigned count=0;
>         for(unsigned i=0;i<IBUFFER_SIZE;i++) {
>             if( m_ibuffer[i].m_valid )
>             	count++;
214c213
<         return count;
---
>     	return count;
217c216
<     unsigned num_issued_inst_in_pipeline() const {return (num_inst_in_pipeline() - num_inst_in_buffer());}
---
>     unsigned num_issued_inst_in_pipeline() const {return (num_inst_in_pipeline()-num_inst_in_buffer());}
220c219
<     void dec_inst_in_pipeline()
---
>     void dec_inst_in_pipeline() 
232c231
<     static const unsigned IBUFFER_SIZE = 2;
---
>     static const unsigned IBUFFER_SIZE=2;
244c243
< 
---
>     
246,248c245,247
<         ibuffer_entry() { m_valid = false; m_inst = NULL; }
<         const warp_inst_t *m_inst;
<         bool m_valid;
---
>        ibuffer_entry() { m_valid = false; m_inst = NULL; }
>        const warp_inst_t *m_inst;
>        bool m_valid;
252c251
<     ibuffer_entry m_ibuffer[IBUFFER_SIZE];
---
>     ibuffer_entry m_ibuffer[IBUFFER_SIZE]; 
254,255c253,254
< 
<     unsigned m_n_atomic;           // number of outstanding atomic operations
---
>                                    
>     unsigned m_n_atomic;           // number of outstanding atomic operations 
268,269c267,268
< inline unsigned hw_tid_from_wid(unsigned wid, unsigned warp_size, unsigned i) {return wid * warp_size + i;};
< inline unsigned wid_from_hw_tid(unsigned tid, unsigned warp_size) {return tid / warp_size;};
---
> inline unsigned hw_tid_from_wid(unsigned wid, unsigned warp_size, unsigned i){return wid * warp_size + i;};
> inline unsigned wid_from_hw_tid(unsigned tid, unsigned warp_size){return tid/warp_size;};
304,306c303,305
<     scheduler_unit(shader_core_stats* stats, shader_core_ctx* shader,
<                    Scoreboard* scoreboard, simt_stack** simt,
<                    std::vector<shd_warp_t>* warp,
---
>     scheduler_unit(shader_core_stats* stats, shader_core_ctx* shader, 
>                    Scoreboard* scoreboard, simt_stack** simt, 
>                    std::vector<shd_warp_t>* warp, 
310c309
<                    int id)
---
>                    int id) 
312,314c311,313
<           m_scoreboard(scoreboard), m_simt_stack(simt), /*m_pipeline_reg(pipe_regs),*/ m_warp(warp),
<           m_sp_out(sp_out), m_sfu_out(sfu_out), m_mem_out(mem_out), m_id(id) {}
<     virtual ~scheduler_unit() {}
---
>         m_scoreboard(scoreboard), m_simt_stack(simt), /*m_pipeline_reg(pipe_regs),*/ m_warp(warp),
>         m_sp_out(sp_out),m_sfu_out(sfu_out),m_mem_out(mem_out), m_id(id){}
>     virtual ~scheduler_unit(){}
335,336c334,335
< 
<     enum OrderingType
---
>     
>     enum OrderingType 
380c379
<     Scoreboard* m_scoreboard;
---
>     Scoreboard* m_scoreboard; 
393c392
<     lrr_scheduler ( shader_core_stats* stats, shader_core_ctx* shader,
---
> 	lrr_scheduler ( shader_core_stats* stats, shader_core_ctx* shader,
400,402c399,401
<         : scheduler_unit ( stats, shader, scoreboard, simt, warp, sp_out, sfu_out, mem_out, id ) {}
<     virtual ~lrr_scheduler () {}
<     virtual void order_warps ();
---
> 	: scheduler_unit ( stats, shader, scoreboard, simt, warp, sp_out, sfu_out, mem_out, id ){}
> 	virtual ~lrr_scheduler () {}
> 	virtual void order_warps ();
410c409
<     gto_scheduler ( shader_core_stats* stats, shader_core_ctx* shader,
---
> 	gto_scheduler ( shader_core_stats* stats, shader_core_ctx* shader,
417,419c416,418
<         : scheduler_unit ( stats, shader, scoreboard, simt, warp, sp_out, sfu_out, mem_out, id ) {}
<     virtual ~gto_scheduler () {}
<     virtual void order_warps ();
---
> 	: scheduler_unit ( stats, shader, scoreboard, simt, warp, sp_out, sfu_out, mem_out, id ){}
> 	virtual ~gto_scheduler () {}
> 	virtual void order_warps ();
429,438c428,437
<     two_level_active_scheduler ( shader_core_stats* stats, shader_core_ctx* shader,
<                                  Scoreboard* scoreboard, simt_stack** simt,
<                                  std::vector<shd_warp_t>* warp,
<                                  register_set* sp_out,
<                                  register_set* sfu_out,
<                                  register_set* mem_out,
<                                  int id,
<                                  char* config_str )
<         : scheduler_unit ( stats, shader, scoreboard, simt, warp, sp_out, sfu_out, mem_out, id ),
<           m_pending_warps()
---
> 	two_level_active_scheduler ( shader_core_stats* stats, shader_core_ctx* shader,
>                           Scoreboard* scoreboard, simt_stack** simt,
>                           std::vector<shd_warp_t>* warp,
>                           register_set* sp_out,
>                           register_set* sfu_out,
>                           register_set* mem_out,
>                           int id,
>                           char* config_str )
> 	: scheduler_unit ( stats, shader, scoreboard, simt, warp, sp_out, sfu_out, mem_out, id ),
> 	  m_pending_warps() 
441c440
<         unsigned outer_level_readin;
---
>         unsigned outer_level_readin; 
448,449c447,448
<         m_inner_level_prioritization = (scheduler_prioritization_type)inner_level_readin;
<         m_outer_level_prioritization = (scheduler_prioritization_type)outer_level_readin;
---
>         m_inner_level_prioritization=(scheduler_prioritization_type)inner_level_readin;
>         m_outer_level_prioritization=(scheduler_prioritization_type)outer_level_readin;
451c450
<     virtual ~two_level_active_scheduler () {}
---
> 	virtual ~two_level_active_scheduler () {}
453c452
<     void add_supervised_warp_id(int i) {
---
> 	void add_supervised_warp_id(int i) {
457c456
<             m_pending_warps.push_back(&warp(i));
---
> 		    m_pending_warps.push_back(&warp(i));
459c458
<     }
---
> 	}
470c469
<     std::deque< shd_warp_t* > m_pending_warps;
---
> 	std::deque< shd_warp_t* > m_pending_warps;
473c472
<     unsigned m_max_active_warps;
---
> 	unsigned m_max_active_warps;
479c478
<     swl_scheduler ( shader_core_stats* stats, shader_core_ctx* shader,
---
> 	swl_scheduler ( shader_core_stats* stats, shader_core_ctx* shader,
487,488c486,487
<     virtual ~swl_scheduler () {}
<     virtual void order_warps ();
---
> 	virtual ~swl_scheduler () {}
> 	virtual void order_warps ();
502,520c501,519
<     // constructors
<     opndcoll_rfu_t()
<     {
<         m_num_banks = 0;
<         m_shader = NULL;
<         m_initialized = false;
<     }
<     void add_cu_set(unsigned cu_set, unsigned num_cu, unsigned num_dispatch);
<     typedef std::vector<register_set*> port_vector_t;
<     typedef std::vector<unsigned int> uint_vector_t;
<     void add_port( port_vector_t & input, port_vector_t & ouput, uint_vector_t cu_sets);
<     void init( unsigned num_banks, shader_core_ctx *shader );
< 
<     // modifiers
<     bool writeback( const warp_inst_t &warp ); // might cause stall
< 
<     void step()
<     {
<         dispatch_ready_cu();
---
>    // constructors
>    opndcoll_rfu_t()
>    {
>       m_num_banks=0;
>       m_shader=NULL;
>       m_initialized=false;
>    }
>    void add_cu_set(unsigned cu_set, unsigned num_cu, unsigned num_dispatch);
>    typedef std::vector<register_set*> port_vector_t;
>    typedef std::vector<unsigned int> uint_vector_t;
>    void add_port( port_vector_t & input, port_vector_t & ouput, uint_vector_t cu_sets);
>    void init( unsigned num_banks, shader_core_ctx *shader );
> 
>    // modifiers
>    bool writeback( const warp_inst_t &warp ); // might cause stall 
> 
>    void step()
>    {
>         dispatch_ready_cu();   
522c521
<         for ( unsigned p = 0 ; p < m_in_ports.size(); p++ )
---
>         for( unsigned p = 0 ; p < m_in_ports.size(); p++ ) 
525c524
<     }
---
>    }
527,536c526,535
<     void dump( FILE *fp ) const
<     {
<         fprintf(fp, "\n");
<         fprintf(fp, "Operand Collector State:\n");
<         for ( unsigned n = 0; n < m_cu.size(); n++ ) {
<             fprintf(fp, "   CU-%2u: ", n);
<             m_cu[n]->dump(fp, m_shader);
<         }
<         m_arbiter.dump(fp);
<     }
---
>    void dump( FILE *fp ) const
>    {
>       fprintf(fp,"\n");
>       fprintf(fp,"Operand Collector State:\n");
>       for( unsigned n=0; n < m_cu.size(); n++ ) {
>          fprintf(fp,"   CU-%2u: ", n);
>          m_cu[n]->dump(fp,m_shader);
>       }
>       m_arbiter.dump(fp);
>    }
538c537
<     shader_core_ctx *shader_core() { return m_shader; }
---
>    shader_core_ctx *shader_core() { return m_shader; }
542,707c541,704
<     void process_banks()
<     {
<         m_arbiter.reset_alloction();
<     }
< 
<     void dispatch_ready_cu();
<     void allocate_cu( unsigned port );
<     void allocate_reads();
< 
<     // types
< 
<     class collector_unit_t;
< 
<     class op_t {
<     public:
< 
<         op_t() { m_valid = false; }
<         op_t( collector_unit_t *cu, unsigned op, unsigned reg, unsigned num_banks, unsigned bank_warp_shift )
<         {
<             m_valid = true;
<             m_warp = NULL;
<             m_cu = cu;
<             m_operand = op;
<             m_register = reg;
<             m_bank = register_bank(reg, cu->get_warp_id(), num_banks, bank_warp_shift);
<         }
<         op_t( const warp_inst_t *warp, unsigned reg, unsigned num_banks, unsigned bank_warp_shift )
<         {
<             m_valid = true;
<             m_warp = warp;
<             m_register = reg;
<             m_cu = NULL;
<             m_operand = -1;
<             m_bank = register_bank(reg, warp->warp_id(), num_banks, bank_warp_shift);
<         }
< 
<         // accessors
<         bool valid() const { return m_valid; }
<         unsigned get_reg() const
<         {
<             assert( m_valid );
<             return m_register;
<         }
<         unsigned get_wid() const
<         {
<             if ( m_warp ) return m_warp->warp_id();
<             else if ( m_cu ) return m_cu->get_warp_id();
<             else abort();
<         }
<         unsigned get_active_count() const
<         {
<             if ( m_warp ) return m_warp->active_count();
<             else if ( m_cu ) return m_cu->get_active_count();
<             else abort();
<         }
<         const active_mask_t & get_active_mask()
<         {
<             if ( m_warp ) return m_warp->get_active_mask();
<             else if ( m_cu ) return m_cu->get_active_mask();
<             else abort();
<         }
<         unsigned get_sp_op() const
<         {
<             if ( m_warp ) return m_warp->sp_op;
<             else if ( m_cu ) return m_cu->get_sp_op();
<             else abort();
<         }
<         unsigned get_oc_id() const { return m_cu->get_id(); }
<         unsigned get_bank() const { return m_bank; }
<         unsigned get_operand() const { return m_operand; }
<         void dump(FILE *fp) const
<         {
<             if (m_cu)
<                 fprintf(fp, " <R%u, CU:%u, w:%02u> ", m_register, m_cu->get_id(), m_cu->get_warp_id());
<             else if ( !m_warp->empty() )
<                 fprintf(fp, " <R%u, wid:%02u> ", m_register, m_warp->warp_id() );
<         }
<         std::string get_reg_string() const
<         {
<             char buffer[64];
<             snprintf(buffer, 64, "R%u", m_register);
<             return std::string(buffer);
<         }
< 
<         // modifiers
<         void reset() { m_valid = false; }
<     private:
<         bool m_valid;
<         collector_unit_t  *m_cu;
<         const warp_inst_t *m_warp;
<         unsigned  m_operand; // operand offset in instruction. e.g., add r1,r2,r3; r2 is oprd 0, r3 is 1 (r1 is dst)
<         unsigned  m_register;
<         unsigned  m_bank;
<     };
< 
<     enum alloc_t {
<         NO_ALLOC,
<         READ_ALLOC,
<         WRITE_ALLOC,
<     };
< 
<     class allocation_t {
<     public:
<         allocation_t() { m_allocation = NO_ALLOC; }
<         bool is_read() const { return m_allocation == READ_ALLOC; }
<         bool is_write() const {return m_allocation == WRITE_ALLOC; }
<         bool is_free() const {return m_allocation == NO_ALLOC; }
<         void dump(FILE *fp) const {
<             if ( m_allocation == NO_ALLOC ) { fprintf(fp, "<free>"); }
<             else if ( m_allocation == READ_ALLOC ) { fprintf(fp, "rd: "); m_op.dump(fp); }
<             else if ( m_allocation == WRITE_ALLOC ) { fprintf(fp, "wr: "); m_op.dump(fp); }
<             fprintf(fp, "\n");
<         }
<         void alloc_read( const op_t &op )  { assert(is_free()); m_allocation = READ_ALLOC; m_op = op; }
<         void alloc_write( const op_t &op ) { assert(is_free()); m_allocation = WRITE_ALLOC; m_op = op; }
<         void reset() { m_allocation = NO_ALLOC; }
<     private:
<         enum alloc_t m_allocation;
<         op_t m_op;
<     };
< 
<     class arbiter_t {
<     public:
<         // constructors
<         arbiter_t()
<         {
<             m_queue = NULL;
<             m_allocated_bank = NULL;
<             m_allocator_rr_head = NULL;
<             _inmatch = NULL;
<             _outmatch = NULL;
<             _request = NULL;
<             m_last_cu = 0;
<         }
<         void init( unsigned num_cu, unsigned num_banks )
<         {
<             assert(num_cu > 0);
<             assert(num_banks > 0);
<             m_num_collectors = num_cu;
<             m_num_banks = num_banks;
<             _inmatch = new int[ m_num_banks ];
<             _outmatch = new int[ m_num_collectors ];
<             _request = new int*[ m_num_banks ];
<             for (unsigned i = 0; i < m_num_banks; i++)
<                 _request[i] = new int[m_num_collectors];
<             m_queue = new std::list<op_t>[num_banks];
<             m_allocated_bank = new allocation_t[num_banks];
<             m_allocator_rr_head = new unsigned[num_cu];
<             for ( unsigned n = 0; n < num_cu; n++ )
<                 m_allocator_rr_head[n] = n % num_banks;
<             reset_alloction();
<         }
< 
<         // accessors
<         void dump(FILE *fp) const
<         {
<             fprintf(fp, "\n");
<             fprintf(fp, "  Arbiter State:\n");
<             fprintf(fp, "  requests:\n");
<             for ( unsigned b = 0; b < m_num_banks; b++ ) {
<                 fprintf(fp, "    bank %u : ", b );
<                 std::list<op_t>::const_iterator o = m_queue[b].begin();
<                 for (; o != m_queue[b].end(); o++ ) {
<                     o->dump(fp);
<                 }
<                 fprintf(fp, "\n");
---
>    void process_banks()
>    {
>       m_arbiter.reset_alloction();
>    }
> 
>    void dispatch_ready_cu();
>    void allocate_cu( unsigned port );
>    void allocate_reads();
> 
>    // types
> 
>    class collector_unit_t;
> 
>    class op_t {
>    public:
> 
>       op_t() { m_valid = false; }
>       op_t( collector_unit_t *cu, unsigned op, unsigned reg, unsigned num_banks, unsigned bank_warp_shift )
>       {
>          m_valid = true;
>          m_warp=NULL;
>          m_cu = cu;
>          m_operand = op;
>          m_register = reg;
>          m_bank = register_bank(reg,cu->get_warp_id(),num_banks,bank_warp_shift);
>       }
>       op_t( const warp_inst_t *warp, unsigned reg, unsigned num_banks, unsigned bank_warp_shift )
>       {
>          m_valid=true;
>          m_warp=warp;
>          m_register=reg;
>          m_cu=NULL;
>          m_operand = -1;
>          m_bank = register_bank(reg,warp->warp_id(),num_banks,bank_warp_shift);
>       }
> 
>       // accessors
>       bool valid() const { return m_valid; }
>       unsigned get_reg() const
>       {
>          assert( m_valid );
>          return m_register;
>       }
>       unsigned get_wid() const
>       {
>           if( m_warp ) return m_warp->warp_id();
>           else if( m_cu ) return m_cu->get_warp_id();
>           else abort();
>       }
>       unsigned get_active_count() const
>       {
>           if( m_warp ) return m_warp->active_count();
>           else if( m_cu ) return m_cu->get_active_count();
>           else abort();
>       }
>       const active_mask_t & get_active_mask()
>       {
>           if( m_warp ) return m_warp->get_active_mask();
>           else if( m_cu ) return m_cu->get_active_mask();
>           else abort();
>       }
>       unsigned get_sp_op() const
>       {
>           if( m_warp ) return m_warp->sp_op;
>           else if( m_cu ) return m_cu->get_sp_op();
>           else abort();
>       }
>       unsigned get_oc_id() const { return m_cu->get_id(); }
>       unsigned get_bank() const { return m_bank; }
>       unsigned get_operand() const { return m_operand; }
>       void dump(FILE *fp) const 
>       {
>          if(m_cu) 
>             fprintf(fp," <R%u, CU:%u, w:%02u> ", m_register,m_cu->get_id(),m_cu->get_warp_id());
>          else if( !m_warp->empty() )
>             fprintf(fp," <R%u, wid:%02u> ", m_register,m_warp->warp_id() );
>       }
>       std::string get_reg_string() const
>       {
>          char buffer[64];
>          snprintf(buffer,64,"R%u", m_register);
>          return std::string(buffer);
>       }
> 
>       // modifiers
>       void reset() { m_valid = false; }
>    private:
>       bool m_valid;
>       collector_unit_t  *m_cu; 
>       const warp_inst_t *m_warp;
>       unsigned  m_operand; // operand offset in instruction. e.g., add r1,r2,r3; r2 is oprd 0, r3 is 1 (r1 is dst)
>       unsigned  m_register;
>       unsigned  m_bank;
>    };
> 
>    enum alloc_t {
>       NO_ALLOC,
>       READ_ALLOC,
>       WRITE_ALLOC,
>    };
> 
>    class allocation_t {
>    public:
>       allocation_t() { m_allocation = NO_ALLOC; }
>       bool is_read() const { return m_allocation==READ_ALLOC; }
>       bool is_write() const {return m_allocation==WRITE_ALLOC; }
>       bool is_free() const {return m_allocation==NO_ALLOC; }
>       void dump(FILE *fp) const {
>          if( m_allocation == NO_ALLOC ) { fprintf(fp,"<free>"); }
>          else if( m_allocation == READ_ALLOC ) { fprintf(fp,"rd: "); m_op.dump(fp); }
>          else if( m_allocation == WRITE_ALLOC ) { fprintf(fp,"wr: "); m_op.dump(fp); }
>          fprintf(fp,"\n");
>       }
>       void alloc_read( const op_t &op )  { assert(is_free()); m_allocation=READ_ALLOC; m_op=op; }
>       void alloc_write( const op_t &op ) { assert(is_free()); m_allocation=WRITE_ALLOC; m_op=op; }
>       void reset() { m_allocation = NO_ALLOC; }
>    private:
>       enum alloc_t m_allocation;
>       op_t m_op;
>    };
> 
>    class arbiter_t {
>    public:
>       // constructors
>       arbiter_t()
>       {
>          m_queue=NULL;
>          m_allocated_bank=NULL;
>          m_allocator_rr_head=NULL;
>          _inmatch=NULL;
>          _outmatch=NULL;
>          _request=NULL;
>          m_last_cu=0;
>       }
>       void init( unsigned num_cu, unsigned num_banks ) 
>       { 
>          assert(num_cu > 0);
>          assert(num_banks > 0);
>          m_num_collectors = num_cu;
>          m_num_banks = num_banks;
>          _inmatch = new int[ m_num_banks ];
>          _outmatch = new int[ m_num_collectors ];
>          _request = new int*[ m_num_banks ];
>          for(unsigned i=0; i<m_num_banks;i++) 
>              _request[i] = new int[m_num_collectors];
>          m_queue = new std::list<op_t>[num_banks];
>          m_allocated_bank = new allocation_t[num_banks];
>          m_allocator_rr_head = new unsigned[num_cu];
>          for( unsigned n=0; n<num_cu;n++ ) 
>             m_allocator_rr_head[n] = n%num_banks;
>          reset_alloction();
>       }
> 
>       // accessors
>       void dump(FILE *fp) const
>       {
>          fprintf(fp,"\n");
>          fprintf(fp,"  Arbiter State:\n");
>          fprintf(fp,"  requests:\n");
>          for( unsigned b=0; b<m_num_banks; b++ ) {
>             fprintf(fp,"    bank %u : ", b );
>             std::list<op_t>::const_iterator o = m_queue[b].begin();
>             for(; o != m_queue[b].end(); o++ ) {
>                o->dump(fp);
709,728c706,726
<             fprintf(fp, "  grants:\n");
<             for (unsigned b = 0; b < m_num_banks; b++) {
<                 fprintf(fp, "    bank %u : ", b );
<                 m_allocated_bank[b].dump(fp);
<             }
<             fprintf(fp, "\n");
<         }
< 
<         // modifiers
<         std::list<op_t> allocate_reads();
< 
<         void add_read_requests( collector_unit_t *cu )
<         {
<             const op_t *src = cu->get_operands();
<             for ( unsigned i = 0; i < MAX_REG_OPERANDS * 2; i++) {
<                 const op_t &op = src[i];
<                 if ( op.valid() ) {
<                     unsigned bank = op.get_bank();
<                     m_queue[bank].push_back(op);
<                 }
---
>             fprintf(fp,"\n");
>          }
>          fprintf(fp,"  grants:\n");
>          for(unsigned b=0;b<m_num_banks;b++) {
>             fprintf(fp,"    bank %u : ", b );
>             m_allocated_bank[b].dump(fp);
>          }
>          fprintf(fp,"\n");
>       }
> 
>       // modifiers
>       std::list<op_t> allocate_reads(); 
> 
>       void add_read_requests( collector_unit_t *cu ) 
>       {
>          const op_t *src = cu->get_operands();
>          for( unsigned i=0; i<MAX_REG_OPERANDS*2; i++) {
>             const op_t &op = src[i];
>             if( op.valid() ) {
>                unsigned bank = op.get_bank();
>                m_queue[bank].push_back(op);
730,856c728,854
<         }
<         bool bank_idle( unsigned bank ) const
<         {
<             return m_allocated_bank[bank].is_free();
<         }
<         void allocate_bank_for_write( unsigned bank, const op_t &op )
<         {
<             assert( bank < m_num_banks );
<             m_allocated_bank[bank].alloc_write(op);
<         }
<         void allocate_for_read( unsigned bank, const op_t &op )
<         {
<             assert( bank < m_num_banks );
<             m_allocated_bank[bank].alloc_read(op);
<         }
<         void reset_alloction()
<         {
<             for ( unsigned b = 0; b < m_num_banks; b++ )
<                 m_allocated_bank[b].reset();
<         }
< 
<     private:
<         unsigned m_num_banks;
<         unsigned m_num_collectors;
< 
<         allocation_t *m_allocated_bank; // bank # -> register that wins
<         std::list<op_t> *m_queue;
< 
<         unsigned *m_allocator_rr_head; // cu # -> next bank to check for request (rr-arb)
<         unsigned  m_last_cu; // first cu to check while arb-ing banks (rr)
< 
<         int *_inmatch;
<         int *_outmatch;
<         int **_request;
<     };
< 
<     class input_port_t {
<     public:
<         input_port_t(port_vector_t & input, port_vector_t & output, uint_vector_t cu_sets)
<             : m_in(input), m_out(output), m_cu_sets(cu_sets)
<         {
<             assert(input.size() == output.size());
<             assert(not m_cu_sets.empty());
<         }
<         //private:
<         port_vector_t m_in, m_out;
<         uint_vector_t m_cu_sets;
<     };
< 
<     class collector_unit_t {
<     public:
<         // constructors
<         collector_unit_t()
<         {
<             m_free = true;
<             m_warp = NULL;
<             m_output_register = NULL;
<             m_src_op = new op_t[MAX_REG_OPERANDS * 2];
<             m_not_ready.reset();
<             m_warp_id = -1;
<             m_num_banks = 0;
<             m_bank_warp_shift = 0;
<         }
<         // accessors
<         bool ready() const;
<         const op_t *get_operands() const { return m_src_op; }
<         void dump(FILE *fp, const shader_core_ctx *shader ) const;
< 
<         unsigned get_warp_id() const { return m_warp_id; }
<         unsigned get_active_count() const { return m_warp->active_count(); }
<         const active_mask_t & get_active_mask() const { return m_warp->get_active_mask(); }
<         unsigned get_sp_op() const { return m_warp->sp_op; }
<         unsigned get_id() const { return m_cuid; } // returns CU hw id
< 
<         // modifiers
<         void init(unsigned n,
<                   unsigned num_banks,
<                   unsigned log2_warp_size,
<                   const core_config *config,
<                   opndcoll_rfu_t *rfu );
<         bool allocate( register_set* pipeline_reg, register_set* output_reg );
< 
<         void collect_operand( unsigned op )
<         {
<             m_not_ready.reset(op);
<         }
<         unsigned get_num_operands() const {
<             return m_warp->get_num_operands();
<         }
<         unsigned get_num_regs() const {
<             return m_warp->get_num_regs();
<         }
<         void dispatch();
<         bool is_free() {return m_free;}
< 
<     private:
<         bool m_free;
<         unsigned m_cuid; // collector unit hw id
<         unsigned m_warp_id;
<         warp_inst_t  *m_warp;
<         register_set* m_output_register; // pipeline register to issue to when ready
<         op_t *m_src_op;
<         std::bitset<MAX_REG_OPERANDS * 2> m_not_ready;
<         unsigned m_num_banks;
<         unsigned m_bank_warp_shift;
<         opndcoll_rfu_t *m_rfu;
< 
<     };
< 
<     class dispatch_unit_t {
<     public:
<         dispatch_unit_t(std::vector<collector_unit_t>* cus)
<         {
<             m_last_cu = 0;
<             m_collector_units = cus;
<             m_num_collectors = (*cus).size();
<             m_next_cu = 0;
<         }
< 
<         collector_unit_t *find_ready()
<         {
<             for ( unsigned n = 0; n < m_num_collectors; n++ ) {
<                 unsigned c = (m_last_cu + n + 1) % m_num_collectors;
<                 if ( (*m_collector_units)[c].ready() ) {
<                     m_last_cu = c;
<                     return &((*m_collector_units)[c]);
<                 }
---
>          }
>       }
>       bool bank_idle( unsigned bank ) const
>       {
>           return m_allocated_bank[bank].is_free();
>       }
>       void allocate_bank_for_write( unsigned bank, const op_t &op )
>       {
>          assert( bank < m_num_banks );
>          m_allocated_bank[bank].alloc_write(op);
>       }
>       void allocate_for_read( unsigned bank, const op_t &op )
>       {
>          assert( bank < m_num_banks );
>          m_allocated_bank[bank].alloc_read(op);
>       }
>       void reset_alloction()
>       {
>          for( unsigned b=0; b < m_num_banks; b++ ) 
>             m_allocated_bank[b].reset();
>       }
> 
>    private:
>       unsigned m_num_banks;
>       unsigned m_num_collectors;
> 
>       allocation_t *m_allocated_bank; // bank # -> register that wins
>       std::list<op_t> *m_queue;
> 
>       unsigned *m_allocator_rr_head; // cu # -> next bank to check for request (rr-arb)
>       unsigned  m_last_cu; // first cu to check while arb-ing banks (rr)
> 
>       int *_inmatch;
>       int *_outmatch;
>       int **_request;
>    };
> 
>    class input_port_t {
>    public:
>        input_port_t(port_vector_t & input, port_vector_t & output, uint_vector_t cu_sets)
>        : m_in(input),m_out(output), m_cu_sets(cu_sets)
>        {
>            assert(input.size() == output.size());
>            assert(not m_cu_sets.empty());
>        }
>    //private:
>        port_vector_t m_in,m_out;
>        uint_vector_t m_cu_sets;
>    };
> 
>    class collector_unit_t {
>    public:
>       // constructors
>       collector_unit_t()
>       { 
>          m_free = true;
>          m_warp = NULL;
>          m_output_register = NULL;
>          m_src_op = new op_t[MAX_REG_OPERANDS*2];
>          m_not_ready.reset();
>          m_warp_id = -1;
>          m_num_banks = 0;
>          m_bank_warp_shift = 0;
>       }
>       // accessors
>       bool ready() const;
>       const op_t *get_operands() const { return m_src_op; }
>       void dump(FILE *fp, const shader_core_ctx *shader ) const;
> 
>       unsigned get_warp_id() const { return m_warp_id; }
>       unsigned get_active_count() const { return m_warp->active_count(); }
>       const active_mask_t & get_active_mask() const { return m_warp->get_active_mask(); }
>       unsigned get_sp_op() const { return m_warp->sp_op; }
>       unsigned get_id() const { return m_cuid; } // returns CU hw id
> 
>       // modifiers
>       void init(unsigned n, 
>                 unsigned num_banks, 
>                 unsigned log2_warp_size,
>                 const core_config *config,
>                 opndcoll_rfu_t *rfu ); 
>       bool allocate( register_set* pipeline_reg, register_set* output_reg );
> 
>       void collect_operand( unsigned op )
>       {
>          m_not_ready.reset(op);
>       }
>       unsigned get_num_operands() const{
>     	  return m_warp->get_num_operands();
>       }
>       unsigned get_num_regs() const{
>     	  return m_warp->get_num_regs();
>       }
>       void dispatch();
>       bool is_free(){return m_free;}
> 
>    private:
>       bool m_free;
>       unsigned m_cuid; // collector unit hw id
>       unsigned m_warp_id;
>       warp_inst_t  *m_warp;
>       register_set* m_output_register; // pipeline register to issue to when ready
>       op_t *m_src_op;
>       std::bitset<MAX_REG_OPERANDS*2> m_not_ready;
>       unsigned m_num_banks;
>       unsigned m_bank_warp_shift;
>       opndcoll_rfu_t *m_rfu;
> 
>    };
> 
>    class dispatch_unit_t {
>    public:
>       dispatch_unit_t(std::vector<collector_unit_t>* cus) 
>       { 
>          m_last_cu=0;
>          m_collector_units=cus;
>          m_num_collectors = (*cus).size();
>          m_next_cu=0;
>       }
> 
>       collector_unit_t *find_ready()
>       {
>          for( unsigned n=0; n < m_num_collectors; n++ ) {
>             unsigned c=(m_last_cu+n+1)%m_num_collectors;
>             if( (*m_collector_units)[c].ready() ) {
>                m_last_cu=c;
>                return &((*m_collector_units)[c]);
858,893c856,892
<             return NULL;
<         }
< 
<     private:
<         unsigned m_num_collectors;
<         std::vector<collector_unit_t>* m_collector_units;
<         unsigned m_last_cu; // dispatch ready cu's rr
<         unsigned m_next_cu;  // for initialization
<     };
< 
<     // opndcoll_rfu_t data members
<     bool m_initialized;
< 
<     unsigned m_num_collector_sets;
<     //unsigned m_num_collectors;
<     unsigned m_num_banks;
<     unsigned m_bank_warp_shift;
<     unsigned m_warp_size;
<     std::vector<collector_unit_t *> m_cu;
<     arbiter_t m_arbiter;
< 
<     //unsigned m_num_ports;
<     //std::vector<warp_inst_t**> m_input;
<     //std::vector<warp_inst_t**> m_output;
<     //std::vector<unsigned> m_num_collector_units;
<     //warp_inst_t **m_alu_port;
< 
<     std::vector<input_port_t> m_in_ports;
<     typedef std::map<unsigned /* collector set */, std::vector<collector_unit_t> /*collector sets*/ > cu_sets_t;
<     cu_sets_t m_cus;
<     std::vector<dispatch_unit_t> m_dispatch_units;
< 
<     //typedef std::map<warp_inst_t**/*port*/,dispatch_unit_t> port_to_du_t;
<     //port_to_du_t                     m_dispatch_units;
<     //std::map<warp_inst_t**,std::list<collector_unit_t*> > m_free_cu;
<     shader_core_ctx                 *m_shader;
---
>          }
>          return NULL;
>       }
> 
>    private:
>       unsigned m_num_collectors;
>       std::vector<collector_unit_t>* m_collector_units;
>       unsigned m_last_cu; // dispatch ready cu's rr
>       unsigned m_next_cu;  // for initialization
>    };
> 
>    // opndcoll_rfu_t data members
>    bool m_initialized;
> 
>    unsigned m_num_collector_sets;
>    //unsigned m_num_collectors;
>    unsigned m_num_banks;
>    unsigned m_bank_warp_shift;
>    unsigned m_warp_size;
>    std::vector<collector_unit_t *> m_cu;
>    arbiter_t m_arbiter;
> 
>    //unsigned m_num_ports;
>    //std::vector<warp_inst_t**> m_input;
>    //std::vector<warp_inst_t**> m_output;
>    //std::vector<unsigned> m_num_collector_units;
>    //warp_inst_t **m_alu_port;
> 
>    std::vector<input_port_t> m_in_ports;
>    typedef std::map<unsigned /* collector set */, std::vector<collector_unit_t> /*collector sets*/ > cu_sets_t;
>    cu_sets_t m_cus;
>    std::vector<dispatch_unit_t> m_dispatch_units;
> 
>    //typedef std::map<warp_inst_t**/*port*/,dispatch_unit_t> port_to_du_t;
>    //port_to_du_t                     m_dispatch_units;
>    //std::map<warp_inst_t**,std::list<collector_unit_t*> > m_free_cu;
>    shader_core_ctx                 *m_shader;
898c897
<     barrier_set_t(shader_core_ctx * shader, unsigned max_warps_per_core, unsigned max_cta_per_core, unsigned max_barriers_per_cta, unsigned warp_size);
---
>    barrier_set_t(shader_core_ctx * shader, unsigned max_warps_per_core, unsigned max_cta_per_core, unsigned max_barriers_per_cta, unsigned warp_size);
900,901c899,900
<     // during cta allocation
<     void allocate_barrier( unsigned cta_id, warp_set_t warps );
---
>    // during cta allocation
>    void allocate_barrier( unsigned cta_id, warp_set_t warps );
903,904c902,903
<     // during cta deallocation
<     void deallocate_barrier( unsigned cta_id );
---
>    // during cta deallocation
>    void deallocate_barrier( unsigned cta_id );
906,907c905,906
<     typedef std::map<unsigned, warp_set_t >  cta_to_warp_t;
<     typedef std::map<unsigned, warp_set_t >  bar_id_to_warp_t; /*set of warps reached a specific barrier id*/
---
>    typedef std::map<unsigned, warp_set_t >  cta_to_warp_t;
>    typedef std::map<unsigned, warp_set_t >  bar_id_to_warp_t; /*set of warps reached a specific barrier id*/
910,911c909,910
<     // individual warp hits barrier
<     void warp_reaches_barrier( unsigned cta_id, unsigned warp_id, warp_inst_t* inst);
---
>    // individual warp hits barrier
>    void warp_reaches_barrier( unsigned cta_id, unsigned warp_id, warp_inst_t* inst);
914,915c913,914
<     // warp reaches exit
<     void warp_exit( unsigned warp_id );
---
>    // warp reaches exit 
>    void warp_exit( unsigned warp_id );
917,918c916,917
<     // assertions
<     bool warp_waiting_at_barrier( unsigned warp_id ) const;
---
>    // assertions
>    bool warp_waiting_at_barrier( unsigned warp_id ) const;
920,921c919,920
<     // debug
<     void dump();
---
>    // debug
>    void dump();
924,932c923,931
<     unsigned m_max_cta_per_core;
<     unsigned m_max_warps_per_core;
<     unsigned m_max_barriers_per_cta;
<     unsigned m_warp_size;
<     cta_to_warp_t m_cta_to_warps;
<     bar_id_to_warp_t m_bar_id_to_warps;
<     warp_set_t m_warp_active;
<     warp_set_t m_warp_at_barrier;
<     shader_core_ctx *m_shader;
---
>    unsigned m_max_cta_per_core;
>    unsigned m_max_warps_per_core;
>    unsigned m_max_barriers_per_cta;
>    unsigned m_warp_size;
>    cta_to_warp_t m_cta_to_warps;
>    bar_id_to_warp_t m_bar_id_to_warps;
>    warp_set_t m_warp_active;
>    warp_set_t m_warp_at_barrier;
>    shader_core_ctx *m_shader;
937,938c936,937
<     unsigned pc;
<     unsigned long latency;
---
>    unsigned pc;
>    unsigned long latency;
942c941
<     ifetch_buffer_t() { m_valid = false; }
---
>     ifetch_buffer_t() { m_valid=false; }
944,949c943,948
<     ifetch_buffer_t( address_type pc, unsigned nbytes, unsigned warp_id )
<     {
<         m_valid = true;
<         m_pc = pc;
<         m_nbytes = nbytes;
<         m_warp_id = warp_id;
---
>     ifetch_buffer_t( address_type pc, unsigned nbytes, unsigned warp_id ) 
>     { 
>         m_valid=true; 
>         m_pc=pc; 
>         m_nbytes=nbytes; 
>         m_warp_id=warp_id;
976c975
<         fprintf(fp, "%s dispatch= ", m_name.c_str() );
---
>         fprintf(fp,"%s dispatch= ", m_name.c_str() );
996,1000c995,999
<         active_mask_t active_lanes;
<         active_lanes.reset();
<         for ( unsigned stage = 0; (stage + 1) < m_pipeline_depth; stage++ ) {
<             if ( !m_pipeline_reg[stage]->empty() )
<                 active_lanes |= m_pipeline_reg[stage]->get_active_mask();
---
>     	active_mask_t active_lanes;
>     	active_lanes.reset();
>         for( unsigned stage=0; (stage+1)<m_pipeline_depth; stage++ ){
>         	if( !m_pipeline_reg[stage]->empty() )
>         		active_lanes|=m_pipeline_reg[stage]->get_active_mask();
1005,1012c1004,1011
<     /*
<         virtual void issue( register_set& source_reg )
<         {
<             //move_warp(m_dispatch_reg,source_reg);
<             //source_reg.move_out_to(m_dispatch_reg);
<             simd_function_unit::issue(source_reg);
<         }
<     */
---
> /*
>     virtual void issue( register_set& source_reg )
>     {
>         //move_warp(m_dispatch_reg,source_reg);
>         //source_reg.move_out_to(m_dispatch_reg);
>         simd_function_unit::issue(source_reg);
>     }
> */
1022,1024c1021,1023
<         for ( int s = m_pipeline_depth - 1; s >= 0; s-- ) {
<             if ( !m_pipeline_reg[s]->empty() ) {
<                 fprintf(fp, "      %s[%2d] ", m_name.c_str(), s );
---
>         for( int s=m_pipeline_depth-1; s>=0; s-- ) {
>             if( !m_pipeline_reg[s]->empty() ) { 
>                 fprintf(fp,"      %s[%2d] ", m_name.c_str(), s );
1042c1041
<         switch (inst.op) {
---
>         switch(inst.op) {
1059,1060c1058,1059
<         switch (inst.op) {
<         case SFU_OP: return false;
---
>         switch(inst.op) {
>         case SFU_OP: return false; 
1081c1080
<                shader_core_ctx *core,
---
>                shader_core_ctx *core, 
1084,1086c1083,1085
<                const shader_core_config *config,
<                const memory_config *mem_config,
<                class shader_core_stats *stats,
---
>                const shader_core_config *config, 
>                const memory_config *mem_config,  
>                class shader_core_stats *stats, 
1092c1091
< 
---
>      
1102c1101
<         switch (inst.op) {
---
>         switch(inst.op) {
1126c1125
<                shader_core_ctx *core,
---
>                shader_core_ctx *core, 
1130c1129
<                const memory_config *mem_config,
---
>                const memory_config *mem_config,  
1137c1136
<                shader_core_ctx *core,
---
>                shader_core_ctx *core, 
1141c1140
<                const memory_config *mem_config,
---
>                const memory_config *mem_config,  
1147,1186c1146,1185
<     bool shared_cycle( warp_inst_t &inst, mem_stage_stall_type &rc_fail, mem_stage_access_type &fail_type);
<     bool constant_cycle( warp_inst_t &inst, mem_stage_stall_type &rc_fail, mem_stage_access_type &fail_type);
<     bool texture_cycle( warp_inst_t &inst, mem_stage_stall_type &rc_fail, mem_stage_access_type &fail_type);
<     bool memory_cycle( warp_inst_t &inst, mem_stage_stall_type &rc_fail, mem_stage_access_type &fail_type);
< 
<     virtual mem_stage_stall_type process_cache_access( cache_t* cache,
<             new_addr_type address,
<             warp_inst_t &inst,
<             std::list<cache_event>& events,
<             mem_fetch *mf,
<             enum cache_request_status status );
<     mem_stage_stall_type process_memory_access_queue( cache_t *cache, warp_inst_t &inst );
< 
<     const memory_config *m_memory_config;
<     class mem_fetch_interface *m_icnt;
<     shader_core_mem_fetch_allocator *m_mf_allocator;
<     class shader_core_ctx *m_core;
<     unsigned m_sid;
<     unsigned m_tpc;
< 
<     tex_cache *m_L1T; // texture cache
<     read_only_cache *m_L1C; // constant cache
<     l1_cache *m_L1D; // data cache
<     std::map<unsigned/*warp_id*/, std::map<unsigned/*regnum*/, unsigned/*count*/> > m_pending_writes;
<     std::list<mem_fetch*> m_response_fifo;
<     opndcoll_rfu_t *m_operand_collector;
<     Scoreboard *m_scoreboard;
< 
<     mem_fetch *m_next_global;
<     warp_inst_t m_next_wb;
<     unsigned m_writeback_arb; // round-robin arbiter for writeback contention between L1T, L1C, shared
<     unsigned m_num_writeback_clients;
< 
<     enum mem_stage_stall_type m_mem_rc;
< 
<     shader_core_stats *m_stats;
< 
<     // for debugging
<     unsigned long long m_last_inst_gpu_sim_cycle;
<     unsigned long long m_last_inst_gpu_tot_sim_cycle;
---
>    bool shared_cycle( warp_inst_t &inst, mem_stage_stall_type &rc_fail, mem_stage_access_type &fail_type);
>    bool constant_cycle( warp_inst_t &inst, mem_stage_stall_type &rc_fail, mem_stage_access_type &fail_type);
>    bool texture_cycle( warp_inst_t &inst, mem_stage_stall_type &rc_fail, mem_stage_access_type &fail_type);
>    bool memory_cycle( warp_inst_t &inst, mem_stage_stall_type &rc_fail, mem_stage_access_type &fail_type);
> 
>    virtual mem_stage_stall_type process_cache_access( cache_t* cache,
>                                                       new_addr_type address,
>                                                       warp_inst_t &inst,
>                                                       std::list<cache_event>& events,
>                                                       mem_fetch *mf,
>                                                       enum cache_request_status status );
>    mem_stage_stall_type process_memory_access_queue( cache_t *cache, warp_inst_t &inst );
> 
>    const memory_config *m_memory_config;
>    class mem_fetch_interface *m_icnt;
>    shader_core_mem_fetch_allocator *m_mf_allocator;
>    class shader_core_ctx *m_core;
>    unsigned m_sid;
>    unsigned m_tpc;
> 
>    tex_cache *m_L1T; // texture cache
>    read_only_cache *m_L1C; // constant cache
>    l1_cache *m_L1D; // data cache
>    std::map<unsigned/*warp_id*/, std::map<unsigned/*regnum*/,unsigned/*count*/> > m_pending_writes;
>    std::list<mem_fetch*> m_response_fifo;
>    opndcoll_rfu_t *m_operand_collector;
>    Scoreboard *m_scoreboard;
> 
>    mem_fetch *m_next_global;
>    warp_inst_t m_next_wb;
>    unsigned m_writeback_arb; // round-robin arbiter for writeback contention between L1T, L1C, shared
>    unsigned m_num_writeback_clients;
> 
>    enum mem_stage_stall_type m_mem_rc;
> 
>    shader_core_stats *m_stats; 
> 
>    // for debugging
>    unsigned long long m_last_inst_gpu_sim_cycle;
>    unsigned long long m_last_inst_gpu_tot_sim_cycle;
1190,1192c1189,1191
<     ID_OC_SP = 0,
<     ID_OC_SFU,
<     ID_OC_MEM,
---
>     ID_OC_SP=0,
>     ID_OC_SFU,  
>     ID_OC_MEM,  
1197c1196
<     N_PIPELINE_STAGES
---
>     N_PIPELINE_STAGES 
1202,1203c1201,1202
<     "ID_OC_SFU",
<     "ID_OC_MEM",
---
>     "ID_OC_SFU",  
>     "ID_OC_MEM",  
1208c1207
<     "N_PIPELINE_STAGES"
---
>     "N_PIPELINE_STAGES" 
1213,1214c1212,1213
<     shader_core_config() {
<         pipeline_widths_string = NULL;
---
>     shader_core_config(){
> 	pipeline_widths_string = NULL;
1219c1218
<         int ntok = sscanf(gpgpu_shader_core_pipeline_opt, "%d:%d",
---
>         int ntok = sscanf(gpgpu_shader_core_pipeline_opt,"%d:%d", 
1222,1238c1221,1237
<         if (ntok != 2) {
<             printf("GPGPU-Sim uArch: error while parsing configuration string gpgpu_shader_core_pipeline_opt\n");
<             abort();
<         }
< 
<         char* toks = new char[100];
<         char* tokd = toks;
<         strcpy(toks, pipeline_widths_string);
< 
<         toks = strtok(toks, ",");
<         for (unsigned i = 0; i < N_PIPELINE_STAGES; i++) {
<             assert(toks);
<             ntok = sscanf(toks, "%d", &pipe_widths[i]);
<             assert(ntok == 1);
<             toks = strtok(NULL, ",");
<         }
<         delete[] tokd;
---
>         if(ntok != 2) {
>            printf("GPGPU-Sim uArch: error while parsing configuration string gpgpu_shader_core_pipeline_opt\n");
>            abort();
> 	}
> 
> 	char* toks = new char[100];
> 	char* tokd = toks;
> 	strcpy(toks,pipeline_widths_string);
> 
> 	toks = strtok(toks,",");
> 	for (unsigned i = 0; i < N_PIPELINE_STAGES; i++) { 
> 	    assert(toks);
> 	    ntok = sscanf(toks,"%d", &pipe_widths[i]);
> 	    assert(ntok == 1); 
> 	    toks = strtok(NULL,",");
> 	}
> 	delete[] tokd;
1241,1243c1240,1242
<             printf("GPGPU-Sim uArch: Error ** increase MAX_THREAD_PER_SM in abstract_hardware_model.h from %u to %u\n",
<                    MAX_THREAD_PER_SM, n_thread_per_shader);
<             abort();
---
>            printf("GPGPU-Sim uArch: Error ** increase MAX_THREAD_PER_SM in abstract_hardware_model.h from %u to %u\n", 
>                   MAX_THREAD_PER_SM, n_thread_per_shader);
>            abort();
1245c1244
<         max_warps_per_shader =  n_thread_per_shader / warp_size;
---
>         max_warps_per_shader =  n_thread_per_shader/warp_size;
1249,1252c1248,1251
<         m_L1I_config.init(m_L1I_config.m_config_string, FuncCachePreferNone);
<         m_L1T_config.init(m_L1T_config.m_config_string, FuncCachePreferNone);
<         m_L1C_config.init(m_L1C_config.m_config_string, FuncCachePreferNone);
<         m_L1D_config.init(m_L1D_config.m_config_string, FuncCachePreferNone);
---
>         m_L1I_config.init(m_L1I_config.m_config_string,FuncCachePreferNone);
>         m_L1T_config.init(m_L1T_config.m_config_string,FuncCachePreferNone);
>         m_L1C_config.init(m_L1C_config.m_config_string,FuncCachePreferNone);
>         m_L1D_config.init(m_L1D_config.m_config_string,FuncCachePreferNone);
1259c1258
<     unsigned num_shader() const { return n_simt_clusters * n_simt_cores_per_cluster; }
---
>     unsigned num_shader() const { return n_simt_clusters*n_simt_cores_per_cluster; }
1262c1261
<     unsigned cid_to_sid( unsigned cid, unsigned cluster_id ) const { return cluster_id * n_simt_cores_per_cluster + cid; }
---
>     unsigned cid_to_sid( unsigned cid, unsigned cluster_id ) const { return cluster_id*n_simt_cores_per_cluster + cid; }
1272c1271
<     unsigned max_warps_per_shader;
---
>     unsigned max_warps_per_shader; 
1285,1286c1284,1285
<     bool gmem_skip_L1D; // on = global memory access always skip the L1 cache
< 
---
>     bool gmem_skip_L1D; // on = global memory access always skip the L1 cache 
>     
1319c1318
< 
---
>     
1322c1321
< 
---
>     
1328,1329c1327,1328
<     int simt_core_sim_order;
< 
---
>     int simt_core_sim_order; 
>     
1335,1336c1334,1335
<     void* shader_core_stats_pod_start[0]; // DO NOT MOVE FROM THE TOP - spaceless pointer to the start of this structure
<     unsigned long long *shader_cycles;
---
> 	void* shader_core_stats_pod_start[0]; // DO NOT MOVE FROM THE TOP - spaceless pointer to the start of this structure
> 	unsigned long long *shader_cycles;
1339,1340c1338,1339
<     unsigned *m_last_num_sim_insn;
<     unsigned *m_last_num_sim_winsn;
---
> 	unsigned *m_last_num_sim_insn;
> 	unsigned *m_last_num_sim_winsn;
1398c1397
< 
---
>     
1400c1399
<     int gpgpu_n_mem_l1_write_allocate;
---
>     int gpgpu_n_mem_l1_write_allocate; 
1417,1456c1416,1455
<         memset(pod, 0, sizeof(shader_core_stats_pod));
<         shader_cycles = (unsigned long long *) calloc(config->num_shader(), sizeof(unsigned long long ));
<         m_num_sim_insn = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_num_sim_winsn = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_last_num_sim_winsn = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_last_num_sim_insn = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_pipeline_duty_cycle = (float*) calloc(config->num_shader(), sizeof(float));
<         m_num_decoded_insn = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_num_FPdecoded_insn = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_num_storequeued_insn = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_num_loadqueued_insn = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_num_INTdecoded_insn = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_num_ialu_acesses = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_num_fp_acesses = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_num_tex_inst = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_num_imul_acesses = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_num_imul24_acesses = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_num_imul32_acesses = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_num_fpmul_acesses = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_num_idiv_acesses = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_num_fpdiv_acesses = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_num_sp_acesses = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_num_sfu_acesses = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_num_trans_acesses = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_num_mem_acesses = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_num_sp_committed = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_num_tlb_hits = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_num_tlb_accesses = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_active_sp_lanes = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_active_sfu_lanes = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_active_fu_lanes = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_active_fu_mem_lanes = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_num_sfu_committed = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_num_mem_committed = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_read_regfile_acesses = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_write_regfile_acesses = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_non_rf_operands = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         m_n_diverge = (unsigned*) calloc(config->num_shader(), sizeof(unsigned));
<         shader_cycle_distro = (unsigned*) calloc(config->warp_size + 3, sizeof(unsigned));
<         last_shader_cycle_distro = (unsigned*) calloc(m_config->warp_size + 3, sizeof(unsigned));
---
>         memset(pod,0,sizeof(shader_core_stats_pod));
>         shader_cycles=(unsigned long long *) calloc(config->num_shader(),sizeof(unsigned long long ));
>         m_num_sim_insn = (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_num_sim_winsn = (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_last_num_sim_winsn = (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_last_num_sim_insn = (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_pipeline_duty_cycle=(float*) calloc(config->num_shader(),sizeof(float));
>         m_num_decoded_insn = (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_num_FPdecoded_insn = (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_num_storequeued_insn=(unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_num_loadqueued_insn=(unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_num_INTdecoded_insn = (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_num_ialu_acesses = (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_num_fp_acesses= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_num_tex_inst= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_num_imul_acesses= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_num_imul24_acesses= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_num_imul32_acesses= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_num_fpmul_acesses= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_num_idiv_acesses= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_num_fpdiv_acesses= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_num_sp_acesses= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_num_sfu_acesses= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_num_trans_acesses= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_num_mem_acesses= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_num_sp_committed= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_num_tlb_hits=(unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_num_tlb_accesses=(unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_active_sp_lanes= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_active_sfu_lanes= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_active_fu_lanes= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_active_fu_mem_lanes= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_num_sfu_committed= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_num_mem_committed= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_read_regfile_acesses= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_write_regfile_acesses= (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_non_rf_operands=(unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         m_n_diverge = (unsigned*) calloc(config->num_shader(),sizeof(unsigned));
>         shader_cycle_distro = (unsigned*) calloc(config->warp_size+3, sizeof(unsigned));
>         last_shader_cycle_distro = (unsigned*) calloc(m_config->warp_size+3, sizeof(unsigned));
1461,1462c1460,1461
<         m_outgoing_traffic_stats = new traffic_breakdown("coretomem");
<         m_incoming_traffic_stats = new traffic_breakdown("memtocore");
---
>         m_outgoing_traffic_stats = new traffic_breakdown("coretomem"); 
>         m_incoming_traffic_stats = new traffic_breakdown("memtocore"); 
1472,1474c1471,1473
<         delete m_outgoing_traffic_stats;
<         delete m_incoming_traffic_stats;
<         free(m_num_sim_insn);
---
>         delete m_outgoing_traffic_stats; 
>         delete m_incoming_traffic_stats; 
>         free(m_num_sim_insn); 
1476c1475
<         free(m_n_diverge);
---
>         free(m_n_diverge); 
1505c1504
<     traffic_breakdown *m_incoming_traffic_stats; // memory partition to core
---
>     traffic_breakdown *m_incoming_traffic_stats; // memory partition to core 
1526,1540c1525,1539
<         m_core_id = core_id;
<         m_cluster_id = cluster_id;
<         m_memory_config = config;
<     }
<     mem_fetch *alloc( new_addr_type addr, mem_access_type type, unsigned size, bool wr ) const
<     {
<         mem_access_t access( type, addr, size, wr );
<         mem_fetch *mf = new mem_fetch( access,
<                                        NULL,
<                                        wr ? WRITE_PACKET_SIZE : READ_PACKET_SIZE,
<                                        -1,
<                                        m_core_id,
<                                        m_cluster_id,
<                                        m_memory_config );
<         return mf;
---
>     	m_core_id = core_id;
>     	m_cluster_id = cluster_id;
>     	m_memory_config = config;
>     }
>     mem_fetch *alloc( new_addr_type addr, mem_access_type type, unsigned size, bool wr ) const 
>     {
>     	mem_access_t access( type, addr, size, wr );
>     	mem_fetch *mf = new mem_fetch( access, 
>     				       NULL,
>     				       wr?WRITE_PACKET_SIZE:READ_PACKET_SIZE, 
>     				       -1, 
>     				       m_core_id, 
>     				       m_cluster_id,
>     				       m_memory_config );
>     	return mf;
1542c1541
< 
---
>     
1546,1548c1545,1547
<         mem_fetch *mf = new mem_fetch(access,
<                                       &inst_copy,
<                                       access.is_write() ? WRITE_PACKET_SIZE : READ_PACKET_SIZE,
---
>         mem_fetch *mf = new mem_fetch(access, 
>                                       &inst_copy, 
>                                       access.is_write()?WRITE_PACKET_SIZE:READ_PACKET_SIZE,
1550,1551c1549,1550
<                                       m_core_id,
<                                       m_cluster_id,
---
>                                       m_core_id, 
>                                       m_cluster_id, 
1581,1582c1580,1581
<     void broadcast_barrier_reduction(unsigned cta_id, unsigned bar_id, warp_set_t warps);
<     void set_kernel( kernel_info_t *k )
---
>     void broadcast_barrier_reduction(unsigned cta_id, unsigned bar_id,warp_set_t warps);
>     void set_kernel( kernel_info_t *k ) 
1585,1586c1584,1585
<         m_kernel = k;
<         k->inc_running();
---
>         m_kernel=k; 
>         k->inc_running(); 
1588c1587
<                m_kernel->name().c_str() );
---
>                  m_kernel->name().c_str() );
1590c1589
< 
---
>    
1596c1595
<     unsigned isactive() const {if (m_n_active_cta > 0) return 1; else return 0;}
---
>     unsigned isactive() const {if(m_n_active_cta>0) return 1; else return 0;}
1603c1602
< 
---
>     
1618c1617
< 
---
>     
1638,1746c1637,1745
<     void incialu_stat(unsigned active_count, double latency) {
<         if (m_config->gpgpu_clock_gated_lanes == false) {
<             m_stats->m_num_ialu_acesses[m_sid] = m_stats->m_num_ialu_acesses[m_sid] + active_count * latency
<                                                  + inactive_lanes_accesses_nonsfu(active_count, latency);
<         } else {
<             m_stats->m_num_ialu_acesses[m_sid] = m_stats->m_num_ialu_acesses[m_sid] + active_count * latency;
<         }
<     }
<     void inctex_stat(unsigned active_count, double latency) {
<         m_stats->m_num_tex_inst[m_sid] = m_stats->m_num_tex_inst[m_sid] + active_count * latency;
<     }
<     void incimul_stat(unsigned active_count, double latency) {
<         if (m_config->gpgpu_clock_gated_lanes == false) {
<             m_stats->m_num_imul_acesses[m_sid] = m_stats->m_num_imul_acesses[m_sid] + active_count * latency
<                                                  + inactive_lanes_accesses_nonsfu(active_count, latency);
<         } else {
<             m_stats->m_num_imul_acesses[m_sid] = m_stats->m_num_imul_acesses[m_sid] + active_count * latency;
<         }
<     }
<     void incimul24_stat(unsigned active_count, double latency) {
<         if (m_config->gpgpu_clock_gated_lanes == false) {
<             m_stats->m_num_imul24_acesses[m_sid] = m_stats->m_num_imul24_acesses[m_sid] + active_count * latency
<                                                    + inactive_lanes_accesses_nonsfu(active_count, latency);
<         } else {
<             m_stats->m_num_imul24_acesses[m_sid] = m_stats->m_num_imul24_acesses[m_sid] + active_count * latency;
<         }
<     }
<     void incimul32_stat(unsigned active_count, double latency) {
<         if (m_config->gpgpu_clock_gated_lanes == false) {
<             m_stats->m_num_imul32_acesses[m_sid] = m_stats->m_num_imul32_acesses[m_sid] + active_count * latency
<                                                    + inactive_lanes_accesses_sfu(active_count, latency);
<         } else {
<             m_stats->m_num_imul32_acesses[m_sid] = m_stats->m_num_imul32_acesses[m_sid] + active_count * latency;
<         }
<         //printf("Int_Mul -- Active_count: %d\n",active_count);
<     }
<     void incidiv_stat(unsigned active_count, double latency) {
<         if (m_config->gpgpu_clock_gated_lanes == false) {
<             m_stats->m_num_idiv_acesses[m_sid] = m_stats->m_num_idiv_acesses[m_sid] + active_count * latency
<                                                  + inactive_lanes_accesses_sfu(active_count, latency);
<         } else {
<             m_stats->m_num_idiv_acesses[m_sid] = m_stats->m_num_idiv_acesses[m_sid] + active_count * latency;
<         }
<     }
<     void incfpalu_stat(unsigned active_count, double latency) {
<         if (m_config->gpgpu_clock_gated_lanes == false) {
<             m_stats->m_num_fp_acesses[m_sid] = m_stats->m_num_fp_acesses[m_sid] + active_count * latency
<                                                + inactive_lanes_accesses_nonsfu(active_count, latency);
<         } else {
<             m_stats->m_num_fp_acesses[m_sid] = m_stats->m_num_fp_acesses[m_sid] + active_count * latency;
<         }
<     }
<     void incfpmul_stat(unsigned active_count, double latency) {
<         // printf("FP MUL stat increament\n");
<         if (m_config->gpgpu_clock_gated_lanes == false) {
<             m_stats->m_num_fpmul_acesses[m_sid] = m_stats->m_num_fpmul_acesses[m_sid] + active_count * latency
<                                                   + inactive_lanes_accesses_nonsfu(active_count, latency);
<         } else {
<             m_stats->m_num_fpmul_acesses[m_sid] = m_stats->m_num_fpmul_acesses[m_sid] + active_count * latency;
<         }
<     }
<     void incfpdiv_stat(unsigned active_count, double latency) {
<         if (m_config->gpgpu_clock_gated_lanes == false) {
<             m_stats->m_num_fpdiv_acesses[m_sid] = m_stats->m_num_fpdiv_acesses[m_sid] + active_count * latency
<                                                   + inactive_lanes_accesses_sfu(active_count, latency);
<         } else {
<             m_stats->m_num_fpdiv_acesses[m_sid] = m_stats->m_num_fpdiv_acesses[m_sid] + active_count * latency;
<         }
<     }
<     void inctrans_stat(unsigned active_count, double latency) {
<         if (m_config->gpgpu_clock_gated_lanes == false) {
<             m_stats->m_num_trans_acesses[m_sid] = m_stats->m_num_trans_acesses[m_sid] + active_count * latency
<                                                   + inactive_lanes_accesses_sfu(active_count, latency);
<         } else {
<             m_stats->m_num_trans_acesses[m_sid] = m_stats->m_num_trans_acesses[m_sid] + active_count * latency;
<         }
<     }
< 
<     void incsfu_stat(unsigned active_count, double latency) {m_stats->m_num_sfu_acesses[m_sid] = m_stats->m_num_sfu_acesses[m_sid] + active_count * latency;}
<     void incsp_stat(unsigned active_count, double latency) {m_stats->m_num_sp_acesses[m_sid] = m_stats->m_num_sp_acesses[m_sid] + active_count * latency;}
<     void incmem_stat(unsigned active_count, double latency) {
<         if (m_config->gpgpu_clock_gated_lanes == false) {
<             m_stats->m_num_mem_acesses[m_sid] = m_stats->m_num_mem_acesses[m_sid] + active_count * latency
<                                                 + inactive_lanes_accesses_nonsfu(active_count, latency);
<         } else {
<             m_stats->m_num_mem_acesses[m_sid] = m_stats->m_num_mem_acesses[m_sid] + active_count * latency;
<         }
<     }
<     void incexecstat(warp_inst_t *&inst);
< 
<     void incregfile_reads(unsigned active_count) {m_stats->m_read_regfile_acesses[m_sid] = m_stats->m_read_regfile_acesses[m_sid] + active_count;}
<     void incregfile_writes(unsigned active_count) {m_stats->m_write_regfile_acesses[m_sid] = m_stats->m_write_regfile_acesses[m_sid] + active_count;}
<     void incnon_rf_operands(unsigned active_count) {m_stats->m_non_rf_operands[m_sid] = m_stats->m_non_rf_operands[m_sid] + active_count;}
< 
<     void incspactivelanes_stat(unsigned active_count) {m_stats->m_active_sp_lanes[m_sid] = m_stats->m_active_sp_lanes[m_sid] + active_count;}
<     void incsfuactivelanes_stat(unsigned active_count) {m_stats->m_active_sfu_lanes[m_sid] = m_stats->m_active_sfu_lanes[m_sid] + active_count;}
<     void incfuactivelanes_stat(unsigned active_count) {m_stats->m_active_fu_lanes[m_sid] = m_stats->m_active_fu_lanes[m_sid] + active_count;}
<     void incfumemactivelanes_stat(unsigned active_count) {m_stats->m_active_fu_mem_lanes[m_sid] = m_stats->m_active_fu_mem_lanes[m_sid] + active_count;}
< 
<     void inc_simt_to_mem(unsigned n_flits) { m_stats->n_simt_to_mem[m_sid] += n_flits; }
<     bool check_if_non_released_reduction_barrier(warp_inst_t &inst);
< 
< private:
<     unsigned inactive_lanes_accesses_sfu(unsigned active_count, double latency) {
<         return  ( ((32 - active_count) >> 1) * latency) + ( ((32 - active_count) >> 3) * latency) + ( ((32 - active_count) >> 3) * latency);
<     }
<     unsigned inactive_lanes_accesses_nonsfu(unsigned active_count, double latency) {
<         return  ( ((32 - active_count) >> 1) * latency);
<     }
---
>     void incialu_stat(unsigned active_count,double latency) {
> 		if(m_config->gpgpu_clock_gated_lanes==false){
> 		  m_stats->m_num_ialu_acesses[m_sid]=m_stats->m_num_ialu_acesses[m_sid]+active_count*latency
> 		    + inactive_lanes_accesses_nonsfu(active_count, latency);
> 		}else {
>         m_stats->m_num_ialu_acesses[m_sid]=m_stats->m_num_ialu_acesses[m_sid]+active_count*latency;
> 		}
> 	 }
>     void inctex_stat(unsigned active_count,double latency){
>     	m_stats->m_num_tex_inst[m_sid]=m_stats->m_num_tex_inst[m_sid]+active_count*latency;
>     }
>     void incimul_stat(unsigned active_count,double latency) {
> 		if(m_config->gpgpu_clock_gated_lanes==false){
> 		  m_stats->m_num_imul_acesses[m_sid]=m_stats->m_num_imul_acesses[m_sid]+active_count*latency
> 		    + inactive_lanes_accesses_nonsfu(active_count, latency);
> 		}else {
>         m_stats->m_num_imul_acesses[m_sid]=m_stats->m_num_imul_acesses[m_sid]+active_count*latency;
> 		}
> 	 }
>     void incimul24_stat(unsigned active_count,double latency) {
>       if(m_config->gpgpu_clock_gated_lanes==false){
>    		m_stats->m_num_imul24_acesses[m_sid]=m_stats->m_num_imul24_acesses[m_sid]+active_count*latency
> 		    + inactive_lanes_accesses_nonsfu(active_count, latency);
> 		}else {
> 		  m_stats->m_num_imul24_acesses[m_sid]=m_stats->m_num_imul24_acesses[m_sid]+active_count*latency;
> 		}
> 	 }
> 	 void incimul32_stat(unsigned active_count,double latency) {
> 		if(m_config->gpgpu_clock_gated_lanes==false){
> 		  m_stats->m_num_imul32_acesses[m_sid]=m_stats->m_num_imul32_acesses[m_sid]+active_count*latency
> 			 + inactive_lanes_accesses_sfu(active_count, latency);			
> 		}else{
> 		  m_stats->m_num_imul32_acesses[m_sid]=m_stats->m_num_imul32_acesses[m_sid]+active_count*latency;
> 		}
> 		//printf("Int_Mul -- Active_count: %d\n",active_count);
> 	 }
> 	 void incidiv_stat(unsigned active_count,double latency) {
> 		if(m_config->gpgpu_clock_gated_lanes==false){
> 		  m_stats->m_num_idiv_acesses[m_sid]=m_stats->m_num_idiv_acesses[m_sid]+active_count*latency
> 			 + inactive_lanes_accesses_sfu(active_count, latency); 
> 		}else {
> 		  m_stats->m_num_idiv_acesses[m_sid]=m_stats->m_num_idiv_acesses[m_sid]+active_count*latency;
> 		}
> 	 }
> 	 void incfpalu_stat(unsigned active_count,double latency) {
> 		if(m_config->gpgpu_clock_gated_lanes==false){
> 		  m_stats->m_num_fp_acesses[m_sid]=m_stats->m_num_fp_acesses[m_sid]+active_count*latency
> 			 + inactive_lanes_accesses_nonsfu(active_count, latency);
> 		}else {
>         m_stats->m_num_fp_acesses[m_sid]=m_stats->m_num_fp_acesses[m_sid]+active_count*latency;
> 		} 
> 	 }
> 	 void incfpmul_stat(unsigned active_count,double latency) {
> 		 		// printf("FP MUL stat increament\n");
>       if(m_config->gpgpu_clock_gated_lanes==false){
> 		  m_stats->m_num_fpmul_acesses[m_sid]=m_stats->m_num_fpmul_acesses[m_sid]+active_count*latency
> 		    + inactive_lanes_accesses_nonsfu(active_count, latency);
> 		}else {
>         m_stats->m_num_fpmul_acesses[m_sid]=m_stats->m_num_fpmul_acesses[m_sid]+active_count*latency;
> 		}
> 	 }
> 	 void incfpdiv_stat(unsigned active_count,double latency) {
> 		if(m_config->gpgpu_clock_gated_lanes==false){
> 		  m_stats->m_num_fpdiv_acesses[m_sid]=m_stats->m_num_fpdiv_acesses[m_sid]+active_count*latency
> 			+ inactive_lanes_accesses_sfu(active_count, latency); 
> 		}else {
> 		  m_stats->m_num_fpdiv_acesses[m_sid]=m_stats->m_num_fpdiv_acesses[m_sid]+active_count*latency;
> 		}
> 	 }
> 	 void inctrans_stat(unsigned active_count,double latency) {
> 		if(m_config->gpgpu_clock_gated_lanes==false){
> 		  m_stats->m_num_trans_acesses[m_sid]=m_stats->m_num_trans_acesses[m_sid]+active_count*latency
> 			+ inactive_lanes_accesses_sfu(active_count, latency); 
> 		}else{
> 		  m_stats->m_num_trans_acesses[m_sid]=m_stats->m_num_trans_acesses[m_sid]+active_count*latency;
> 		}
> 	 }
> 
> 	 void incsfu_stat(unsigned active_count,double latency) {m_stats->m_num_sfu_acesses[m_sid]=m_stats->m_num_sfu_acesses[m_sid]+active_count*latency;}
> 	 void incsp_stat(unsigned active_count,double latency) {m_stats->m_num_sp_acesses[m_sid]=m_stats->m_num_sp_acesses[m_sid]+active_count*latency;}
> 	 void incmem_stat(unsigned active_count,double latency) {
> 		if(m_config->gpgpu_clock_gated_lanes==false){
> 		  m_stats->m_num_mem_acesses[m_sid]=m_stats->m_num_mem_acesses[m_sid]+active_count*latency
> 		    + inactive_lanes_accesses_nonsfu(active_count, latency);
> 		}else {
> 		  m_stats->m_num_mem_acesses[m_sid]=m_stats->m_num_mem_acesses[m_sid]+active_count*latency;
> 		}
> 	 }
> 	 void incexecstat(warp_inst_t *&inst);
> 
> 	 void incregfile_reads(unsigned active_count) {m_stats->m_read_regfile_acesses[m_sid]=m_stats->m_read_regfile_acesses[m_sid]+active_count;}
> 	 void incregfile_writes(unsigned active_count){m_stats->m_write_regfile_acesses[m_sid]=m_stats->m_write_regfile_acesses[m_sid]+active_count;}
> 	 void incnon_rf_operands(unsigned active_count){m_stats->m_non_rf_operands[m_sid]=m_stats->m_non_rf_operands[m_sid]+active_count;}
> 
> 	 void incspactivelanes_stat(unsigned active_count) {m_stats->m_active_sp_lanes[m_sid]=m_stats->m_active_sp_lanes[m_sid]+active_count;}
> 	 void incsfuactivelanes_stat(unsigned active_count) {m_stats->m_active_sfu_lanes[m_sid]=m_stats->m_active_sfu_lanes[m_sid]+active_count;}
> 	 void incfuactivelanes_stat(unsigned active_count) {m_stats->m_active_fu_lanes[m_sid]=m_stats->m_active_fu_lanes[m_sid]+active_count;}
> 	 void incfumemactivelanes_stat(unsigned active_count) {m_stats->m_active_fu_mem_lanes[m_sid]=m_stats->m_active_fu_mem_lanes[m_sid]+active_count;}
> 
> 	 void inc_simt_to_mem(unsigned n_flits){ m_stats->n_simt_to_mem[m_sid] += n_flits; }
> 	 bool check_if_non_released_reduction_barrier(warp_inst_t &inst);
> 
> 	private:
> 	 unsigned inactive_lanes_accesses_sfu(unsigned active_count,double latency){
>       return  ( ((32-active_count)>>1)*latency) + ( ((32-active_count)>>3)*latency) + ( ((32-active_count)>>3)*latency);
> 	 }
> 	 unsigned inactive_lanes_accesses_nonsfu(unsigned active_count,double latency){
>       return  ( ((32-active_count)>>1)*latency);
> 	 }
1756c1755
< 
---
>     
1764c1763
<     // Returns numbers of addresses in translated_addrs
---
>      // Returns numbers of addresses in translated_addrs
1768c1767
< 
---
>     
1770c1769
< 
---
>     
1772c1771
< 
---
>     
1786c1785
<     // statistics
---
>     // statistics 
1791,1792c1790,1791
<     unsigned m_cta_status[MAX_CTA_PER_SHADER]; // CTAs status
<     unsigned m_not_completed; // number of threads to be completed (==0 when all thread on this core completed)
---
>     unsigned m_cta_status[MAX_CTA_PER_SHADER]; // CTAs status 
>     unsigned m_not_completed; // number of threads to be completed (==0 when all thread on this core completed) 
1794,1795c1793,1794
< 
<     // thread contexts
---
>     
>     // thread contexts 
1797c1796
< 
---
>     
1801c1800
< 
---
>     
1839,1841c1838,1840
<     simt_core_cluster( class gpgpu_sim *gpu,
<                        unsigned cluster_id,
<                        const struct shader_core_config *config,
---
>     simt_core_cluster( class gpgpu_sim *gpu, 
>                        unsigned cluster_id, 
>                        const struct shader_core_config *config, 
1897,1898c1896,1897
<     shader_memory_interface( shader_core_ctx *core, simt_core_cluster *cluster ) { m_core = core; m_cluster = cluster; }
<     virtual bool full( unsigned size, bool write ) const
---
>     shader_memory_interface( shader_core_ctx *core, simt_core_cluster *cluster ) { m_core=core; m_cluster=cluster; }
>     virtual bool full( unsigned size, bool write ) const 
1900c1899
<         return m_cluster->icnt_injection_buffer_full(size, write);
---
>         return m_cluster->icnt_injection_buffer_full(size,write);
1902c1901
<     virtual void push(mem_fetch *mf)
---
>     virtual void push(mem_fetch *mf) 
1904,1905c1903,1904
<         m_core->inc_simt_to_mem(mf->get_num_flits(true));
<         m_cluster->icnt_inject_request_packet(mf);
---
>     	m_core->inc_simt_to_mem(mf->get_num_flits(true));
>         m_cluster->icnt_inject_request_packet(mf);        
1914c1913
<     perfect_memory_interface( shader_core_ctx *core, simt_core_cluster *cluster ) { m_core = core; m_cluster = cluster; }
---
>     perfect_memory_interface( shader_core_ctx *core, simt_core_cluster *cluster ) { m_core=core; m_cluster=cluster; }
1924c1923
<         m_cluster->push_response_fifo(mf);
---
>         m_cluster->push_response_fifo(mf);        
